id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/774:10372,usability,input,input,10372,".967773 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:26.024448 140099871606592 make_examples_core.py:301] Task 0/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0217 23:33:34.679437 140533724936000 make_examples_core.py:301] Task 15/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.748554 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.253728 140616181937984 make_examples_core.py:301] Task 14/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.663679 140616181937984 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:10525,usability,input,input,10525,"448 140099871606592 make_examples_core.py:301] Task 0/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0217 23:33:34.679437 140533724936000 make_examples_core.py:301] Task 15/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.748554 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.253728 140616181937984 make_examples_core.py:301] Task 14/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.663679 140616181937984 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:10678,usability,input,input,10678,"r10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']. I0217 23:33:34.679437 140533724936000 make_examples_core.py:301] Task 15/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.748554 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.253728 140616181937984 make_examples_core.py:301] Task 14/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.663679 140616181937984 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:10827,usability,input,input,10827,"3:34.679437 140533724936000 make_examples_core.py:301] Task 15/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.748554 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.253728 140616181937984 make_examples_core.py:301] Task 14/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.663679 140616181937984 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with Nati",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:11313,usability,input,inputs,11313,"_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:34.663679 140616181937984 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:35.675196 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:11611,usability,input,input,11611,"23:33:34.663670 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:35.675196 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:11764,usability,input,input,11764,"3:34.887505 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:35.675196 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.3021",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:11913,usability,input,input,11913,"3:34.888105 140533724936000 make_examples_core.py:301] Task 15/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00015-of-00016.gz. I0217 23:33:34.888602 140533724936000 make_examples_core.py:301] Task 15/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00015-of-00016.gz. I0217 23:33:34.888697 140533724936000 make_examples_core.py:301] Task 15/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.912838 140533724936000 make_examples_core.py:301] Task 15/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:35.675196 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants. I0218 00:34:18.302218 140191938357056 make_examples_core.py:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:12400,usability,input,inputs,12400,"5/16: 0 candidates (0 examples) [0.02s elapsed]. I0217 23:33:35.455924 139726133032768 make_examples_core.py:301] Task 4/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. I0217 23:33:35.526908 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. I0217 23:33:35.675196 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants. I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s. user	928m53.495s. sys	2m16.403s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:12972,usability,user,user,12972," NativeSamReader. ... I0217 23:33:34.810906 140099871606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants. I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s. user	928m53.495s. sys	2m16.403s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has en",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:13023,usability,command,command,13023,"606592 make_examples_core.py:301] Task 0/16: Writing gvcf records to /tmp/tmpd74of138/gvcf.tfrecord-00000-of-00016.gz. I0217 23:33:34.811542 140099871606592 make_examples_core.py:301] Task 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants. I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s. user	928m53.495s. sys	2m16.403s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:13213,usability,input,input,13213,"0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz. I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds. I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]. ... I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]. I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants. I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s. user	928m53.495s. sys	2m16.403s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:13929,usability,User,UserWarning,13929,"ted 10372 examples. real	62m19.124s. user	928m53.495s. sys	2m16.403s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:14042,usability,minim,minimal,14042,"t/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:14508,usability,error,error,14508,"usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:14756,usability,input,input,14756,"ensorRT, please make sure the missing libraries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/ince",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:14799,usability,input,input,14799,"ries mentioned above are installed properly. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model us",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:14908,usability,input,input,14908,"fa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:15303,usability,Input,InputLayer,15303,"b.com/tensorflow/addons/issues/2807 . warnings.warn(. 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started. I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:15776,usability,User,UserWarning,15776," 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:15823,usability,input,input,15823,", 5, 6, 19]. I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:15884,usability,input,input,15884,"nts.py:506] Shape of input examples: [100, 221, 7]. I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False. Model: ""inceptionv3"". __________________________________________________________________________________________________. Layer (type) Output Shape Param # Connected to . ==================================================================================================. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:16282,usability,memor,memory,16282,"=======. input_1 (InputLayer) [(None, 100, 221, 7 0 [] . )] . ... classification (Dense) (None, 3) 6147 ['dropout[0][0]'] . . ==================================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:16422,usability,memor,memory,16422,"====================================================================================. Total params: 21,810,083. Trainable params: 21,775,651. Non-trainable params: 34,432. __________________________________________________________________________________________________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:16681,usability,memor,memory,16681,"___________. /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s. user	118m36.634s. sys	25m56.983s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_appt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:16821,usability,memor,memory,16821,"nput channels. However, it was passed an input_shape with 7 input channels. input_shape = imagenet_utils.obtain_input_shape(. I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s. user	118m36.634s. sys	25m56.983s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:16962,usability,memor,memory,16962,"923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95. I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified. 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s. user	118m36.634s. sys	25m56.983s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:17436,usability,user,user,17436,":35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s. user	118m36.634s. sys	25m56.983s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:17488,usability,command,command,17488,"edicted 1024 examples in 1 batches [0.637 sec per 100]. 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory. 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory. I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100]. I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100]. I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100]. I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s. user	118m36.634s. sys	25m56.983s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:18634,usability,error,error,18634,"ants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes. I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes. I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s. user	0m58.218s. sys	0m5.086s. ***** Running the command:*****. time /opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:19565,usability,user,user,19565,"_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes. I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes. I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s. user	0m58.218s. sys	0m5.086s. ***** Running the command:*****. time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:19613,usability,command,command,19613,"UNKNOWN: unknown error. I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes. I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001. I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes. I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s. user	0m58.218s. sys	0m5.086s. ***** Running the command:*****. time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:57.421117 139673283618624 genomics_reader.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:20553,usability,error,error,20553,"59.941s. user	0m58.218s. sys	0m5.086s. ***** Running the command:*****. time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s. user	0m12.056s. sys	0m2.006s. ```. ----------------------------------------------------------------------------------. ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** . ``` . Sat Feb 17 23:40:49 2024 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off | N/A |. | 30% 27C P8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:20723,usability,user,user,20723,"output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s. user	0m12.056s. sys	0m2.006s. ```. ----------------------------------------------------------------------------------. ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** . ``` . Sat Feb 17 23:40:49 2024 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off | N/A |. | 30% 27C P8 9W / 125W | 110MiB / 8192MiB | 0% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. | 1 Quadro P4000 On | 0000000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:21354,usability,Memor,Memory-Usage,21354,raries mentioned above are installed properly. 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s. user	0m12.056s. sys	0m2.006s. ```. ----------------------------------------------------------------------------------. ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** . ``` . Sat Feb 17 23:40:49 2024 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off | N/A |. | 30% 27C P8 9W / 125W | 110MiB / 8192MiB | 0% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. | 1 Quadro P4000 On | 00000000:65:00.0 On | N/A |. | 46% 33C P0 28W / 105W | 1048MiB / 8192MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | 0 N/A N/A 2236 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 3948 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 4070 C+G ...ome-remote-desktop-daemon 96MiB |. | 1 N/A N/A 2236 G /usr/lib/xorg/Xorg 21MiB |. | 1 N/A N/A 2504,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:22038,usability,Memor,Memory,22038," . **nvidia-smi** . ``` . Sat Feb 17 23:40:49 2024 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off | N/A |. | 30% 27C P8 9W / 125W | 110MiB / 8192MiB | 0% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. | 1 Quadro P4000 On | 00000000:65:00.0 On | N/A |. | 46% 33C P0 28W / 105W | 1048MiB / 8192MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | 0 N/A N/A 2236 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 3948 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 4070 C+G ...ome-remote-desktop-daemon 96MiB |. | 1 N/A N/A 2236 G /usr/lib/xorg/Xorg 21MiB |. | 1 N/A N/A 2504 G ...mviewer/tv_bin/TeamViewer 37MiB |. | 1 N/A N/A 3948 G /usr/lib/xorg/Xorg 509MiB |. | 1 N/A N/A 4182 G /usr/bin/gnome-shell 195MiB |. | 1 N/A N/A 11053 G uex 1MiB |. | 1 N/A N/A 1285104 G ...on=20240130-180151.247000 148MiB |. | 1 N/A N/A 1287635 G ...--variations-seed-version 19MiB |. +-----------------------------------------------------------------------------+. ```. I did run ```export CUDA_VISIBLE_DEVICES=0``` before running deepvariant, as you suggested in this issue https://github.com/google/deepvariant/issues/761. . Why the GPU was not used in my run? Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:22935,usability,help,help,22935," . **nvidia-smi** . ``` . Sat Feb 17 23:40:49 2024 . +-----------------------------------------------------------------------------+. | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |. |-------------------------------+----------------------+----------------------+. | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |. | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |. | | | MIG M. |. |===============================+======================+======================|. | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off | N/A |. | 30% 27C P8 9W / 125W | 110MiB / 8192MiB | 0% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. | 1 Quadro P4000 On | 00000000:65:00.0 On | N/A |. | 46% 33C P0 28W / 105W | 1048MiB / 8192MiB | 1% Default |. | | | N/A |. +-------------------------------+----------------------+----------------------+. . +-----------------------------------------------------------------------------+. | Processes: |. | GPU GI CI PID Type Process name GPU Memory |. | ID ID Usage |. |=============================================================================|. | 0 N/A N/A 2236 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 3948 G /usr/lib/xorg/Xorg 4MiB |. | 0 N/A N/A 4070 C+G ...ome-remote-desktop-daemon 96MiB |. | 1 N/A N/A 2236 G /usr/lib/xorg/Xorg 21MiB |. | 1 N/A N/A 2504 G ...mviewer/tv_bin/TeamViewer 37MiB |. | 1 N/A N/A 3948 G /usr/lib/xorg/Xorg 509MiB |. | 1 N/A N/A 4182 G /usr/bin/gnome-shell 195MiB |. | 1 N/A N/A 11053 G uex 1MiB |. | 1 N/A N/A 1285104 G ...on=20240130-180151.247000 148MiB |. | 1 N/A N/A 1287635 G ...--variations-seed-version 19MiB |. +-----------------------------------------------------------------------------+. ```. I did run ```export CUDA_VISIBLE_DEVICES=0``` before running deepvariant, as you suggested in this issue https://github.com/google/deepvariant/issues/761. . Why the GPU was not used in my run? Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/775:697,availability,error,error,697,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:597,deployability,log,logs,597,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:331,energy efficiency,model,model,331,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:337,energy efficiency,model,model,337,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:697,performance,error,error,697,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:597,safety,log,logs,597,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:697,safety,error,error,697,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:331,security,model,model,331,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:337,security,model,model,337,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:597,security,log,logs,597,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:597,testability,log,logs,597,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:20,usability,tool,tool,20,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:51,usability,command,command,51,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:697,usability,error,error,697,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:710,usability,command,command,710,"ouput variants from tool; I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing. BIN_VERSION=""1.5.0"". ```dockerfile. docker run \. -v ""$(pwd):$(pwd)"" \. -w $(pwd) \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38_no_alt_analysis_set.fasta \. --reads=test_data/Aligned.sortedByCoord.out.bam \. --output_vcf=output/output.vcf.gz \. --num_shards=30 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --logging_dir=output/logs \. --intermediate_results_dir output/intermediate_results_dir. ``` . Please let me know if any error in the command i ran.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/776:733,availability,error,error,733,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:781,availability,error,error,781,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1083,availability,checkpoint,checkpoint,1083,"0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1316,availability,mainten,maintenance,1316,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1400,availability,down,downstream,1400,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:2045,availability,error,errored,2045,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:747,deployability,log,log,747,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1332,deployability,releas,release,1332,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1429,deployability,depend,dependencies,1429,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1911,deployability,contain,contain,1911,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1100,energy efficiency,model,models,1100,"es. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1429,integrability,depend,dependencies,1429,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1453,integrability,repositor,repositories,1453,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1453,interoperability,repositor,repositories,1453,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1143,modifiability,pac,packages,1143,"ocker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1429,modifiability,depend,dependencies,1429,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:733,performance,error,error,733,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:781,performance,error,error,781,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:846,performance,time,time,846,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:2045,performance,error,errored,2045,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1083,reliability,checkpoint,checkpoint,1083,"0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1316,reliability,mainten,maintenance,1316,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:572,safety,compl,completed,572,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:709,safety,compl,completed,709,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:733,safety,error,error,733,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:747,safety,log,log,747,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:781,safety,error,error,781,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1429,safety,depend,dependencies,1429,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1993,safety,Compl,Complete,1993,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:2045,safety,error,errored,2045,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:572,security,compl,completed,572,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:709,security,compl,completed,709,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:747,security,log,log,747,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1100,security,model,models,1100,"es. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1393,security,modif,modify,1393,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1993,security,Compl,Complete,1993,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:747,testability,log,log,747,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1353,testability,plan,planned,1353,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1429,testability,depend,dependencies,1429,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:733,usability,error,error,733,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:781,usability,error,error,781,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:831,usability,command,command,831,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:907,usability,user,users,907,"Not all shards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complet",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1007,usability,user,users,1007,"ards generated; Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash. singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1195,usability,User,UserWarning,1195,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1308,usability,minim,minimal,1308,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:1825,usability,user,users,1825,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:2045,usability,error,errored,2045,"/deepvariant:1.6.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${6} \. --ref=./human_g1k_v37_decoy.fasta \. --reads=./${2}_md.recal.cram \. --output_vcf=./${2}_hg37.dv.vcf.gz \. --output_gvcf=./${2}_hg37.dv.g.vcf.gz \. --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \. --num_shards=32. ```. Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024. Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(. I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started. W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records. I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants. ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards. Erin.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/777:1026,deployability,modul,module,1026,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:4,energy efficiency,model,model,4,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:16,energy efficiency,model,model,16,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:22,energy efficiency,model,model,22,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:208,energy efficiency,model,model,208,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:422,energy efficiency,model,model,422,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:428,energy efficiency,model,model,428,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1610,energy efficiency,model,model,1610,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1673,energy efficiency,model,model,1673,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1685,energy efficiency,model,model,1685,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1691,energy efficiency,model,model,1691,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:859,modifiability,interm,intermediate,859,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1026,modifiability,modul,module,1026,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1086,modifiability,pac,packages,1086,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1186,modifiability,pac,packages,1186,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1026,safety,modul,module,1026,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1833,safety,input,input-files-eg-could-not-open,1833,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:4,security,model,model,4,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:16,security,model,model,16,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:22,security,model,model,22,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:208,security,model,model,208,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:422,security,model,model,422,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:428,security,model,model,428,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1610,security,model,model,1610,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1673,security,model,model,1673,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1685,security,model,model,1685,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1691,security,model,model,1691,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:928,testability,Trace,Traceback,928,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:1833,usability,input,input-files-eg-could-not-open,1833,"The model files model/model.ckpt* do not exist; Hi I have an issue with running the DeepVariant v.1.4.0 and v.1.5.0 on RNA-seq data. I tried to follow the example you have here on git, but it cannot find the model.ckpt. What am I doing wrong here? ```. BIN_VERSION=""1.4.0"". docker run \. -v ""$(pwd):$(pwd)"" \. -w ""$(pwd)"" \. google/deepvariant:""${BIN_VERSION}"" \. run_deepvariant \. --model_type=WES \. --customized_model=model/model.ckpt \. --ref=reference/GRCh38.primary_assembly.genome.fa \. --reads=data/VR0024SA.deduped.sortedByCoord.bam \. --output_vcf=output_DeepVariant/VR0024SA.output.vcf.gz \. --num_shards=100 \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --intermediate_results_dir output_DeepVariant/intermediate_results_dir. ```. ```I0228 18:37:53.652402 140377021216576 run_deepvariant.py:342] Re-using the directory for intermediate results in output_DeepVariant/intermediate_results_dir. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 490, in <module>. app.run(main). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run. _run_main(main, args). File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 464, in main. commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 375, in create_all_commands_and_logfiles. check_flags(). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 354, in check_flags. raise RuntimeError('The model files {}* do not exist. Potentially '. RuntimeError: The model files model/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/778:0,interoperability,Compatib,Compatibility,0,"Compatibility of GATK4 gVCF files with DeepVariant for joint calling; Dear developer,. Thank you for providing this wonderful tool! I am wondering about the compatibility of individual-level gVCF files obtained through GATK4 with DeepVariant for joint calling of multiple samples. Specifically, I have numerous individual gVCF files and would like to know if DeepVariant can effectively handle the joint calling process using these gVCFs. Best regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:157,interoperability,compatib,compatibility,157,"Compatibility of GATK4 gVCF files with DeepVariant for joint calling; Dear developer,. Thank you for providing this wonderful tool! I am wondering about the compatibility of individual-level gVCF files obtained through GATK4 with DeepVariant for joint calling of multiple samples. Specifically, I have numerous individual gVCF files and would like to know if DeepVariant can effectively handle the joint calling process using these gVCFs. Best regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:281,interoperability,Specif,Specifically,281,"Compatibility of GATK4 gVCF files with DeepVariant for joint calling; Dear developer,. Thank you for providing this wonderful tool! I am wondering about the compatibility of individual-level gVCF files obtained through GATK4 with DeepVariant for joint calling of multiple samples. Specifically, I have numerous individual gVCF files and would like to know if DeepVariant can effectively handle the joint calling process using these gVCFs. Best regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:126,usability,tool,tool,126,"Compatibility of GATK4 gVCF files with DeepVariant for joint calling; Dear developer,. Thank you for providing this wonderful tool! I am wondering about the compatibility of individual-level gVCF files obtained through GATK4 with DeepVariant for joint calling of multiple samples. Specifically, I have numerous individual gVCF files and would like to know if DeepVariant can effectively handle the joint calling process using these gVCFs. Best regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:375,usability,effectiv,effectively,375,"Compatibility of GATK4 gVCF files with DeepVariant for joint calling; Dear developer,. Thank you for providing this wonderful tool! I am wondering about the compatibility of individual-level gVCF files obtained through GATK4 with DeepVariant for joint calling of multiple samples. Specifically, I have numerous individual gVCF files and would like to know if DeepVariant can effectively handle the joint calling process using these gVCFs. Best regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/779:665,availability,state,state,665,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:177,energy efficiency,power,powerful,177,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:764,energy efficiency,reduc,reduced,764,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:820,energy efficiency,reduc,reduced,820,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:369,integrability,filter,filters,369,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:450,integrability,Filter,Filters,450,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:665,integrability,state,state,665,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:970,interoperability,share,share,970,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:247,performance,perform,performed,247,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:29,safety,isol,isolate,29,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:262,safety,test,tests,262,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:898,safety,isol,isolate,898,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:29,security,iso,isolate,29,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:109,security,ident,identification,109,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:898,security,iso,isolate,898,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:29,testability,isol,isolate,29,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:262,testability,test,tests,262,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:898,testability,isol,isolate,898,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:247,usability,perform,performed,247,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:328,usability,workflow,workflow,328,"DeepTrio from 1.6.0 : how to isolate de novo calls from trio VCF ?; Hi, . I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". . I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. . Franois.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/780:0,availability,Error,Errors,0,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:741,availability,error,errors,741,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1076,availability,down,downloaded,1076,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1286,availability,down,downloaded,1286,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:344,energy efficiency,model,model,344,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:887,energy efficiency,model,model,887,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:893,energy efficiency,model,model,893,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1091,energy efficiency,model,model,1091,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1178,energy efficiency,model,model,1178,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1185,energy efficiency,model,model,1185,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1216,energy efficiency,model,model,1216,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1245,energy efficiency,model,model,1245,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1262,energy efficiency,model,model,1262,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1362,energy efficiency,model,models,1362,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1439,energy efficiency,model,model,1439,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1472,energy efficiency,model,model,1472,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1478,energy efficiency,model,model,1478,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1558,energy efficiency,model,models,1558,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1635,energy efficiency,model,model,1635,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1666,energy efficiency,model,model,1666,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1672,energy efficiency,model,model,1672,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1750,energy efficiency,model,models,1750,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1827,energy efficiency,model,model,1827,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1846,energy efficiency,model,model,1846,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1852,energy efficiency,model,model,1852,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1918,energy efficiency,model,models,1918,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1995,energy efficiency,model,model,1995,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:2013,energy efficiency,model,model,2013,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:2019,energy efficiency,model,model,2019,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:0,performance,Error,Errors,0,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:741,performance,error,errors,741,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:0,safety,Error,Errors,0,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:741,safety,error,errors,741,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:344,security,model,model,344,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:887,security,model,model,887,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:893,security,model,model,893,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1091,security,model,model,1091,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1178,security,model,model,1178,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1185,security,model,model,1185,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1216,security,model,model,1216,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1245,security,model,model,1245,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1262,security,model,model,1262,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1362,security,model,models,1362,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1439,security,model,model,1439,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1472,security,model,model,1472,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1478,security,model,model,1478,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1558,security,model,models,1558,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1635,security,model,model,1635,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1666,security,model,model,1666,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1672,security,model,model,1672,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1750,security,model,models,1750,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1827,security,model,model,1827,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1846,security,model,model,1846,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1852,security,model,model,1852,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1918,security,model,models,1918,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1995,security,model,model,1995,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:2013,security,model,model,2013,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:2019,security,model,model,2019,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:0,usability,Error,Errors,0,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:741,usability,error,errors,741,"Errors running DeepVariant with Singularity; Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/780:1047,usability,command,command,1047," have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} . ```. But I get the following errors:. ```. /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. I've downloaded the model.ckpt files:. ```. ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model. model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta. ```. I downloaded them with:. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. Please adv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/780
https://github.com/google/deepvariant/issues/781:802,deployability,version,version,802,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:8,energy efficiency,model,model,8,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:42,energy efficiency,model,model,42,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:102,energy efficiency,model,models,102,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:179,energy efficiency,model,model,179,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:212,energy efficiency,model,model,212,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:218,energy efficiency,model,model,218,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:298,energy efficiency,model,models,298,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:375,energy efficiency,model,model,375,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:406,energy efficiency,model,model,406,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:412,energy efficiency,model,model,412,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:490,energy efficiency,model,models,490,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:567,energy efficiency,model,model,567,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:586,energy efficiency,model,model,586,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:592,energy efficiency,model,model,592,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:658,energy efficiency,model,models,658,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:735,energy efficiency,model,model,735,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:753,energy efficiency,model,model,753,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:759,energy efficiency,model,model,759,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:802,integrability,version,version,802,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:14,interoperability,compatib,compatibility,14,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:781,interoperability,compatib,compatible,781,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:802,modifiability,version,version,802,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:8,security,model,model,8,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:42,security,model,model,42,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:102,security,model,models,102,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:179,security,model,model,179,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:212,security,model,model,212,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:218,security,model,model,218,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:298,security,model,models,298,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:375,security,model,model,375,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:406,security,model,model,406,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:412,security,model,model,412,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:490,security,model,models,490,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:567,security,model,model,567,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:586,security,model,model,586,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:592,security,model,model,592,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:658,security,model,models,658,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:735,security,model,model,735,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:753,security,model,model,753,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:759,security,model,model,759,"RNA-seq model compatibility; Hi,. Is this model. ```. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```. compatible with this version of hg38 refdata-gex-GRCh38-2020-A, 10X Genomics? Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/782:0,availability,Error,Error,0,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:764,availability,error,error,764,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:892,availability,error,error,892,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1258,availability,error,error,1258,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:770,deployability,log,log,770,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:943,deployability,modul,module,943,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:967,deployability,API,API,967,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:971,deployability,version,version,971,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:993,deployability,version,version,993,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1039,deployability,API,API,1039,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1171,deployability,api,api-incompatibility,1171,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:287,energy efficiency,model,model,287,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:667,energy efficiency,core,cores,667,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:731,energy efficiency,CPU,CPU,731,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:735,energy efficiency,core,cores,735,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:967,integrability,API,API,967,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:971,integrability,version,version,971,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:993,integrability,version,version,993,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1039,integrability,API,API,1039,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1171,integrability,api,api-incompatibility,1171,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:913,interoperability,compatib,compatibility,913,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:967,interoperability,API,API,967,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1039,interoperability,API,API,1039,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1043,interoperability,incompatib,incompatibility,1043,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1171,interoperability,api,api-incompatibility,1171,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:943,modifiability,modul,module,943,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:971,modifiability,version,version,971,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:993,modifiability,version,version,993,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:0,performance,Error,Error,0,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:731,performance,CPU,CPU,731,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:764,performance,error,error,764,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:892,performance,error,error,892,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1258,performance,error,error,1258,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:0,safety,Error,Error,0,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:764,safety,error,error,764,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:770,safety,log,log,770,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:892,safety,error,error,892,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:943,safety,modul,module,943,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1258,safety,error,error,1258,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:287,security,model,model,287,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:770,security,log,log,770,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:770,testability,log,log,770,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:0,usability,Error,Error,0,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:764,usability,error,error,764,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:892,usability,error,error,892,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1131,usability,user,user,1131,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1195,usability,indicat,indications,1195,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:1258,usability,error,error,1258,"Error running DeepVariant; Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. The error log is attached. . [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/783:0,availability,Error,Error,0,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:994,availability,error,error,994,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1419,availability,operat,operations,1419,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1465,availability,operat,operations,1465,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:341,energy efficiency,model,model,341,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:721,energy efficiency,core,cores,721,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:785,energy efficiency,CPU,CPU,785,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:789,energy efficiency,core,cores,789,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1232,energy efficiency,core,core,1232,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1298,energy efficiency,optim,optimized,1298,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1378,energy efficiency,CPU,CPU,1378,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1737,energy efficiency,model,model,1737,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1743,energy efficiency,model,model,1743,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1237,interoperability,platform,platform,1237,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:0,performance,Error,Error,0,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:785,performance,CPU,CPU,785,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:994,performance,error,error,994,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1173,performance,cach,cached,1173,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1298,performance,optimiz,optimized,1298,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1332,performance,Network,Network,1332,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1378,performance,CPU,CPU,1378,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1398,performance,perform,performance-critical,1398,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:0,safety,Error,Error,0,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:994,safety,error,error,994,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1078,safety,compl,complete,1078,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:341,security,model,model,341,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1078,security,compl,complete,1078,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1332,security,Network,Network,1332,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1737,security,model,model,1737,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1743,security,model,model,1743,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:0,usability,Error,Error,0,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:69,usability,command,command,69,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:994,usability,error,error,994,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1398,usability,perform,performance-critical,1398,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1559,usability,help,helpshort,1559,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1574,usability,help,helpfull,1574,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1590,usability,help,help,1590,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:1897,usability,command,command,1897,"Error running DeepVariant via singularity; Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```. singularity run -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```. Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```. INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). INFO: Using cached SIF image. 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. --ref is required. Pass --helpshort or --helpfull to see help on flags. /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory. /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found. ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/784:166,integrability,filter,filtered,166,"Somatic and germline variants; Hi,. Does DeepVariant attempt to distinguish germline and somatic variants? If the VAF of a variant is low, will that mutation be then filtered out or does the tool report only variants of which VAF is close to 0.5 (heterozygous) or 1 (homozygous)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:36,reliability,Doe,Does,36,"Somatic and germline variants; Hi,. Does DeepVariant attempt to distinguish germline and somatic variants? If the VAF of a variant is low, will that mutation be then filtered out or does the tool report only variants of which VAF is close to 0.5 (heterozygous) or 1 (homozygous)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:182,reliability,doe,does,182,"Somatic and germline variants; Hi,. Does DeepVariant attempt to distinguish germline and somatic variants? If the VAF of a variant is low, will that mutation be then filtered out or does the tool report only variants of which VAF is close to 0.5 (heterozygous) or 1 (homozygous)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:191,usability,tool,tool,191,"Somatic and germline variants; Hi,. Does DeepVariant attempt to distinguish germline and somatic variants? If the VAF of a variant is low, will that mutation be then filtered out or does the tool report only variants of which VAF is close to 0.5 (heterozygous) or 1 (homozygous)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:233,usability,close,close,233,"Somatic and germline variants; Hi,. Does DeepVariant attempt to distinguish germline and somatic variants? If the VAF of a variant is low, will that mutation be then filtered out or does the tool report only variants of which VAF is close to 0.5 (heterozygous) or 1 (homozygous)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/785:112,energy efficiency,model,model,112,"model_type for ONT WGS data; Hi, . Thanks for developing this package. It looks like there are both WGS and ONT model type, and I am wondering what model type is recommended for ONT WGS data? . Best,. YM.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/785:148,energy efficiency,model,model,148,"model_type for ONT WGS data; Hi, . Thanks for developing this package. It looks like there are both WGS and ONT model type, and I am wondering what model type is recommended for ONT WGS data? . Best,. YM.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/785:62,modifiability,pac,package,62,"model_type for ONT WGS data; Hi, . Thanks for developing this package. It looks like there are both WGS and ONT model type, and I am wondering what model type is recommended for ONT WGS data? . Best,. YM.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/785:112,security,model,model,112,"model_type for ONT WGS data; Hi, . Thanks for developing this package. It looks like there are both WGS and ONT model type, and I am wondering what model type is recommended for ONT WGS data? . Best,. YM.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/785:148,security,model,model,148,"model_type for ONT WGS data; Hi, . Thanks for developing this package. It looks like there are both WGS and ONT model type, and I am wondering what model type is recommended for ONT WGS data? . Best,. YM.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/786:429,availability,error,error,429,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:698,availability,error,error,698,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:720,availability,Operat,Operating,720,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1464,availability,Error,Error,1464,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:34,deployability,fail,fails,34,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:413,deployability,fail,failed,413,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:788,deployability,version,version,788,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:807,deployability,Instal,Installation,807,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1165,deployability,log,logs-parallel,1165,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1193,deployability,log,log,1193,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1209,deployability,log,logs-parallel,1209,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:314,energy efficiency,reduc,reduce,314,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:435,integrability,messag,message,435,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:788,integrability,version,version,788,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:304,interoperability,format,format,304,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:435,interoperability,messag,message,435,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:788,modifiability,version,version,788,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:0,performance,Parallel,Parallel,0,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:254,performance,parallel,parallel,254,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:400,performance,parallel,parallel,400,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:429,performance,error,error,429,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:515,performance,parallel,parallel,515,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:540,performance,parallel,parallel,540,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:563,performance,parallel,parallel,563,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:589,performance,parallel,parallel,589,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:698,performance,error,error,698,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1104,performance,parallel,parallel,1104,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1170,performance,parallel,parallel,1170,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1214,performance,parallel,parallel,1214,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1464,performance,Error,Error,1464,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:34,reliability,fail,fails,34,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:413,reliability,fail,failed,413,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:429,safety,error,error,429,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:698,safety,error,error,698,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1165,safety,log,logs-parallel,1165,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1193,safety,log,log,1193,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1209,safety,log,logs-parallel,1209,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1464,safety,Error,Error,1464,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1165,security,log,logs-parallel,1165,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1193,security,log,log,1193,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1209,security,log,logs-parallel,1209,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:161,testability,simpl,simple,161,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:915,testability,instrument,instrument,915,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1165,testability,log,logs-parallel,1165,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1193,testability,log,log,1193,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1209,testability,log,logs-parallel,1209,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1470,testability,trace,trace,1470,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:161,usability,simpl,simple,161,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:389,usability,command,command,389,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:429,usability,error,error,429,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:698,usability,error,error,698,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1061,usability,Command,Command,1061,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:1464,usability,Error,Error,1464,"Parallel execution of DeepVariant fails when using CRAM alignment files.; **Describe the issue:**. I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : . 1. non-parallel + bam . 2. non-parallel + cram  . 3. parallel + bam  . 4. non-parallel + cram  . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**. - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64. - DeepVariant version: v1.6.0. - Installation method (Docker, built from source, etc.): HPC, sorry I don't know. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Not special, I used common toy data. **Steps to reproduce:**. - Command: . ```. seq 0 $((N_SHARDS-1)) \. | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \. --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \. make_examples --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples output/examples.tfrecord@${N_SHARDS}.gz\. --channels insert_size \. --task {} \. || exit 1. ```. - Error trace: (if applicable). ```. META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/pull/787:29,security,team,team,29,For 1.6.1; This PR is from a team member.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/787
https://github.com/google/deepvariant/issues/788:439,deployability,version,versions,439,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:502,energy efficiency,model,model,502,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:615,energy efficiency,model,model,615,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:439,integrability,version,versions,439,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:439,modifiability,version,versions,439,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:756,modifiability,extens,extensively,756,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:460,reliability,Doe,Does,460,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:535,safety,safe,safe,535,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:569,safety,valid,validate,569,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:659,safety,safe,safe,659,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:502,security,model,model,502,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:569,security,validat,validate,569,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:593,security,NIST,NIST,593,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:615,security,model,model,615,"Question about retraining DeepVariant; Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/789:418,availability,down,downloading,418,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:43,deployability,version,version,43,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:108,deployability,build,build,108,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:136,deployability,instal,install,136,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:290,deployability,version,version,290,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,deployability,depend,depending,369,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:391,deployability,build,build,391,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:440,deployability,version,versions,440,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:480,deployability,version,version,480,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:710,deployability,version,version,710,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:592,energy efficiency,GPU,GPU,592,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:43,integrability,version,version,43,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:290,integrability,version,version,290,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,integrability,depend,depending,369,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:440,integrability,version,versions,440,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:480,integrability,version,version,480,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:710,integrability,version,version,710,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:35,interoperability,specif,specify,35,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:545,interoperability,conflict,conflicts,545,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:652,interoperability,specif,specifically,652,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:665,interoperability,specif,specify,665,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:43,modifiability,version,version,43,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:211,modifiability,pac,package,211,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:290,modifiability,version,version,290,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,modifiability,depend,depending,369,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:440,modifiability,version,versions,440,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:480,modifiability,version,version,480,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:710,modifiability,version,version,710,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:592,performance,GPU,GPU,592,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,safety,depend,depending,369,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,testability,depend,depending,369,"Don't use nvidia-tensorrt. Instead specify version with tensorrt==8.5.3.1; The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is. https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/790:771,availability,avail,available,771,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:127,energy efficiency,model,model,127,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:167,energy efficiency,model,models,167,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:512,energy efficiency,model,model,512,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:731,energy efficiency,model,model,731,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:866,energy efficiency,model,model,866,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:966,energy efficiency,model,model,966,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:872,interoperability,architectur,architectures,872,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:771,reliability,availab,available,771,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:425,safety,input,input,425,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:485,safety,test,test,485,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:771,safety,avail,available,771,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:127,security,model,model,127,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:167,security,model,models,167,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:512,security,model,model,512,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:731,security,model,model,731,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:771,security,availab,available,771,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:866,security,model,model,866,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:966,security,model,model,966,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:485,testability,test,test,485,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:29,usability,custom,custom,29,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:348,usability,command,command,348,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:413,usability,workflow,workflow,413,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:425,usability,input,input,425,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:431,usability,custom,custom,431,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:895,usability,learn,learn,895,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:1175,usability,document,documentation,1175,"Question: How to train using custom tags in BAM as features?; Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose? 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features. 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed? Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/791:200,integrability,repositor,repository,200,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:200,interoperability,repositor,repository,200,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:406,performance,perform,perform,406,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:680,performance,perform,perform,680,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:737,reliability,Doe,Does,737,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:150,usability,tool,tools,150,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:406,usability,perform,perform,406,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:680,usability,perform,perform,680,"RNA-seq pre-processing; Hi,. A few questions about RNA-seq pre-processing before DeepVariant? - Do you recommend read trimming before alignment using tools such as fastp? - I can see that this is the repository for the human reference genome, which DeepVariant recommends. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/ . But when I perform read alignment using STAR, can I use GCA_000001405.15_GRCh38_full_analysis_set.fna instead of GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz? GCA_000001405.15_GRCh38_full_analysis_set.fna has the GTF file, which is needed for creating the STAR index. - Or can I perform the alignment using any GRCh38 reference genome? Does it have to be the same one, GCA_000001405.15_GRCh38_no_alt_analysis_set, used in the RNA-seq tutorial of DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/792:446,deployability,build,build,446,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:524,deployability,build,build-test,524,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:851,interoperability,share,share,851,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:530,safety,test,test,530,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:35,security,modif,modifying,35,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:398,testability,understand,understand,398,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:530,testability,test,test,530,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:83,usability,custom,custom,83,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:207,usability,custom,custom-channels,207,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:452,usability,guid,guide,452,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:862,usability,tip,tips,862,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:912,usability,guid,guide,912,"Developers notes / environment for modifying code; Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/793:466,availability,down,downloaded,466,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:546,availability,error,error,546,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:955,availability,error,errors,955,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2092,availability,error,error,2092," shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4067,availability,error,error,4067,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4611,availability,error,error,4611,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:35,deployability,version,versions,35,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:360,deployability,stage,stage,360,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:488,deployability,version,version,488,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:626,deployability,instal,install,626,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:653,deployability,version,version,653,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:702,deployability,version,version,702,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:774,deployability,version,version,774,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:809,deployability,instal,installed,809,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:828,deployability,version,version,828,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:892,deployability,instal,install,892,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1007,deployability,version,versions,1007,"ipt not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1057,deployability,instal,installed,1057,"ages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1222,deployability,pipelin,pipeline,1222,"anced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2270,deployability,modul,module,2270,"to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2429,deployability,modul,module,2429,"osomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2585,deployability,modul,module,2585,"ple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_imp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2638,deployability,Modul,ModuleNotFoundError,2638," for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2662,deployability,modul,module,2662,"sed Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2915,deployability,modul,module,2915,"y much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. rai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3075,deployability,modul,module,3075,". Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3246,deployability,modul,module,3246,"__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3424,deployability,modul,module,3424," <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3608,deployability,modul,module,3608,"multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3755,deployability,modul,module,3755,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3906,deployability,modul,module,3906,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4054,deployability,fail,failed,4054,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4157,deployability,instal,installed,4157,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4351,deployability,version,version,4351,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4455,deployability,version,version,4455,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4509,deployability,version,versions,4509,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4625,deployability,modul,module,4625,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:95,energy efficiency,model,model,95,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:694,energy efficiency,current,current,694,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2238,energy efficiency,core,core,2238,"tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2395,energy efficiency,core,core,2395,"cuments, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coder",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2553,energy efficiency,core,core,2553,"training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2605,energy efficiency,core,core,2605,"ntly for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2682,energy efficiency,core,core,2682,"r my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3778,energy efficiency,core,core,3778,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3874,energy efficiency,core,core,3874,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4645,energy efficiency,core,core,4645,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:35,integrability,version,versions,35,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:488,integrability,version,version,488,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:653,integrability,version,version,653,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:702,integrability,version,version,702,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:774,integrability,version,version,774,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:828,integrability,version,version,828,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1007,integrability,version,versions,1007,"ipt not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1222,integrability,pipelin,pipeline,1222,"anced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4351,integrability,version,version,4351,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4455,integrability,version,version,4455,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4509,integrability,version,versions,4509,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:19,interoperability,compatib,compatible,19,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:672,interoperability,incompatib,incompatible,672,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:850,interoperability,compatib,compatible,850,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:35,modifiability,version,versions,35,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:58,modifiability,pac,packages,58,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:488,modifiability,version,version,488,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:653,modifiability,version,version,653,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:702,modifiability,version,version,702,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:774,modifiability,version,version,774,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:828,modifiability,version,version,828,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1007,modifiability,version,versions,1007,"ipt not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2223,modifiability,pac,packages,2223,"ine from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2270,modifiability,modul,module,2270,"to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2380,modifiability,pac,packages,2380,"ts. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2429,modifiability,modul,module,2429,"osomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2538,modifiability,pac,packages,2538,"_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2585,modifiability,modul,module,2585,"ple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_imp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2638,modifiability,Modul,ModuleNotFoundError,2638," for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2662,modifiability,modul,module,2662,"sed Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2915,modifiability,modul,module,2915,"y much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. rai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3027,modifiability,pac,packages,3027,"preciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3075,modifiability,modul,module,3075,". Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3191,modifiability,pac,packages,3191,"sorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some commo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3246,modifiability,modul,module,3246,"__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3371,modifiability,pac,packages,3371,"site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3424,modifiability,modul,module,3424," <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3551,modifiability,pac,packages,3551,"re/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3608,modifiability,modul,module,3608,"multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3712,modifiability,pac,packages,3712,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3755,modifiability,modul,module,3755,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3859,modifiability,pac,packages,3859,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3906,modifiability,modul,module,3906,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4043,modifiability,extens,extensions,4043,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4351,modifiability,version,version,4351,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4455,modifiability,version,version,4455,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4509,modifiability,version,versions,4509,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4625,modifiability,modul,module,4625,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:546,performance,error,error,546,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:955,performance,error,errors,955,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1592,performance,time,times,1592,"and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1934,performance,time,time,1934," getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apach",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2092,performance,error,error,2092," shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4067,performance,error,error,4067,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4611,performance,error,error,4611,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4054,reliability,fail,failed,4054,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:546,safety,error,error,546,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:955,safety,error,errors,955,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1371,safety,valid,validation,1371,"ed to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1629,safety,valid,validation,1629,"because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1726,safety,valid,validation,1726,"was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2092,safety,error,error,2092," shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2270,safety,modul,module,2270,"to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2429,safety,modul,module,2429,"osomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2585,safety,modul,module,2585,"ple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_imp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2638,safety,Modul,ModuleNotFoundError,2638," for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2662,safety,modul,module,2662,"sed Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2736,safety,except,exception,2736," should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2755,safety,except,exception,2755,"ted runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <modul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2915,safety,modul,module,2915,"y much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. rai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3075,safety,modul,module,3075,". Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3246,safety,modul,module,3246,"__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3424,safety,modul,module,3424," <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3608,safety,modul,module,3608,"multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3755,safety,modul,module,3755,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3906,safety,modul,module,3906,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4067,safety,error,error,4067,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4611,safety,error,error,4611,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4625,safety,modul,module,4625,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:95,security,model,model,95,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1371,security,validat,validation,1371,"ed to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1629,security,validat,validation,1629,"because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1726,security,validat,validation,1726,"was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1201,testability,understand,understanding,1201,"long with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2098,testability,trace,traceback,2098," script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam im",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2112,testability,Trace,Traceback,2112,"uidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2776,testability,Trace,Traceback,2776,"ifferent chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:546,usability,error,error,546,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:955,usability,error,errors,955,"Shuffle script not compatible with versions of tensorflow packages? ; Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1116,usability,guidanc,guidance,1116,"ly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Tracebac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1190,usability,confirm,confirm,1190,"es following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1395,usability,document,documents,1395,"e training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1976,usability,clear,clearly,1976,"u have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2092,usability,error,error,2092," shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. . First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, . Haley . Here is the error traceback: . `Traceback (most recent call last):. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4067,usability,error,error,4067,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4225,usability,tip,tips,4225,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4261,usability,user,user,4261,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4557,usability,document,documentation,4557,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4596,usability,help,help,4596,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4611,usability,error,error,4611,"amed 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>. import apache_beam as beam. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>. from apache_beam import coders. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>. from apache_beam.coders.coders import *. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>. from apache_beam.coders import coder_impl. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>. import numpy as np. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>. from . import core. File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3"". * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/794:13,availability,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:33,availability,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:233,availability,error,error,233,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:253,availability,fault,fault,253,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:292,availability,Operat,Operating,292,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:688,availability,error,error,688,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1303,availability,Error,Error,1303,"em: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]. I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]. I0325 17:32:28.232726 47276001879872 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8301,availability,error,error,8301,"7424450524992 make_examples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8321,availability,fault,fault,8321,"xamples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:330,deployability,version,version,330,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:348,deployability,Instal,Installation,348,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9614,deployability,modul,module,9614," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9767,deployability,fail,failed,9767," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:33,energy efficiency,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:253,energy efficiency,fault,fault,253,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8321,energy efficiency,fault,fault,8321,"xamples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8328,energy efficiency,Current,Current,8328,"core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:330,integrability,version,version,330,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:330,modifiability,version,version,330,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:526,modifiability,Pac,PacBio,526,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:971,modifiability,PAC,PACBIO,971,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9614,modifiability,modul,module,9614," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:13,performance,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:33,performance,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:233,performance,error,error,233,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:253,performance,fault,fault,253,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:688,performance,error,error,688,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1303,performance,Error,Error,1303,"em: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]. I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]. I0325 17:32:28.232726 47276001879872 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8301,performance,error,error,8301,"7424450524992 make_examples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8321,performance,fault,fault,8321,"xamples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9748,performance,parallel,parallel,9748," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:33,reliability,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:253,reliability,fault,fault,253,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8321,reliability,fault,fault,8321,"xamples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9767,reliability,fail,failed,9767," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9783,reliability,Doe,Does,9783," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:13,safety,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:33,safety,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:233,safety,error,error,233,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:253,safety,fault,fault,253,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:688,safety,error,error,688,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:873,safety,input,input,873,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:985,safety,input,input,985,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1007,safety,input,input,1007,"on error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1303,safety,Error,Error,1303,"em: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]. I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]. I0325 17:32:28.232726 47276001879872 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8301,safety,error,error,8301,"7424450524992 make_examples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8321,safety,fault,fault,8321,"xamples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9614,safety,modul,module,9614," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9804,safety,test,test,9804," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9840,safety,test,test,9840," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:444,testability,instrument,instrument,444,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1309,testability,trace,trace,1309,"nt. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]. I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]. I0325 17:32:28.232726 47276001879872 make_examples_core.py:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9804,testability,test,test,9804," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:9840,testability,test,test,9840," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:10014,testability,context,context,10014," elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>. I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]. parallel: This job failed:. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:13,usability,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:166,usability,clear,clear,166,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:233,usability,error,error,233,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:688,usability,error,error,688,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:722,usability,Command,Command,722,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:873,usability,input,input,873,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:985,usability,input,input,985,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed].",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1007,usability,input,input,1007,"on error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**. - Operating system: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1303,usability,Error,Error,1303,"em: Cent. - DeepVariant version: 1.6.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**. - Command:. ```bash. #!/bin/bash. sample=$1. threads=$2. chr=$3. indir=""01.mapping"". outdir=""02.snps"". sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}"". rm -rf ${outdir}/${sample}_chr${chr}. ```. - Error trace: (if applicable). ```bash. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]. I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]. I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]. I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]. I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]. I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]. I0325 17:32:28.232726 47276001879872 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:8301,usability,error,error,8301,"7424450524992 make_examples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]. I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]. I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]. I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]. I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]. I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]. I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]. I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]. Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner. File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/795:202,availability,error,error,202,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:839,availability,error,error,839,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:336,deployability,modul,module,336,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:463,deployability,modul,module,463,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:598,deployability,modul,module,598,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:651,deployability,Modul,ModuleNotFoundError,651,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:675,deployability,modul,module,675,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:947,deployability,version,version,947,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:986,deployability,manag,managed,986,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1391,deployability,contain,containers,1391,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:304,energy efficiency,core,core,304,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:429,energy efficiency,core,core,429,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:566,energy efficiency,core,core,566,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:618,energy efficiency,core,core,618,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:695,energy efficiency,core,core,695,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:986,energy efficiency,manag,managed,986,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:947,integrability,version,version,947,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:955,interoperability,incompatib,incompatibility,955,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:289,modifiability,pac,packages,289,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:336,modifiability,modul,module,336,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:414,modifiability,pac,packages,414,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:463,modifiability,modul,module,463,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:551,modifiability,pac,packages,551,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:598,modifiability,modul,module,598,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:651,modifiability,Modul,ModuleNotFoundError,651,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:675,modifiability,modul,module,675,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:947,modifiability,version,version,947,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:202,performance,error,error,202,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:839,performance,error,error,839,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:202,safety,error,error,202,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:336,safety,modul,module,336,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:463,safety,modul,module,463,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:598,safety,modul,module,598,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:651,safety,Modul,ModuleNotFoundError,651,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:675,safety,modul,module,675,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:839,safety,error,error,839,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:986,safety,manag,managed,986,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1422,security,privil,privileges,1422,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:210,testability,Trace,Traceback,210,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1018,testability,simpl,simply,1018,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:87,usability,tool,tool,87,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:202,usability,error,error,202,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:839,usability,error,error,839,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1018,usability,simpl,simply,1018,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1276,usability,document,documentation,1276,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:1312,usability,user,users,1312,"Singularity import issue and possible solution; Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>. from . import multiarray. File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>. from numpy.core._multiarray_umath import (. ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/796:326,integrability,filter,filters,326,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:457,integrability,filter,filters,457,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:586,integrability,filter,filtering,586,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:1058,integrability,filter,filters,1058,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:1154,integrability,filter,filters,1154,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:13,modifiability,Pac,Pacbio,13,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/796:40,modifiability,Pac,Pacbio,40,"How to merge Pacbio gvcfs?; I generated Pacbio gvcfs of 30 samples using Deepvariant. And now I want to merge those gvcfs using GLnexus, what --config should I choose? Is ""DeepVariant"" suitable? Name CRC32C	Description. gatk 1926883223	Joint-call GATK-style gVCFs. gatk_unfiltered 4039280095	Merge GATK-style gVCFs with no QC filters or genotype revision. xAtlas 1991666133	Joint-call xAtlas gVCFs. xAtlas_unfiltered 221875257	Merge xAtlas gVCFs with no QC filters or genotype revision. weCall 2898360729	Joint-call weCall gVCFs. weCall_unfiltered 4254257210	Merge weCall gVCFs with no filtering or genotype revision. DeepVariant 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWGS 2932316105	Joint call DeepVariant whole genome sequencing gVCFs. DeepVariantWES 1063427682	Joint call DeepVariant whole exome sequencing gVCFs. DeepVariantWES_MED_DP 2412618877	Joint call DeepVariant whole exome sequencing gVCFs, populating 0/0 DP from MED_DP instead of MIN_DP. DeepVariant_unfiltered 3285998180	Merge DeepVariant gVCFs with no QC filters or genotype revision. Strelka2 395868656	[EXPERIMENTAL] Merge Strelka2 gVCFs with no QC filters or genotype revision.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/797:175,availability,error,errors,175,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:352,availability,cluster,cluster,352,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:977,availability,error,error,977,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1908,availability,error,error,1908,"ed to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1964,availability,error,error,1964,"ll as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2975,availability,checkpoint,checkpoints,2975,"placed by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3862,availability,error,error,3862,"eepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3917,availability,error,error,3917,"t_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4662,availability,checkpoint,checkpoints,4662,"s. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5071,availability,Error,Error,5071,"sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5798,availability,checkpoint,checkpoints,5798,"90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorph",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9601,availability,error,error,9601,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9680,availability,State,StatefulPartitionedCall,9680,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:352,deployability,cluster,cluster,352,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:602,deployability,fail,fail,602,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1788,deployability,FAIL,FAIL,1788," fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2034,deployability,MODUL,MODULES,2034,"he output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2475,deployability,modul,module,2475,"-gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 proce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3743,deployability,FAIL,FAIL,3743,"ant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3987,deployability,MODUL,MODULES,3987,"deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Sa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4428,deployability,modul,module,4428,"f nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7496,deployability,modul,module,7496,"t):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:21,energy efficiency,model,model,21,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:144,energy efficiency,model,model,144,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:313,energy efficiency,model,model,313,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:402,energy efficiency,gpu,gpu,402,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:460,energy efficiency,model,model,460,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:708,energy efficiency,model,model,708,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:927,energy efficiency,model,model,927,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:955,energy efficiency,model,model,955,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1335,energy efficiency,model,model,1335,"remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1479,energy efficiency,gpu,gpus-per-node,1479,"appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1510,energy efficiency,core,core,1510,"t seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1543,energy efficiency,core,core,1543,"iants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1569,energy efficiency,gpu,gpu,1569,"rning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --conf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2029,energy efficiency,LOAD,LOAD,2029,"ttach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2482,energy efficiency,load,load,2482,"per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2950,energy efficiency,model,models,2950,"b standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3306,energy efficiency,model,model,3306,"MPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3483,energy efficiency,core,core,3483,"d apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3516,energy efficiency,core,core,3516,"ariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3982,energy efficiency,LOAD,LOAD,3982,"ts/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4435,energy efficiency,load,load,4435,"s. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5631,energy efficiency,model,model,5631,"t_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6276,energy efficiency,model,model,6276,"y.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6420,energy efficiency,model,model,6420,"put examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8126,energy efficiency,predict,predictions,8126,"ured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._buil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8140,energy efficiency,model,model,8140,"ure_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8866,energy efficiency,load,load,8866,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8176,integrability,batch,batch,8176," signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9649,integrability,filter,filter,9649,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9680,integrability,State,StatefulPartitionedCall,9680,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:392,interoperability,specif,specify,392,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1575,interoperability,standard,standard,1575,"fter which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90da",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1852,interoperability,standard,standard,1852," would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1955,interoperability,standard,standard,1955,"l, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/De",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3550,interoperability,standard,standard,3550,"n/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --cust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3806,interoperability,standard,standard,3806,"0daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90dayd",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3908,interoperability,standard,standard,3908,"nfig.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2034,modifiability,MODUL,MODULES,2034,"he output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2475,modifiability,modul,module,2475,"-gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 proce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3987,modifiability,MODUL,MODULES,3987,"deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Sa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4428,modifiability,modul,module,4428,"f nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6540,modifiability,pac,packages,6540,": [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/hal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6741,modifiability,pac,packages,6741,"aining/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6965,modifiability,pac,packages,6965,"ls.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7496,modifiability,modul,module,7496,"t):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8223,modifiability,pac,packages,8223,"arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8401,modifiability,pac,packages,8401,"iles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8596,modifiability,pac,packages,8596,"_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8827,modifiability,pac,packages,8827,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9009,modifiability,pac,packages,9009,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9215,modifiability,pac,packages,9215,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9378,modifiability,pac,packages,9378,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:175,performance,error,errors,175,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:402,performance,gpu,gpu,402,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:977,performance,error,error,977,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1389,performance,time,time,1389," specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1479,performance,gpu,gpus-per-node,1479,"appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1569,performance,gpu,gpu,1569,"rning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --conf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1908,performance,error,error,1908,"ed to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1964,performance,error,error,1964,"ll as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2029,performance,LOAD,LOAD,2029,"ttach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2482,performance,load,load,2482,"per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3360,performance,time,time,3360,"nvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3862,performance,error,error,3862,"eepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3917,performance,error,error,3917,"t_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3982,performance,LOAD,LOAD,3982,"ts/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4435,performance,load,load,4435,"s. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5071,performance,Error,Error,5071,"sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8176,performance,batch,batch,8176," signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8866,performance,load,load,8866,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9601,performance,error,error,9601,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:593,reliability,doe,does,593,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:602,reliability,fail,fail,602,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:616,reliability,doe,does,616,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1788,reliability,FAIL,FAIL,1788," fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2975,reliability,checkpoint,checkpoints,2975,"placed by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3743,reliability,FAIL,FAIL,3743,"ant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4662,reliability,checkpoint,checkpoints,4662,"s. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5798,reliability,checkpoint,checkpoints,5798,"90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorph",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6302,reliability,doe,does,6302,"cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6,safety,test,testing,6,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:175,safety,error,errors,175,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:277,safety,valid,validation,277,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:301,safety,test,testing,301,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:451,safety,test,test,451,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:692,safety,input,input,692,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:946,safety,test,test,946,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:977,safety,error,error,977,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1000,safety,test,testing,1000,"e testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1908,safety,error,error,1908,"ed to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1964,safety,error,error,1964,"ll as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2034,safety,MODUL,MODULES,2034,"he output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2475,safety,modul,module,2475,"-gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 proce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3290,safety,test,test,3290,"ULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3862,safety,error,error,3862,"eepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3917,safety,error,error,3917,"t_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3987,safety,MODUL,MODULES,3987,"deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Sa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:4428,safety,modul,module,4428,"f nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5071,safety,Error,Error,5071,"sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5378,safety,input,input,5378,"h=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Inp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5421,safety,input,input,5421,"ts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5530,safety,input,input,5530,"el_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5846,safety,input,input,5846,"g/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_str",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5889,safety,input,input,5889,"bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signatu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6103,safety,input,input,6103,"0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6146,safety,input,input,6146,"ariants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_su",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6246,safety,Input,Input,6246,":365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. Du",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6378,safety,Input,Input,6378,"ut examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.ar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7276,safety,except,exception,7276," shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/mono",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7295,safety,except,exception,7295,"] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7496,safety,modul,module,7496,"t):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8126,safety,predict,predictions,8126,"ured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._buil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9601,safety,error,error,9601,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9609,safety,input,input,9609,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:21,security,model,model,21,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:144,security,model,model,144,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:277,security,validat,validation,277,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:313,security,model,model,313,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:460,security,model,model,460,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:708,security,model,model,708,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:927,security,model,model,927,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:955,security,model,model,955,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1335,security,model,model,1335,"remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:2950,security,model,models,2950,"b standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3306,security,model,model,3306,"MPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5631,security,model,model,5631,"t_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6276,security,model,model,6276,"y.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6420,security,model,model,6420,"put examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8140,security,model,model,8140,"ure_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:8146,security,sign,signatures,8146,"ary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6,testability,test,testing,6,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:301,testability,test,testing,301,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:451,testability,test,test,451,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:946,testability,test,test,946,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1000,testability,test,testing,1000,"e testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3290,testability,test,test,3290,"ULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag10",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6468,testability,Trace,Traceback,6468,"06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_varia",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:7316,testability,Trace,Traceback,7316,"27 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>. app.run(main). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __ca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:14,usability,custom,custom,14,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:175,usability,error,errors,175,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:625,usability,progress,progress--just,625,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:692,usability,input,input,692,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:977,usability,error,error,977,"Issue testing custom model ; Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1096,usability,stop,stops,1096,"and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1672,usability,user,user,1672,"ms to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1908,usability,error,error,1908,"ed to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1964,usability,error,error,1964,"ll as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, . Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt). . **Code to train the model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=gpu # standard node(s). #SBATCH --ntasks=48. #SBATCH --job-name=""deepvariant_training"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3299,usability,custom,custom,3299,"PDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \. --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/shei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3627,usability,user,user,3627,"/dv_config.py:base \. --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \. --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3862,usability,error,error,3862,"eepvariant_output/validation_set.pbtxt"" \. --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:3917,usability,error,error,3917,"t_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \. --config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \. --strategy=mirrored \. --config.batch_size=512 . `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas . #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS). #SBATCH --nodes=1 # number of nodes. #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core. #SBATCH --partition=atlas # standard node(s). #SBATCH --job-name=""deepvariant_modeltest"". #SBATCH --mail-user=haley.arnold@usda.gov # email address. #SBATCH --mail-type=BEGIN. #SBATCH --mail-type=END. #SBATCH --mail-type=FAIL. #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id). #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id). #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosom",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5071,usability,Error,Error,5071,"sratoolkit/sratoolkit.2.10.9-centos_linux64/bin. export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR . export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs. softwarepath=/project/ag100pest/sheina.sim/software. slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5378,usability,input,input,5378,"h=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Inp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5421,usability,input,input,5421,"ts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \. --model_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5530,usability,input,input,5530,"el_type WGS \. --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \. --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \. --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5846,usability,input,input,5846,"g/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_str",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:5889,usability,input,input,5889,"bam"" \. --regions ""Chromosome4"" \. --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(. I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signatu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6103,usability,input,input,6103,"0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6146,usability,input,input,6146,"ariants.py:471] Total 1 writing processes started. I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_su",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6246,usability,Input,Input,6246,":365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. Du",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:6378,usability,Input,Input,6378,"ut examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]. I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True. I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl. return self._call_with_structured_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1561, in _call_with_structured_signature. self._structured_signature_check_missing_args(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1581, in _structured_signature_check_missing_args. raise TypeError(f""{self._structured_signature_summary()} missing "". TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/local/scratch/haley.ar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9601,usability,error,error,9601,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:9609,usability,input,input,9609,"n. sys.exit(main(argv)). File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main. call_variants(. File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants. predictions = model.signatures['serving_default'](batch[1]). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__. return self._call_impl(args, kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl. return self._call_with_flat_signature(args, kwargs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature. return self._call_flat(args, self.captured_inputs, cancellation_manager). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat. return super(_WrapperFunction, self)._call_flat(args, captured_inputs,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat. return self._build_call_outputs(self._inference_function.call(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call. outputs = execute.execute(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute. tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,. tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6. [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/798:0,availability,Error,Error,0,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1308,availability,error,error,1308,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1608,availability,down,down,1608,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:821,deployability,modul,module,821,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:846,deployability,modul,module,846,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1584,deployability,fail,failed,1584,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:137,energy efficiency,model,model,137,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:229,energy efficiency,model,model-case-study,229,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:757,energy efficiency,gpu,gpuvolta,757,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:828,energy efficiency,load,load,828,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:853,energy efficiency,load,load,853,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1234,energy efficiency,cpu,cpu-threads-per-stream,1234,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1265,energy efficiency,gpu,gpu-num-per-partition,1265,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:130,modifiability,Pac,PacBio,130,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:222,modifiability,pac,pacbio-model-case-study,222,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:687,modifiability,paramet,parameters,687,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:821,modifiability,modul,module,821,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:846,modifiability,modul,module,846,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:0,performance,Error,Error,0,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:757,performance,gpu,gpuvolta,757,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:828,performance,load,load,828,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:853,performance,load,load,853,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1234,performance,cpu,cpu-threads-per-stream,1234,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1265,performance,gpu,gpu-num-per-partition,1265,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1308,performance,error,error,1308,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1584,reliability,fail,failed,1584,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:0,safety,Error,Error,0,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:821,safety,modul,module,821,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:846,safety,modul,module,846,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1308,safety,error,error,1308,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:137,security,model,model,137,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:229,security,model,model-case-study,229,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:709,security,modif,modify,709,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:0,usability,Error,Error,0,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:720,usability,command,command,720,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:1308,usability,error,error,1308,"Error about expected cur_seq.size() < Max_READ_LEN; Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam. samtools index ./N006942-20231016.bam. However, when I tried to Run DeepVariant on chromosome 20 alignments as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta. #PBS -l ncpus=48. #PBS -l ngpus=4. #PBS -l mem=384GB. module load singularity. module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'. INPUT_BAM='/data/N006942-20231016.bam'. OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant. ulimit -u 100000. singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \. --ref ${REF_FA} \. --in-bam ${INPUT_BAM} \. --out-variants ${OUTPUT_VCF} \. --run-partition \. --num-cpu-threads-per-stream 12 \. --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue? Cheers,.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/799:121,availability,error,error,121,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:207,availability,operat,operations,207,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:238,availability,sli,slightly,238,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:307,availability,error,errors,307,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:443,availability,error,error,443,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:10,energy efficiency,gpu,gpus,10,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:55,energy efficiency,gpu,gpus,55,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:89,energy efficiency,gpu,gpu,89,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:170,energy efficiency,core,core,170,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:477,energy efficiency,gpu,gpus,477,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:580,energy efficiency,gpu,gpus,580,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:632,energy efficiency,gpu,gpus,632,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:387,modifiability,variab,variable,387,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:10,performance,gpu,gpus,10,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:55,performance,gpu,gpus,55,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:89,performance,gpu,gpu,89,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:121,performance,error,error,121,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:307,performance,error,errors,307,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:443,performance,error,error,443,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:477,performance,gpu,gpus,477,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:580,performance,gpu,gpus,580,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:632,performance,gpu,gpus,632,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:238,reliability,sli,slightly,238,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:537,reliability,Doe,Does,537,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:121,safety,error,error,121,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:307,safety,error,errors,307,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:443,safety,error,error,443,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:121,usability,error,error,121,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:200,usability,custom,custom,200,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:307,usability,error,errors,307,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:443,usability,error,error,443,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:458,usability,command,command,458,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:491,usability,help,helpshort,491,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:506,usability,help,helpfull,506,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:522,usability,help,help,522,"Missing --gpus flag in deeptrio; Hi,. When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:. 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. FATAL Flags parsing error: Unknown command line flag 'gpus'. Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/800:24,integrability,filter,filter,24,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:152,integrability,FILTER,FILTER,152,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:207,integrability,filter,filtering,207,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:340,integrability,filter,filtering,340,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:396,integrability,filter,filtering,396,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:130,reliability,doe,does,130,"How to set threshold to filter vcf files; Hello. I have two problem about this:. 1. In a VCF file generated by DeepVariant, what does ""RefCall"" in the FILTER column represent, and should it be removed when filtering VCF files? 2. For PASS-flagged variants in the PASS column, their QUAL values seem relatively small. Are there appropriate filtering thresholds for VCF, and are there recommended filtering criteria for corresponding values in the INFO column to ensure more accurate variant calling? Thanks! ![_17119681423827](https://github.com/google/deepvariant/assets/143433202/e2ee15e6-3448-49e6-a12d-2dba90744207).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/801:40,interoperability,architectur,architecture,40,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:215,modifiability,layer,layer,215,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:79,safety,input,input,79,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:132,safety,input,input,132,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:31,usability,learn,learning,31,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:57,usability,learn,learned,57,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:79,usability,input,input,79,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:132,usability,input,input,132,"I am confusion about your deep learning architecture.; I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/802:0,availability,error,error,0,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:930,availability,error,error,930,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:956,availability,checkpoint,checkpoint,956,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:974,availability,checkpoint,checkpoint,974,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1189,availability,checkpoint,checkpoint,1189,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1015,deployability,log,log,1015,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1066,deployability,log,log,1066,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1144,deployability,log,log,1144,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:229,energy efficiency,model,model,229,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:276,energy efficiency,model,model,276,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:329,energy efficiency,gpu,gpus,329,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:409,energy efficiency,gpu,gpu,409,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:0,performance,error,error,0,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:306,performance,time,time,306,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:329,performance,gpu,gpus,329,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:409,performance,gpu,gpu,409,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:930,performance,error,error,930,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:956,reliability,checkpoint,checkpoint,956,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:974,reliability,checkpoint,checkpoint,974,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1189,reliability,checkpoint,checkpoint,1189,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:0,safety,error,error,0,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:930,safety,error,error,930,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1015,safety,log,log,1015,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1066,safety,log,log,1066,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1144,safety,log,log,1144,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1232,safety,input,input,1232,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:229,security,model,model,229,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:276,security,model,model,276,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1015,security,log,log,1015,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1066,security,log,log,1066,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1144,security,log,log,1144,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1015,testability,log,log,1015,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1066,testability,log,log,1066,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1144,testability,log,log,1144,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1209,testability,understand,understand,1209,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:0,usability,error,error,0,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:52,usability,guid,guide,52,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:192,usability,command,command,192,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:350,usability,USER,USER,350,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:364,usability,USER,USER,364,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:384,usability,USER,USER,384,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:930,usability,error,error,930,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1163,usability,clear,clear,1163,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:1232,usability,input,input,1232,"error in training DeepVariant ; Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:. ```. time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=0 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512 \. --debug 'true'. ```. I received an error regarding about the checkpoint: ```No checkpoint found.```. I also attached my log for training step here: . [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/803:97,availability,down,downloaded,97,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:167,availability,error,error,167,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:76,deployability,version,version,76,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:338,deployability,contain,containing,338,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:139,energy efficiency,cpu,cpu,139,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:76,integrability,version,version,76,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:76,modifiability,version,version,76,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:139,performance,cpu,cpu,139,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:160,performance,memor,memory,160,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:167,performance,error,error,167,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:167,safety,error,error,167,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:160,usability,memor,memory,160,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:167,usability,error,error,167,"Restarting from post process variants step of deeptrio; Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/804:4629,availability,error,error,4629,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4657,availability,Operat,Operating,4657,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1263,deployability,instal,installed,1263,"ling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4703,deployability,version,version,4703,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4722,deployability,Instal,Installation,4722,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:670,energy efficiency,cpu,cpus,670,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:797,energy efficiency,load,load,797,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1185,energy efficiency,GPU,GPU,1185," DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1724,energy efficiency,CPU,CPUs,1724,"piler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1599,integrability,Transform,Transforming,1599,"/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._wri",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1760,integrability,transform,transformation,1760,"t/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2566,integrability,queue,queues,2566,"postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connectio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3406,integrability,wrap,wrapped,3406,"recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3459,integrability,queue,queues,3459,"ocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4095,integrability,queue,queues,4095,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4703,integrability,version,version,4703,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:752,interoperability,platform,platform,752,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:882,interoperability,share,shared,882,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1599,interoperability,Transform,Transforming,1599,"/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._wri",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1760,interoperability,transform,transformation,1760,"t/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4703,modifiability,version,version,4703,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:473,performance,time,time,473,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:670,performance,cpu,cpus,670,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:797,performance,load,load,797,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1185,performance,GPU,GPU,1185," DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1724,performance,CPU,CPUs,1724,"piler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1733,performance,parallel,parallelization,1733,"m_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2566,performance,queue,queues,2566,"postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connectio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3459,performance,queue,queues,3459,"ocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4095,performance,queue,queues,4095,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4629,performance,error,error,4629,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:528,safety,input,input,528,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:652,safety,input,input,652,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2100,safety,input,input,2100,"T Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2190,safety,input,input,2190," TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3031,safety,except,exception,3031,"86048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3050,safety,except,exception,3050,"ariants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4629,safety,error,error,4629,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:388,security,control,control,388,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:388,testability,control,control,388,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2394,testability,Trace,Traceback,2394,"ple name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3071,testability,Trace,Traceback,3071,"ng output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3923,testability,Trace,Traceback,3923,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:458,usability,command,command,458,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:528,usability,input,input,528,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:652,usability,input,input,652,"BrokenPipeErro during postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: **Yes**. **Describe the issue:**. Hi developers of DeepVariant,. I was using the latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:. ```bash. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to V",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2100,usability,input,input,2100,"T Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2190,usability,input,input,2190," TensorRT, please make sure the missing libraries mentioned above are installed properly. I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2334,usability,user,user,2334,"iants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes. I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation. I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default. I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF. I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz. I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with NativeVcfWriter. I0408 07:30:12.688740 140282942986048 postprocess_variants.py:987] 1 variants written. real 70m57.596s. user 45m18.578s. sys 30m4.764s. Process ForkPoolWorker-83:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:4629,usability,error,error,4629,"connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-42:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes. self._send(buf). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. ```. There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens? **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: v1.6.1. - Installation method : singularity. - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/805:120,energy efficiency,model,model,120,"Question about training; I have successfully run the training tutorial. However, I have a question about how to train a model if you have multiple samples. Do I have to create various examples for training and validation and then merge them? Thank you in advance for any clarification.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/805
https://github.com/google/deepvariant/issues/805:210,safety,valid,validation,210,"Question about training; I have successfully run the training tutorial. However, I have a question about how to train a model if you have multiple samples. Do I have to create various examples for training and validation and then merge them? Thank you in advance for any clarification.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/805
https://github.com/google/deepvariant/issues/805:120,security,model,model,120,"Question about training; I have successfully run the training tutorial. However, I have a question about how to train a model if you have multiple samples. Do I have to create various examples for training and validation and then merge them? Thank you in advance for any clarification.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/805
https://github.com/google/deepvariant/issues/805:210,security,validat,validation,210,"Question about training; I have successfully run the training tutorial. However, I have a question about how to train a model if you have multiple samples. Do I have to create various examples for training and validation and then merge them? Thank you in advance for any clarification.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/805
https://github.com/google/deepvariant/issues/806:480,availability,Error,Error,480,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:80,deployability,instal,install,80,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:166,deployability,instal,installed,166,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:207,deployability,releas,release,207,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:246,deployability,version,version,246,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:260,deployability,instal,install,260,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:308,deployability,Instal,Installation,308,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:432,deployability,instal,install,432,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:504,deployability,instal,install,504,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:225,energy efficiency,Core,Core,225,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:246,integrability,version,version,246,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:246,modifiability,version,version,246,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:564,modifiability,pac,package,564,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:480,performance,Error,Error,480,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:616,reliability,Doe,Does,616,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:480,safety,Error,Error,480,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:637,safety,test,test,637,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:673,safety,test,test,673,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:486,testability,trace,trace,486,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:637,testability,test,test,637,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:673,testability,test,test,673,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:851,testability,context,context,851,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:417,usability,Command,Command,417,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:480,usability,Error,Error,480,"unable to run deepvariant using conda; **Describe the issue:**. while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**. - OS: CentOS Linux release 7.4.1708 (Core). - DeepVariant version:conda install bioconda/label/cf201901::deepvariant. - Installation method (Docker, built from source, etc.): Conda. - Type of data: NA. **Steps to reproduce:**. - Command: conda install bioconda/label/cf201901::deepvariant. - Error trace: . '''conda install bioconda/label/cf201901::deepvariant -y. Collecting package metadata: done. Solving environment: '''. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**. NA.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/807:13,availability,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:33,availability,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:174,availability,error,error,174,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:197,availability,error,error,197,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,availability,fault,fault,217,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:238,availability,Operat,Operating,238,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1015,availability,Error,Error,1015,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:291,deployability,version,version,291,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:309,deployability,Instal,Installation,309,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:33,energy efficiency,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,energy efficiency,fault,fault,217,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:291,integrability,version,version,291,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:537,interoperability,format,format,537,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:567,interoperability,format,format,567,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:291,modifiability,version,version,291,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:786,modifiability,PAC,PACBIO,786,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:13,performance,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:33,performance,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:174,performance,error,error,174,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:197,performance,error,error,197,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,performance,fault,fault,217,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1015,performance,Error,Error,1015,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:33,reliability,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,reliability,fault,fault,217,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1047,reliability,Doe,Does,1047,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:13,safety,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:33,safety,fault,fault,33,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:174,safety,error,error,174,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:197,safety,error,error,197,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,safety,fault,fault,217,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:510,safety,Input,Input,510,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:652,safety,input,input,652,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:803,safety,input,input,803,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:835,safety,input,input,835,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1015,safety,Error,Error,1015,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1068,safety,test,test,1068,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1104,safety,test,test,1104,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1200,safety,Test,Test,1200,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:401,testability,instrument,instrument,401,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1021,testability,trace,trace,1021,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1068,testability,test,test,1068,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1104,testability,test,test,1104,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1200,testability,Test,Test,1200,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1439,testability,context,context,1439,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:13,usability,error,error,13,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:174,usability,error,error,174,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:197,usability,error,error,197,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:510,usability,Input,Input,510,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:604,usability,Command,Command,604,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:652,usability,input,input,652,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:803,usability,input,input,803,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:835,usability,input,input,835,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1015,usability,Error,Error,1015,"Fatal Python error: Segmentation fault; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**. - Operating system: Ubuntu 22.04.2 LTS . - DeepVariant version: 1.6.1. - Installation method (Docker, built from source, etc.): Docker . - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**. - Command: sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=/input/RILWLs1.fasta \. --reads=/input/Out.fastq \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. . ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/808:232,usability,command,command,232,"How to get list of variants after make_examples step?; Hi,. Is it possible to get a list of variants in files ***.tfrecord-?????-of-000??.gz ( the output of `make_examples` step)? One thing I can think of is to call `show_examples` command which will generate images for each variant from the file, but is there other faster way to get the list of variants? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/809:98,reliability,doe,does,98,"Highest mapping quality = 42 in bowtie2; Hello,. Since the highest mapping quality in bowtie2 42, does it affect the the mapping quality channel in deepvariant ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/810:394,availability,Operat,Operating,394,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1098,availability,Error,Error,1098,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1114,availability,error,errors,1114,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:211,deployability,contain,container,211,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:341,deployability,contain,contain,341,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:444,deployability,version,version,444,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:456,deployability,Instal,Installation,456,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:444,integrability,version,version,444,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:353,modifiability,interm,intermediate,353,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:444,modifiability,version,version,444,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:637,modifiability,Pac,PacBio,637,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:887,modifiability,PAC,PACBIO,887,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1098,performance,Error,Error,1098,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1114,performance,error,errors,1114,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:333,reliability,doe,doesn,333,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:739,safety,input,input,739,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:964,safety,input,input,964,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1098,safety,Error,Error,1098,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1114,safety,error,errors,1114,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:547,testability,instrument,instrument,547,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1104,testability,trace,trace,1104,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1139,testability,context,context,1139,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:672,usability,Command,Command,672,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:707,usability,USER,USER,707,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:739,usability,input,input,739,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:964,usability,input,input,964,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1098,usability,Error,Error,1098,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1114,usability,error,errors,1114,"Output files are missing after running deepvariant. ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**. After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**. - Operating system: Ubuntu 22.04 LTS. - DeepVariant version:. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**. - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc). - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/811:1201,energy efficiency,model,model,1201,"egions indicated as PAR, but through GLnexus merging and later imputation can gain ""impossible"" heterozygous genotypes. The probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The unrevised (glnexus output, with no hets) and imputed (beagle output, with hets) variants are here, along with the gvcfs. I can provide the alignments, references, etc. if you need as these samples are alrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:2207,integrability,pub,public,2207,"he probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The unrevised (glnexus output, with no hets) and imputed (beagle output, with hets) variants are here, along with the gvcfs. I can provide the alignments, references, etc. if you need as these samples are already public. [Y_haploid_vcf.tar.gz](https://github.com/google/deepvariant/files/15077490/Y_haploid_vcf.tar.gz). Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:479,safety,input,input,479,"Merging gvcf with GLnexus introduces non-zero heterozygous PL in hemizygous PAR; This is a follow up from an email thread with @pichuan, where DeepVariant correctly produces only hemizygous genotypes in regions indicated as PAR, but through GLnexus merging and later imputation can gain ""impossible"" heterozygous genotypes. The probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:1201,security,model,model,1201,"egions indicated as PAR, but through GLnexus merging and later imputation can gain ""impossible"" heterozygous genotypes. The probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The unrevised (glnexus output, with no hets) and imputed (beagle output, with hets) variants are here, along with the gvcfs. I can provide the alignments, references, etc. if you need as these samples are alrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:1458,security,modif,modified,1458,"he probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The unrevised (glnexus output, with no hets) and imputed (beagle output, with hets) variants are here, along with the gvcfs. I can provide the alignments, references, etc. if you need as these samples are already public. [Y_haploid_vcf.tar.gz](https://github.com/google/deepvariant/files/15077490/Y_haploid_vcf.tar.gz). Best,. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:211,usability,indicat,indicated,211,"Merging gvcf with GLnexus introduces non-zero heterozygous PL in hemizygous PAR; This is a follow up from an email thread with @pichuan, where DeepVariant correctly produces only hemizygous genotypes in regions indicated as PAR, but through GLnexus merging and later imputation can gain ""impossible"" heterozygous genotypes. The probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:479,usability,input,input,479,"Merging gvcf with GLnexus introduces non-zero heterozygous PL in hemizygous PAR; This is a follow up from an email thread with @pichuan, where DeepVariant correctly produces only hemizygous genotypes in regions indicated as PAR, but through GLnexus merging and later imputation can gain ""impossible"" heterozygous genotypes. The probably mainly seems to originate in GLnexus merging, where uncalled alleles (e.g., for ""GT:DP:AD:GQ:PL:RNC"" ""./.:5:5,0:0:14,0,104:II"" where I = gVCF input site is non-called) has a non-zero likelihood of being heterozygous according the PL tag, which is later imputed to a ""0/1"" genotype. But this also occurs for samples called as reference (0/0) but with non-zero PL tags for 0/1. For example, the third sample gains a heterozygous genotype at site `Y:6899016` after imputation, and has the gvcf block. ```. Y 6898989 . C <*> 0 . END=6899117 GT:GQ:MIN_DP:MED_DP:PL 0/0:1:0:0:0,3,29. ```. leads to the GLnexus merged GT. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/0:0:0,0:1:0,3,29:.. ```. and then the imputed. ```. Y 6899016 Y_6899016_T_C T C 47 . 0/1:1:0,1,0. ```. ### variant calling with DeepVariant. Running with singularity image v1.6.1. This was done with the ""WGS"" model, using `--channels ""insert_size"" --include_med_dp --gvcf <gvcf>` for _make\_examples_ and `--haploid_contigs ""X,Y"" --par_regions_bed <PAR.bed>` for _postprocess_. ### merging with GLnexus. Running with singularity image v1.4.1-0-g68e25e5. This uses a modified DeepVariant config, where there is no genotype revision (`revise_genotypes: false`). ```. /usr/local/bin/glnexus_cli \. --dir $TMPDIR/GLnexus.DB \. --config <deepvariant_preset_with_revise_genotypes_false> \. --threads 4 \. --mem-gbytes 20 \. *.g.vcf |\. bcftools view - |\. bgzip -@ 8 -c > Yhap.Unrevised.vcf.gz. ```. ### imputing with beagle4. This is where the ""./."" genotypes with non-zero het GL turn into ""0/1"". ```. java -jar -Xss25m -Xmx50G beagle.27Jan18.7e1.jar ne=100 gl=Yhap.Unrevised.vcf.gz out=Yhap.beagle4. ```. The un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/812:69,availability,error,error,69,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1503,availability,Error,Error,1503,"inux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4582,availability,error,error,4582,"ythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5091,availability,Error,Error,5091,"WFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:13,deployability,fail,failed,13,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:33,deployability,fail,fails,33,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:146,deployability,fail,failed,146,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:208,deployability,releas,release,208,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:250,deployability,VERSION,VERSION,250,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,deployability,LOG,LOGO,449,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:462,deployability,log,logo-icon,462,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:830,deployability,version,version,830,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:852,deployability,Instal,Installation,852,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1740,deployability,Fail,Failed,1740,"_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1944,deployability,modul,module,1944,"instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3672,deployability,fail,failed,3672," = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3699,deployability,fail,failed,3699,"ign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4358,deployability,observ,observed,4358," line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4545,deployability,instal,installing,4545,"ext. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4674,deployability,instal,install,4674,". parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5201,deployability,modul,module,5201,"010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5318,deployability,modul,module,5318," by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5482,deployability,modul,module,5482,"issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5631,deployability,modul,module,5631,"ty exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5794,deployability,modul,module,5794,"	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6019,deployability,modul,module,6019,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6231,deployability,modul,module,6231,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6471,deployability,modul,module,6471,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6680,deployability,modul,module,6680,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5656,energy efficiency,core,core,5656,"1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5748,energy efficiency,core,core,5748,"{INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['Serializ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5819,energy efficiency,core,core,5819,"NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5971,energy efficiency,core,core,5971,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6044,energy efficiency,core,core,6044,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6187,energy efficiency,core,core,6187,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6256,energy efficiency,core,core,6256,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6418,energy efficiency,core,core,6418,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6496,energy efficiency,core,core,6496,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6637,energy efficiency,core,core,6637,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:250,integrability,VERSION,VERSION,250,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:830,integrability,version,version,830,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:364,interoperability,platform,platform,364,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:250,modifiability,VERSION,VERSION,250,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:830,modifiability,version,version,830,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1944,modifiability,modul,module,1944,"instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5201,modifiability,modul,module,5201,"010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5271,modifiability,pac,packages,5271,"** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework impo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5318,modifiability,modul,module,5318," by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5428,modifiability,pac,packages,5428,"e with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5482,modifiability,modul,module,5482,"issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5572,modifiability,pac,packages,5572,"s and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5631,modifiability,modul,module,5631,"ty exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5728,modifiability,pac,packages,5728,"e=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5794,modifiability,modul,module,5794,"	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5951,modifiability,pac,packages,5951,"_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6019,modifiability,modul,module,6019,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6167,modifiability,pac,packages,6167,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6231,modifiability,modul,module,6231,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6398,modifiability,pac,packages,6398,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6471,modifiability,modul,module,6471,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6617,modifiability,pac,packages,6617,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6680,modifiability,modul,module,6680,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:69,performance,error,error,69,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1503,performance,Error,Error,1503,"inux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1614,performance,Overhead,Overhead,1614,"URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3680,performance,parallel,parallel,3680,"egion_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4582,performance,error,error,4582,"ythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5091,performance,Error,Error,5091,"WFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:13,reliability,fail,failed,13,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:33,reliability,fail,fails,33,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:146,reliability,fail,failed,146,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1740,reliability,Fail,Failed,1740,"_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3672,reliability,fail,failed,3672," = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3699,reliability,fail,failed,3699,"ign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4229,reliability,Doe,Does,4229," enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:51,safety,test,test,51,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:69,safety,error,error,69,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:126,safety,valid,valid,126,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,safety,LOG,LOGO,449,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:462,safety,log,logo-icon,462,"Dynamic cast failed; DeepVariant fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1030,safety,Test,Test,1030," fails to run with test data, giving error:. ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**. running from HPC. OS info:. `cat /etc/os-release`. output:. ```. NAME=""AlmaLinux"". VERSION=""9.3 (Shamrock Pampas Cat)"". ID=""almalinux"". ID_LIKE=""rhel centos fedora"". VERSION_ID=""9.3"". PLATFORM_ID=""platform:el9"". PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)"". ANSI_COLOR=""0;34"". LOGO=""fedora-logo-icon"". CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1503,safety,Error,Error,1503,"inux:9::baseos"". HOME_URL=""https://almalinux.org/"". DOCUMENTATION_URL=""https://wiki.almalinux.org/"". BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1637,safety,input,inputs,1637,"linux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"". ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3"". REDHAT_SUPPORT_PRODUCT=""AlmaLinux"". REDHAT_SUPPORT_PRODUCT_VERSION=""9.3"". ```. - DeepVariant version: **1.6.1**. - Installation method (Docker, built from source, etc.): **Docker**. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1944,safety,modul,module,1944,"instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**. - Command: . ``` . run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. - Error trace: (if applicable). ```. I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. region_processor.process(region, region_n). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3652,safety,valid,valid,3652,"rocess. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3806,safety,test,testdata,3806," line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3896,safety,test,testdata,3896,"unfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4250,safety,test,test,4250,". File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4582,safety,error,error,4582,"ythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5091,safety,Error,Error,5091,"WFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5201,safety,modul,module,5201,"010,000 --task 0. ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5318,safety,modul,module,5318," by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5482,safety,modul,module,5482,"issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```. singularity exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5631,safety,modul,module,5631,"ty exec DeepVariant_1.6.1.sif bash. pip install --user google-nucleus. run_deepvariant --model_type=WGS \. 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5794,safety,modul,module,5794,"	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. 	--regions ""chr20:10,000,000-10,010,000"" \. 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6019,safety,modul,module,6019,"vcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. 	--num_shards=12. ```. Error:. ```. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>. from tensorflow.core.framework import function_pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>. from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>. from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>. from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>. from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py"", line 71, in <module>. _SERIALIZEDDTYPE = DESCRIPTOR.message_types_by_name['SerializedDType']. KeyError: 'SerializedDType'. ```. **Any additional context:** Some other issues suggested that the cause may be related to using singularity. I don't really have a choice in that, unfortunately. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
