id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/568:51,modifiability,coupl,couple,51,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:87,safety,updat,updating,87,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:87,security,updat,updating,87,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:51,testability,coupl,couple,51,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:33,usability,guid,guidelines,33,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:113,usability,guid,guidelines,113,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:130,usability,help,help,130,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/issues/569:37,availability,slo,slowly,37,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:159,availability,cluster,clusters,159,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:180,availability,slo,slowly,180,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:203,availability,slo,slowers,203,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:159,deployability,cluster,clusters,159,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:110,energy efficiency,heat,heatmap,110,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:197,performance,time,times,197,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:37,reliability,slo,slowly,37,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:180,reliability,slo,slowly,180,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:203,reliability,slo,slowers,203,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/569:248,security,modif,modify,248,"sc.pl.rank_genes_groups_heatmap runs slowly; Dear,. I used sc.pl.rank_genes_groups_heatmap(adata) to create a heatmap of top100 marker genes of 8,000 cells, 4 clusters, but it ran slowly, about 30 times slowers than seurat's Doheatmap(). Could you modify it to accelerate the process ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/569
https://github.com/scverse/scanpy/issues/570:78,availability,redund,redundant,78,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:78,deployability,redundan,redundant,78,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:238,interoperability,share,shared,238,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:78,reliability,redundan,redundant,78,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:78,safety,redund,redundant,78,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:184,safety,valid,valid,184,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:88,usability,document,documentation,88,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:116,usability,learn,learned,116,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:245,usability,document,documentation,245,"deduplication of louvain and leiden doc; `louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/571:201,deployability,log,logo,201,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:290,deployability,log,logo,290,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:201,safety,log,logo,201,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:290,safety,log,logo,290,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:201,security,log,logo,201,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:290,security,log,logo,290,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:201,testability,log,logo,201,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:290,testability,log,logo,290,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:340,testability,simpl,simply,340,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:340,usability,simpl,simply,340,"Purpose of the “stable” branch; Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console. $ git checkout 1.4 -b stable. $ git cherry-pick 4b1504c c78de5b # the logo commits. ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/pull/572:409,availability,error,error,409,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:913,availability,error,error,913,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:984,deployability,contain,contain,984,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:96,modifiability,variab,variable,96,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:409,performance,error,error,409,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:913,performance,error,error,913,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:1156,reliability,doe,does,1156,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:301,safety,test,test,301,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:409,safety,error,error,409,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:913,safety,error,error,913,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:879,security,sign,significant,879,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:1165,security,modif,modify,1165,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:301,testability,test,test,301,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:409,usability,error,error,409,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:710,usability,minim,minimum,710,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:913,usability,error,error,913,"Add 'equal_frequency' option to highly_variable_genes; This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:. ```R. else if (binning.method==""equal_frequency"") {. data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))). }. ```. The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/573:45,availability,down,downloader,45,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:137,availability,down,downloaded,137,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:242,availability,down,downloaded,242,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:291,deployability,build,build,291,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:385,deployability,API,API,385,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:385,integrability,API,API,385,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:356,interoperability,format,format,356,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:385,interoperability,API,API,385,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:168,modifiability,variab,variable,168,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:205,performance,content,contentious,205,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:348,reliability,doe,doesn,348,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:400,reliability,doe,does,400,"Expression atlas; Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/574:110,availability,error,error,110,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:176,availability,Mask,MaskedArray,176,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:110,performance,error,error,110,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:23,safety,test,testing,23,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:110,safety,error,error,110,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:23,testability,test,testing,23,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:110,usability,error,error,110,"Fix read softgz; While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python. ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>. ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/issues/575:113,deployability,version,version,113,"feature request for embedding_density(); Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you! Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:113,integrability,version,version,113,"feature request for embedding_density(); Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you! Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:113,modifiability,version,version,113,"feature request for embedding_density(); Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you! Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:230,modifiability,exten,extend,230,"feature request for embedding_density(); Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you! Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/issues/575:281,usability,visual,visualize,281,"feature request for embedding_density(); Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you! Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/pull/576:59,deployability,fail,fails,59,"Remove frozen umap; Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:59,reliability,fail,fails,59,"Remove frozen umap; Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:54,safety,test,test,54,"Remove frozen umap; Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:54,testability,test,test,54,"Remove frozen umap; Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:202,usability,clear,clear,202,"Remove frozen umap; Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/issues/577:422,deployability,modul,module,422,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:926,integrability,compon,components,926,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:926,interoperability,compon,components,926,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:422,modifiability,modul,module,422,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:926,modifiability,compon,components,926,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:938,modifiability,layer,layer,938,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:1135,modifiability,layer,layer,1135,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:1141,modifiability,layer,layer,1141,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:1390,modifiability,layer,layer,1390,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:396,safety,input,input-,396,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:422,safety,modul,module,422,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:227,testability,Trace,Traceback,227,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:352,testability,Trace,Traceback,352,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/issues/577:396,usability,input,input-,396,"Scatter plots broken on master; To recreate:. ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. pbmcs = sc.datasets.pbmc3k(). sc.pp.pca(pbmcs). sc.pl.pca(pbmcs, color=""MALAT1""). ```. <details>. <summary>Traceback:</summary>. ```python. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-1-a8a0d93afa77> in <module>. 3 pbmcs = sc.datasets.pbmc3k(). 4 sc.pp.pca(pbmcs). ----> 5 sc.pl.pca(pbmcs, color=""MALAT1""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs). 413 If `show==False` a `matplotlib.Axis` or a list of it. 414 """""". --> 415 return plot_scatter(adata, 'pca', **kwargs). 416 . 417 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 149 adata, value_to_plot, layer=layer,. 150 groups=groups, palette=palette,. --> 151 use_raw=use_raw, gene_symbols=gene_symbols,. 152 ). 153 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer). 726 elif use_raw and value_to_plot in adata.var_names:. 727 color_vector = adata.raw[:, value_to_plot].X. --> 728 elif value_to_plot in adata.raw.var_names:. 729 color_vector = adata[:, value_to_plot].X. 730 . AttributeError: 'NoneType' object has no attribute 'var_names'. ```. </details>. Looks like the `raw` attribute is being checked when it's not there. @fidelram, looks like you might have been working on this recently?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/577
https://github.com/scverse/scanpy/pull/578:77,safety,test,tests,77,"Fix for #577; I think this should fix it – it fixes it on my machine and the tests pass. There's no reason to access raw if `use_raw` isn't passed, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/578
https://github.com/scverse/scanpy/pull/578:110,security,access,access,110,"Fix for #577; I think this should fix it – it fixes it on my machine and the tests pass. There's no reason to access raw if `use_raw` isn't passed, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/578
https://github.com/scverse/scanpy/pull/578:77,testability,test,tests,77,"Fix for #577; I think this should fix it – it fixes it on my machine and the tests pass. There's no reason to access raw if `use_raw` isn't passed, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/578
https://github.com/scverse/scanpy/pull/579:4,deployability,log,logic,4,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:41,deployability,log,logic,41,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:4,safety,log,logic,4,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:41,safety,log,logic,41,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:69,safety,test,test,69,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:4,security,log,logic,4,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:41,security,log,logic,41,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:4,testability,log,logic,4,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:41,testability,log,logic,41,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:69,testability,test,test,69,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:397,testability,simpl,simplify,397,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:397,usability,simpl,simplify,397,"Fix logic in scatterplot; Seems like the logic was broken. ```. # we test for raw, but check in adata.var_names. elif use_raw and value_to_plot in adata.var_names:. color_vector = adata.raw[:, value_to_plot].X. # use_raw might be false but we still check adata.raw.var_names. elif value_to_plot in adata.raw.var_names:. color_vector = adata[:, value_to_plot].X. ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/issues/580:292,availability,error,error,292,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:20,deployability,fail,failing,20,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:89,deployability,fail,failing,89,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:111,deployability,build,build,111,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:167,deployability,build,build,167,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:195,deployability,build,build,195,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:298,deployability,log,log,298,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:313,deployability,build,build,313,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:292,performance,error,error,292,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:20,reliability,fail,failing,20,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:89,reliability,fail,failing,89,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:56,safety,test,test,56,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:292,safety,error,error,292,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:298,safety,log,log,298,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:502,safety,Test,Test,502,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1472,safety,test,tests,1472,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:298,security,log,log,298,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:56,testability,test,test,56,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:298,testability,log,log,298,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:502,testability,Test,Test,502,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1416,testability,assert,assert,1416,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1446,testability,assert,assert,1446,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1472,testability,test,tests,1472,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1510,testability,Assert,AssertionError,1510,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:292,usability,error,error,292,"test_marker_overlap failing occasionally on travis; The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```. _____________________________ test_marker_overlap ______________________________. def test_marker_overlap():. # Test all overlap calculations on artificial data. test_data = sc.AnnData(X = np.ones((9,10))). test_data.uns['rank_genes_groups'] = dict(). test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(. [['a', 'b','c','d','e'], ['a','f','g','h','i']]). test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(. [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]). . marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}. . t1 = sc.tl.marker_gene_overlap(test_data, marker_genes). t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'). t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'). t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'). t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2). t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01). . > assert t1.iloc[1,1] == 3.0. E assert 1.0 == 3.0. scanpy/tests/test_marker_gene_overlap.py:22: AssertionError. ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/581:7,deployability,scale,scale,7,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:150,deployability,scale,scale,150,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:303,deployability,scale,scale,303,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:680,deployability,log,log-ed,680,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:7,energy efficiency,scale,scale,7,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:150,energy efficiency,scale,scale,150,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:303,energy efficiency,scale,scale,303,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:7,modifiability,scal,scale,7,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:150,modifiability,scal,scale,150,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:303,modifiability,scal,scale,303,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:7,performance,scale,scale,7,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:150,performance,scale,scale,150,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:303,performance,scale,scale,303,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:680,safety,log,log-ed,680,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:680,security,log,log-ed,680,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:680,testability,log,log-ed,680,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/581:174,usability,User,Users,174,"`sc.pp.scale` returning matrix of nan?; ```python. import scanpy as sc. print(sc.__version__) # 1.4+222.gdd052ed. a = sc.datasets.pbmc3k(). b = sc.pp.scale(a, copy=True). # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide. # X /= scale. a.X.data. # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32). b.X. # array([[nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # ...,. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan],. # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32). ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/582:50,availability,cluster,clustering,50,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:152,availability,cluster,clustering,152,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:282,availability,cluster,cluster,282,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:50,deployability,cluster,clustering,50,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:152,deployability,cluster,clustering,152,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:282,deployability,cluster,cluster,282,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:254,integrability,sub,subclustering,254,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:273,interoperability,specif,specific,273,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:230,modifiability,paramet,parameter,230,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:96,performance,synch,synchronize,96,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/issues/582:318,reliability,doe,does,318,"feature request - `restrict_to` option for leiden clustering; Hi, . I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. . `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). . I'd be happy to have that. . Best,. M.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/pull/583:505,deployability,fail,failing,505,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:147,performance,perform,performed,147,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:505,reliability,fail,failing,505,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:80,safety,input,input,80,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:80,usability,input,input,80,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:147,usability,perform,performed,147,"Marker overlap fix; Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:. 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data. 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/issues/584:156,availability,cluster,cluster,156,"Getting cell counts for cells with more than 1 label; Hello,. How can I get a count of cells that have multiple labels assigned to them ? For example, from cluster 9 (louvain label) I want to pick out just the cells that came from female patients (another adata.obs I have created) ? . I also want to be able to do this with multiple labels and get counts for different groups of cells (having 5-6 labels attached to them) and I am not sure how to do this. Any help is appreciated, thanks a lot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:156,deployability,cluster,cluster,156,"Getting cell counts for cells with more than 1 label; Hello,. How can I get a count of cells that have multiple labels assigned to them ? For example, from cluster 9 (louvain label) I want to pick out just the cells that came from female patients (another adata.obs I have created) ? . I also want to be able to do this with multiple labels and get counts for different groups of cells (having 5-6 labels attached to them) and I am not sure how to do this. Any help is appreciated, thanks a lot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/issues/584:461,usability,help,help,461,"Getting cell counts for cells with more than 1 label; Hello,. How can I get a count of cells that have multiple labels assigned to them ? For example, from cluster 9 (louvain label) I want to pick out just the cells that came from female patients (another adata.obs I have created) ? . I also want to be able to do this with multiple labels and get counts for different groups of cells (having 5-6 labels attached to them) and I am not sure how to do this. Any help is appreciated, thanks a lot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584
https://github.com/scverse/scanpy/pull/585:20,availability,error,error,20,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:122,availability,error,error,122,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:506,availability,error,errors,506,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:376,deployability,Modul,ModuleNotFoundError,376,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:698,deployability,version,versions,698,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:730,deployability,Version,Versions,730,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:898,deployability,log,logging,898,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:982,deployability,version,versions,982,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1014,deployability,Version,Versions,1014,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1209,deployability,version,versions,1209,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1241,deployability,Version,Versions,1241,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1439,deployability,Modul,ModuleNotFoundError,1439,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1463,deployability,modul,module,1463,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:236,integrability,wrap,wrap,236,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:698,integrability,version,versions,698,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:730,integrability,Version,Versions,730,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:982,integrability,version,versions,982,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1014,integrability,Version,Versions,1014,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1209,integrability,version,versions,1209,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1241,integrability,Version,Versions,1241,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:376,modifiability,Modul,ModuleNotFoundError,376,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:698,modifiability,version,versions,698,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:730,modifiability,Version,Versions,730,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:762,modifiability,pac,packages,762,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:982,modifiability,version,versions,982,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1014,modifiability,Version,Versions,1014,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1046,modifiability,pac,packages,1046,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1209,modifiability,version,versions,1209,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1241,modifiability,Version,Versions,1241,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1273,modifiability,pac,packages,1273,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1439,modifiability,Modul,ModuleNotFoundError,1439,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1463,modifiability,modul,module,1463,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:20,performance,error,error,20,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:122,performance,error,error,122,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:506,performance,error,errors,506,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:20,safety,error,error,20,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:122,safety,error,error,122,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:274,safety,except,except,274,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:376,safety,Modul,ModuleNotFoundError,376,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:506,safety,error,errors,506,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:898,safety,log,logging,898,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1439,safety,Modul,ModuleNotFoundError,1439,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1463,safety,modul,module,1463,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:898,security,log,logging,898,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:396,testability,Trace,Traceback,396,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:898,testability,log,logging,898,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:20,usability,error,error,20,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:122,usability,error,error,122,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:506,usability,error,errors,506,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:542,usability,hint,hints,542,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1318,usability,tool,tools,1318,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:1354,usability,tool,tools,1354,"Fixes scanpy import error on master; I am not sure if it has been already addressed. This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). in . ----> 1 import scanpy as sc. 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures. 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in . 31 from . import preprocessing as pp. 32 from . import plotting as pl. ---> 33 from . import datasets, logging, queries, settings, external. 34 . 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in . ----> 1 from . import tl. 2 from . import pl. 3 from . import pp. 4 . 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in . 2 from ..tools._phate import phate. 3 from ..tools._phenograph import phenograph. ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/586:202,availability,cluster,cluster,202,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:644,availability,cluster,clusters,644,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:885,availability,cluster,clusters,885,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:906,availability,cluster,clusters,906,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1160,availability,cluster,clusters,1160,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1181,availability,cluster,clusters,1181,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:202,deployability,cluster,cluster,202,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:644,deployability,cluster,clusters,644,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:885,deployability,cluster,clusters,885,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:906,deployability,cluster,clusters,906,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1160,deployability,cluster,clusters,1160,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1181,deployability,cluster,clusters,1181,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:828,integrability,sub,subsequent,828,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:897,integrability,sub,subsplit,897,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1172,integrability,sub,subsplit,1172,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:19,modifiability,paramet,parameter,19,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:48,modifiability,paramet,parameter,48,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:103,safety,Test,Tests,103,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:103,testability,Test,Tests,103,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:133,testability,simpl,simple,133,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:133,usability,simpl,simple,133,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:1424,usability,user,user-images,1424,"Leiden restrict_to parameter; Added restrict_to parameter to leiden by using louvain code as template. Tests are not yet provided. A simple example of execution and checks:. ```python. # First split on cluster 4. sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,. key_added='leiden_res0.4_4_sub'). # Additional split. sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),. resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together. sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',. 'leiden_res0.4_4_add_sub']). # Partition size check. ## Original size of clusters. adata.obs['leiden_res0.4'].value_counts(). 0 932. 1 853. 3 676. 2 676. 4 338. 5 57. Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits). ## Assignment of samples in original clusters to subsplit clusters. adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 4,0 103. 4,1 68. 4,2 66. 4,3 57. 4,4 44. 5 0. 3 0. 2 0. 1 0. 0 0. Name: leiden_res0.4_4_sub, dtype: int64. ## Assignment of samples not in original clusters to subsplit clusters. adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),. 'leiden_res0.4_4_sub'].value_counts(). 0 932. 1 853. 3 676. 2 676. 5 57. 4,4 0. 4,3 0. 4,2 0. 4,1 0. 4,0 0. Name: leiden_res0.4_4_sub, dtype: int64. ... ```. ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/issues/587:11,availability,error,error,11,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:48,deployability,instal,installing,48,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:311,deployability,modul,module,311,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:367,deployability,Continu,Continuum,367,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:428,deployability,modul,module,428,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:581,deployability,version,version,581,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:602,deployability,version,versioneer,602,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:634,deployability,Continu,Continuum,634,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:692,deployability,modul,module,692,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:757,deployability,log,logging,757,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:768,deployability,log,logg,768,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:811,deployability,Continu,Continuum,811,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:856,deployability,log,logging,856,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:871,deployability,modul,module,871,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:959,deployability,log,logging,959,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1014,deployability,Continu,Continuum,1014," scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1076,deployability,modul,module,1076,"prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1266,deployability,Continu,Continuum,1266,"st). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specifi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1324,deployability,modul,module,1324,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1467,deployability,Continu,Continuum,1467,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1534,deployability,modul,module,1534,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1722,deployability,Continu,Continuum,1722,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1789,deployability,modul,module,1789,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1906,deployability,Continu,Continuum,1906,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1965,deployability,modul,module,1965,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2252,deployability,fail,failed,2252,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2247,energy efficiency,load,load,2247,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:581,integrability,version,version,581,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:602,integrability,version,versioneer,602,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2264,interoperability,specif,specified,2264,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:311,modifiability,modul,module,311,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:396,modifiability,pac,packages,396,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:428,modifiability,modul,module,428,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:581,modifiability,version,version,581,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:602,modifiability,version,versioneer,602,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:663,modifiability,pac,packages,663,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:692,modifiability,modul,module,692,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:840,modifiability,pac,packages,840,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:871,modifiability,modul,module,871,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1043,modifiability,pac,packages,1043," installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1076,modifiability,modul,module,1076,"prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1295,modifiability,pac,packages,1295,"bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1324,modifiability,modul,module,1324,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1417,modifiability,layer,layers,1417,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1496,modifiability,pac,packages,1496,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1534,modifiability,modul,module,1534,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1751,modifiability,pac,packages,1751,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1789,modifiability,modul,module,1789,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1935,modifiability,pac,packages,1935,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1965,modifiability,modul,module,1965,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:11,performance,error,error,11,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:891,performance,time,time,891,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2247,performance,load,load,2247,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2252,reliability,fail,failed,2252,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:11,safety,error,error,11,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:285,safety,input,input-,285,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:311,safety,modul,module,311,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:428,safety,modul,module,428,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:692,safety,modul,module,692,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:757,safety,log,logging,757,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:768,safety,log,logg,768,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:856,safety,log,logging,856,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:871,safety,modul,module,871,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:959,safety,log,logging,959,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1076,safety,modul,module,1076,"prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1324,safety,modul,module,1324,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1534,safety,modul,module,1534,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1789,safety,modul,module,1789,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1965,safety,modul,module,1965,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:757,security,log,logging,757,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:768,security,log,logg,768,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:856,security,log,logging,856,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:959,security,log,logging,959,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:241,testability,Trace,Traceback,241,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:757,testability,log,logging,757,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:768,testability,log,logg,768,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:856,testability,log,logging,856,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:959,testability,log,logging,959,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1346,testability,mock,mock,1346,"in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:11,usability,error,error,11,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:285,usability,input,input-,285,"getting an error in scanpy after 'successfully' installing it through anaconda prompt; this is what i get after running `import scanpy as sc`. ```pytb. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-1-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(). 16 . 17 from . import settings. ---> 18 from . import logging as logg. 19 . 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(). 4 import time as time_module. 5 import datetime. ----> 6 from anndata import logging. 7 from . import settings. 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .base import AnnData. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(). 40 return 'mock zappy.base.ZappyArray'. 41 . ---> 42 from . import h5py. 43 from .layers import AnnDataLayers. 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 5 . 6 import six. ----> 7 import h5py. 8 import numpy as np. 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/588:68,deployability,modul,modules,68,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:373,deployability,api,api,373,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:322,integrability,discover,discoverable,322,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:373,integrability,api,api,373,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:453,integrability,topic,topic,453,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:322,interoperability,discover,discoverable,322,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:373,interoperability,api,api,373,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:68,modifiability,modul,modules,68,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:68,safety,modul,modules,68,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/588:322,usability,discov,discoverable,322,"Give `external` higher billing in the docs?; At the moment external modules are kind of hidden in the docs. I think it'd be worth making them more visible (at least on the same page as everything else). I've been giving this a shot, but have hit the limit of my sphinx/ rst abilities. Two ideas for how they could be more discoverable:. * They get their own heading under `api`. * They're mixed in with everything else (so everything stays organized by topic), but their names are prepended with `sce` while scanpy functions are prepended with `sc`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588
https://github.com/scverse/scanpy/issues/589:351,availability,cluster,cluster,351,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:708,availability,cluster,cluster,708,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:885,availability,sli,slice,885,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:962,availability,cluster,clusters,962,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:993,availability,cluster,cluster,993,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:1277,availability,cluster,cluster,1277,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:33,deployability,fail,fails,33,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:82,deployability,Observ,Observation,82,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:351,deployability,cluster,cluster,351,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:708,deployability,cluster,cluster,708,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:962,deployability,cluster,clusters,962,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:993,deployability,cluster,cluster,993,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:1277,deployability,cluster,cluster,1277,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:68,integrability,batch,batch,68,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:120,integrability,batch,batch,120,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:777,integrability,batch,batch,777,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:879,integrability,batch,batch,879,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:390,interoperability,registr,registration,390,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:917,interoperability,distribut,distributions,917,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:68,performance,batch,batch,68,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:120,performance,batch,batch,120,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:777,performance,batch,batch,777,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:879,performance,batch,batch,879,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:33,reliability,fail,fails,33,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:885,reliability,sli,slice,885,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:82,testability,Observ,Observation,82,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:446,usability,user,user-images,446,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/589:593,usability,user,user-images,593,"color persistence in violin plot fails when keys not present in all batch groups; Observation: `sc.pl.violin(adata.obs['batch'] == 'batch_no'], keys = 'gene_of_interest', groupby = 'desc_0.8', palette = 'Set2')` gives inconsistent color mapping when comparing plots between different batch_no and especially if a gene_of_interest (GOI) is absent in a cluster; that seems to upset the color registration. <img width=""290"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530566-a0792600-5674-11e9-8088-89c0029847b2.png"">. versus:. <img width=""301"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/55530580-aa9b2480-5674-11e9-927c-6acda646cf7c.png"">. In the upper plot, cluster -6- is skipped as possibly no value is generated for the GOI/batch combination that was interrogated. In contrast, the bottom plot shows the result of a different batch slice with the same GOI. There, distributions for the GOI are present in all clusters. Noticeably, skipping cluster six in the top plot, brings the color mapping out of order when compared with the plot below. This is annoying if one likes to compare the output of many violin plots of that sort. Is there a way to 'hard-code' the desired association between color value and in this case the cluster number (0 to 13 here). As there is no adata.uns['desc_0.8_colors'], I am afraid that I do not know where to start here.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/589
https://github.com/scverse/scanpy/issues/590:124,usability,user,user-images,124,All-zero genes show up as white columns in matrixplots if standard_scale='var' is given; Looks like this:. ![image](https://user-images.githubusercontent.com/1140359/55589119-7ddf1f80-56fd-11e9-9496-a4da41ec2e50.png). Not that it's wrong but it'd be prettier to show as regular as a regular grid.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/590
https://github.com/scverse/scanpy/pull/591:89,deployability,manag,manage,89,"Add pipenv files to .gitignore; Would anyone mind if these were added? I've used this to manage my `scanpy` dev environment, but would like it to stop showing up in `git status`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/591:89,energy efficiency,manag,manage,89,"Add pipenv files to .gitignore; Would anyone mind if these were added? I've used this to manage my `scanpy` dev environment, but would like it to stop showing up in `git status`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/591:89,safety,manag,manage,89,"Add pipenv files to .gitignore; Would anyone mind if these were added? I've used this to manage my `scanpy` dev environment, but would like it to stop showing up in `git status`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/591:146,usability,stop,stop,146,"Add pipenv files to .gitignore; Would anyone mind if these were added? I've used this to manage my `scanpy` dev environment, but would like it to stop showing up in `git status`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/591:170,usability,statu,status,170,"Add pipenv files to .gitignore; Would anyone mind if these were added? I've used this to manage my `scanpy` dev environment, but would like it to stop showing up in `git status`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591
https://github.com/scverse/scanpy/pull/592:423,availability,error,error,423,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,deployability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,integrability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:558,integrability,coupl,couple,558,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,interoperability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,modifiability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:558,modifiability,coupl,couple,558,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:423,performance,error,error,423,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:445,performance,cach,cache-ing,445,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:656,performance,cach,cache,656,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,reliability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:299,safety,test,tests,299,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:423,safety,error,error,423,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:491,safety,test,tests,491,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:669,safety,test,testing,669,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,security,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:144,testability,integr,integration,144,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:299,testability,test,tests,299,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:491,testability,test,tests,491,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:558,testability,coupl,couple,558,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:669,testability,test,testing,669,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:122,usability,learn,learn,122,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:423,usability,error,error,423,"Minor fixes, mostly io related; This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error. * I've removed cache-ing in a few places. * The `read_10x_*` tests, where that definitely shouldn't have been happening. * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/issues/593:23,availability,error,error,23,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:67,availability,error,error,67,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2332,availability,error,error,2332,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:329,deployability,modul,module,329,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:420,deployability,Continu,Continuum,420,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:533,deployability,log,log,533,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:844,deployability,log,log,844,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:980,deployability,Continu,Continuum,980,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1104,deployability,log,log,1104,".dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1311,deployability,Continu,Continuum,1311,"51f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1642,deployability,Continu,Continuum,1642,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1943,deployability,Continu,Continuum,1943,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2071,deployability,contain,contain,2071,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2087,deployability,observ,observation,2087,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2255,deployability,contain,contain,2255,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2271,deployability,observ,observation,2271,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2425,deployability,contain,contains,2425,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2477,integrability,sub,subsetting,2477,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2135,interoperability,format,format,2135,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:329,modifiability,modul,module,329,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:449,modifiability,pac,packages,449,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:663,modifiability,layer,layer,663,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:865,modifiability,layer,layer,865,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:871,modifiability,layer,layer,871,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1009,modifiability,pac,packages,1009," Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1125,modifiability,layer,layer,1125,"_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1167,modifiability,layer,layers,1167," ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return position",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1174,modifiability,layer,layer,1174,"---------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.value",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1340,modifiability,pac,packages,1340,"> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1671,modifiability,pac,packages,1671,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1972,modifiability,pac,packages,1972,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2099,modifiability,variab,variables,2099,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2283,modifiability,variab,variables,2283,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:23,performance,error,error,23,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:67,performance,error,error,67,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2332,performance,error,error,2332,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:23,safety,error,error,23,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:67,safety,error,error,67,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:301,safety,input,input-,301,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:329,safety,modul,module,329,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:533,safety,log,log,533,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:844,safety,log,log,844,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1104,safety,log,log,1104,".dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2332,safety,error,error,2332,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:533,security,log,log,533,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:844,security,log,log,844,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1104,security,log,log,1104,".dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:257,testability,Trace,Traceback,257,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:533,testability,log,log,533,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:844,testability,log,log,844,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1104,testability,log,log,1104,".dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2087,testability,observ,observation,2087,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2271,testability,observ,observation,2271,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:23,usability,error,error,23,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:67,usability,error,error,67,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:301,usability,input,input-,301,"sc.pl.dotplot Keyvalue error; Hi guys,. I am getting the following error after running this:. ```py. sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ```. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-138-e642551f77de> in <module>(). ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2332,usability,error,error,2332,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2398,usability,command,command,2398,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2508,usability,help,help,2508,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1350 if isinstance(var_names, str):. 1351 var_names = [var_names]. -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer). 1353 . 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer). 1983 matrix = adata[:, var_names].layers[layer]. 1984 elif use_raw:. -> 1985 matrix = adata.raw[:, var_names].X. 1986 else:. 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index). 495 . 496 def __getitem__(self, index):. --> 497 oidx, vidx = self._normalize_indices(index). 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]. 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index). 523 obs, var = super()._unpack_index(packed_index). 524 obs = _normalize_index(obs, self._adata.obs_names). --> 525 var = _normalize_index(var, self.var_names). 526 return obs, var. 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names). 268 raise KeyError(. 269 'Indices ""{}"" contain invalid observation/variables names/indices.'. --> 270 .format(index)). 271 return positions.values. 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'. ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/595:16,deployability,instal,installed,16,"how can it been installed if linux env dose not have a GUI and Tkinner, and I only have a user account;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/595:90,usability,user,user,90,"how can it been installed if linux env dose not have a GUI and Tkinner, and I only have a user account;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595
https://github.com/scverse/scanpy/issues/596:485,energy efficiency,green,green,485,"Store predefined group colors as dict?; Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python. import seaborn as sns. iris = sns.load_dataset(""iris""). g = sns.FacetGrid(. iris, . row=""species"", . hue=""species"", . palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}. ). g.map(sns.kdeplot, ""sepal_width"").show(). ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:238,security,modif,modify,238,"Store predefined group colors as dict?; Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python. import seaborn as sns. iris = sns.load_dataset(""iris""). g = sns.FacetGrid(. iris, . row=""species"", . hue=""species"", . palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}. ). g.map(sns.kdeplot, ""sepal_width"").show(). ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:208,usability,intuit,intuitive,208,"Store predefined group colors as dict?; Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python. import seaborn as sns. iris = sns.load_dataset(""iris""). g = sns.FacetGrid(. iris, . row=""species"", . hue=""species"", . palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}. ). g.map(sns.kdeplot, ""sepal_width"").show(). ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:251,usability,tool,tools,251,"Store predefined group colors as dict?; Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python. import seaborn as sns. iris = sns.load_dataset(""iris""). g = sns.FacetGrid(. iris, . row=""species"", . hue=""species"", . palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}. ). g.map(sns.kdeplot, ""sepal_width"").show(). ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:584,usability,user,user-images,584,"Store predefined group colors as dict?; Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python. import seaborn as sns. iris = sns.load_dataset(""iris""). g = sns.FacetGrid(. iris, . row=""species"", . hue=""species"", . palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}. ). g.map(sns.kdeplot, ""sepal_width"").show(). ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/pull/597:4,interoperability,standard,standardscale,4,Fix standardscale NA bug; Fixes #590.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/597
https://github.com/scverse/scanpy/issues/598:287,availability,error,error,287,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1711,availability,sli,slice,1711,"eys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1856,availability,Sli,Slice,1856,"pt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:458,deployability,modul,module,458,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:3482,deployability,version,versions,3482," in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, loomR_0.2.1.9000, scanpy==1.4). Am I doing wrong by typing that reading code below?:. `a = scanpy.read_loom('brain10x.loom', sparse=True)`. Thanks...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:3482,integrability,version,versions,3482," in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, loomR_0.2.1.9000, scanpy==1.4). Am I doing wrong by typing that reading code below?:. `a = scanpy.read_loom('brain10x.loom', sparse=True)`. Thanks...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:458,modifiability,modul,module,458,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:563,modifiability,pac,packages,563,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:706,modifiability,layer,layers,706,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:749,modifiability,layer,layers,749,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:801,modifiability,layer,layers,801,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:833,modifiability,layer,layers,833,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:887,modifiability,pac,packages,887,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1059,modifiability,layer,layers,1059," matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1123,modifiability,layer,layers,1123,"ct by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. --",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1181,modifiability,pac,packages,1181,"file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1234,modifiability,layer,layers,1234," by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1273,modifiability,layer,layer,1273," have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1339,modifiability,layer,layer,1339,"---------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no att",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1628,modifiability,pac,packages,1628,"anup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1983,modifiability,pac,packages,1983,". --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2221,modifiability,pac,packages,2221,"s, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2507,modifiability,pac,packages,2507,"ile_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:2830,modifiability,pac,packages,2830," in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, loomR_0.2.1.9000, scanpy==1.4). Am I doing wrong by typing that reading code below?:. `a = scanpy.read_loom('brain10x.loom', sparse=True)`. Thanks...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:3063,modifiability,pac,packages,3063," in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, loomR_0.2.1.9000, scanpy==1.4). Am I doing wrong by typing that reading code below?:. `a = scanpy.read_loom('brain10x.loom', sparse=True)`. Thanks...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:3482,modifiability,version,versions,3482," in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in __init__(self, arg1, shape, dtype, copy). 190 self.data = self.data.astype(dtype, copy=False). 191 . --> 192 self._check(). 193 . 194 def reshape(self, *args, **kwargs):. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py in _check(self). 279 raise ValueError('row index exceeds matrix dimensions'). 280 if self.col.max() >= self.shape[1]:. --> 281 raise ValueError('column index exceeds matrix dimensions'). 282 if self.row.min() < 0:. 283 raise ValueError('negative row index found'). ValueError: column index exceeds matrix dimensions. ```. I can read loom file with loompy seamlessly. They are in the latest versions (Seurat_3.0.0.9000, loomR_0.2.1.9000, scanpy==1.4). Am I doing wrong by typing that reading code below?:. `a = scanpy.read_loom('brain10x.loom', sparse=True)`. Thanks...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:287,performance,error,error,287,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1318,performance,Memor,MemoryLoomLayer,1318,"-------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1711,reliability,sli,slice,1711,"eys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1856,reliability,Sli,Slice,1856,"pt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' object has no attribute {thing}""). 117 else:. --> 118 return self.__getattr__(thing). 119 . 120 def __getattr__(self, name: str) -> sparse.coo_matrix:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getattr__(self, name). 127 c = self.ds._file[a][name][""b""]. 128 w = self.ds._file[a][name][""w""]. --> 129 g = sparse.coo_matrix((w, (r, c)), shape=(self.ds.shape[self.axis], self.ds.shape[self.axis])). 130 self.__dict__[""storage""][name] = g. 131 return g. /opt/conda/lib/python3.7/site-packages/scipy/sparse/coo.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:287,safety,error,error,287,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:432,safety,input,input-,432,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:458,safety,modul,module,458,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:388,testability,Trace,Traceback,388,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:196,usability,close,close,196,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:287,usability,error,error,287,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:432,usability,input,input-,432,"Cannot read loom file created in Seurat3 (column index exceeds matrix dimensions); I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1318,usability,Memor,MemoryLoomLayer,1318,"-------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-7-aed61d3d5eef> in <module>. 1 import scanpy as sc. ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype). 156 . 157 if X_name not in lc.layers.keys(): X_name = ''. --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T. 159 . 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols). 109 col: List[np.ndarray] = []. 110 i = 0. --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):. 112 if rows is not None:. 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size). 597 for key, layer in vals.items():. 598 lm[key] = loompy.MemoryLoomLayer(key, layer). --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs). 600 yield (ix, ix + selection, view). 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:. 97 gm = GraphManager(None, axis=self.axis). ---> 98 for key, g in self.items():. 99 # Slice the graph matrix properly without making it dense. 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self). 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:. 56 for key in self.keys():. ---> 57 yield (key, self[key]). 58 . 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing). 116 raise AttributeError(f""'{type(self)}' o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/599:179,deployability,pipelin,pipeline,179,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:0,integrability,filter,filter,0,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:65,integrability,filter,filter,65,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:179,integrability,pipelin,pipeline,179,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:381,integrability,filter,filtering,381,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:519,integrability,filter,filtering,519,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:18,interoperability,specif,specific,18,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:131,interoperability,specif,specific,131,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:596,reliability,doe,doesn,596,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:328,usability,command,command,328,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:622,usability,help,help,622,"filter cells by a specific gene value; Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito. adata = adata[adata.obs['n_genes'] < 3500, :]. adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav. adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/601:15,availability,error,error,15,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:147,availability,error,error,147,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:7,deployability,version,version,7,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:58,deployability,instal,instal,58,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:76,deployability,version,version,76,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:260,deployability,modul,module,260,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:379,deployability,modul,module,379,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:412,deployability,log,logging,412,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:527,deployability,modul,module,527,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:635,deployability,modul,module,635,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:684,deployability,Modul,ModuleNotFoundError,684,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:708,deployability,modul,module,708,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:7,integrability,version,version,7,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:76,integrability,version,version,76,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:7,modifiability,version,version,7,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:76,modifiability,version,version,76,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:260,modifiability,modul,module,260,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:336,modifiability,pac,packages,336,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:379,modifiability,modul,module,379,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:476,modifiability,pac,packages,476,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:527,modifiability,modul,module,527,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:590,modifiability,pac,packages,590,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:635,modifiability,modul,module,635,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:684,modifiability,Modul,ModuleNotFoundError,684,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:708,modifiability,modul,module,708,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:15,performance,error,error,15,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:147,performance,error,error,147,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:15,safety,error,error,15,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:147,safety,error,error,147,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:260,safety,modul,module,260,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:379,safety,modul,module,379,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:412,safety,log,logging,412,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:527,safety,modul,module,527,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:635,safety,modul,module,635,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:684,safety,Modul,ModuleNotFoundError,684,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:708,safety,modul,module,708,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:412,security,log,logging,412,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:155,testability,Trace,Traceback,155,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:412,testability,log,logging,412,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:15,usability,error,error,15,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:147,usability,error,error,147,"latest version error; Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):. File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>. import scanpy.external as sce. File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>. from . import datasets, logging, queries, external. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>. from . import tl. File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>. from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? . Kind regards,. Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/pull/602:0,availability,Down,Downsample,0,Downsample improvements; Just a minor improvement. Now you can specify the total counts to downsample to on a per-cell basis by passing an array for `counts_per_cell`. This is so I don't have to split and merge an AnnData object when I want multiple distributions of counts per cell.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/602
https://github.com/scverse/scanpy/pull/602:91,availability,down,downsample,91,Downsample improvements; Just a minor improvement. Now you can specify the total counts to downsample to on a per-cell basis by passing an array for `counts_per_cell`. This is so I don't have to split and merge an AnnData object when I want multiple distributions of counts per cell.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/602
https://github.com/scverse/scanpy/pull/602:63,interoperability,specif,specify,63,Downsample improvements; Just a minor improvement. Now you can specify the total counts to downsample to on a per-cell basis by passing an array for `counts_per_cell`. This is so I don't have to split and merge an AnnData object when I want multiple distributions of counts per cell.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/602
https://github.com/scverse/scanpy/pull/602:250,interoperability,distribut,distributions,250,Downsample improvements; Just a minor improvement. Now you can specify the total counts to downsample to on a per-cell basis by passing an array for `counts_per_cell`. This is so I don't have to split and merge an AnnData object when I want multiple distributions of counts per cell.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/602
https://github.com/scverse/scanpy/pull/605:36,safety,test,tests,36,"Some changes to normalize_total and tests; Added tests for normalize_total, changed quantile argument name to fraction.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/605
https://github.com/scverse/scanpy/pull/605:49,safety,test,tests,49,"Some changes to normalize_total and tests; Added tests for normalize_total, changed quantile argument name to fraction.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/605
https://github.com/scverse/scanpy/pull/605:36,testability,test,tests,36,"Some changes to normalize_total and tests; Added tests for normalize_total, changed quantile argument name to fraction.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/605
https://github.com/scverse/scanpy/pull/605:49,testability,test,tests,49,"Some changes to normalize_total and tests; Added tests for normalize_total, changed quantile argument name to fraction.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/605
https://github.com/scverse/scanpy/issues/606:405,availability,cluster,clusters,405,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:638,availability,cluster,clusters,638,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:50,deployability,upgrad,upgraded,50,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:118,deployability,version,versions,118,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:405,deployability,cluster,clusters,405,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:638,deployability,cluster,clusters,638,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:892,deployability,INSTAL,INSTALLED,892,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:902,deployability,VERSION,VERSIONS,902,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:1002,deployability,releas,release,1002,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:26,energy efficiency,heat,heatmap,26,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:154,energy efficiency,heat,heatmap,154,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:442,energy efficiency,heat,heatmap,442,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:604,energy efficiency,heat,heatmap,604,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:1060,energy efficiency,Model,Model,1060,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:118,integrability,version,versions,118,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:902,integrability,VERSION,VERSIONS,902,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:50,modifiability,upgrad,upgraded,50,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:118,modifiability,version,versions,118,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:902,modifiability,VERSION,VERSIONS,902,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:1386,performance,bottleneck,bottleneck,1386,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:390,security,ident,identifying,390,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:1060,security,Model,Model,1060,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:468,usability,user,user-images,468,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:800,usability,learn,learn,800,"Remove white margins from heatmap; Hi! I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:. sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated! Thanks!!! Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . matplotlib == 2.2.3. INSTALLED VERSIONS. ------------------. commit: None. python: 3.7.0.final.0. python-bits: 64. OS: Windows. OS-release: 8.1. machine: AMD64. processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel. byteorder: little. LC_ALL: None. LANG: None. LOCALE: None.None. pandas: 0.23.4. pytest: 3.8.0. pip: 19.0.3. setuptools: 40.2.0. Cython: 0.28.5. numpy: 1.15.4. scipy: 1.1.0. pyarrow: None. xarray: None. IPython: 6.5.0. sphinx: 1.7.9. patsy: 0.5.0. dateutil: 2.7.3. pytz: 2018.5. blosc: None. bottleneck: 1.2.1. tables: 3.4.4. numexpr: 2.6.8. feather: None. matplotlib: 2.2.3. openpyxl: 2.5.6. xlrd: 1.1.0. xlwt: 1.3.0. xlsxwriter: 1.1.0. lxml: 4.2.5. bs4: 4.6.3. html5lib: 1.0.1. sqlalchemy: 1.2.11. pymysql: None. psycopg2: None. jinja2: 2.10. s3fs: None. fastparquet: None. pandas_gbq: None. pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/607:96,deployability,releas,release,96,"single molecule sequencing; Dear,. Scanpy are winning more and more popularity since it's first release in single cell sequencing field. Now single molecule sequencing is growing fast, do you plan to develop a python package for single molecule sequencing analysis (both Nanopore and PacBio) ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:217,modifiability,pac,package,217,"single molecule sequencing; Dear,. Scanpy are winning more and more popularity since it's first release in single cell sequencing field. Now single molecule sequencing is growing fast, do you plan to develop a python package for single molecule sequencing analysis (both Nanopore and PacBio) ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:284,modifiability,Pac,PacBio,284,"single molecule sequencing; Dear,. Scanpy are winning more and more popularity since it's first release in single cell sequencing field. Now single molecule sequencing is growing fast, do you plan to develop a python package for single molecule sequencing analysis (both Nanopore and PacBio) ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:192,testability,plan,plan,192,"single molecule sequencing; Dear,. Scanpy are winning more and more popularity since it's first release in single cell sequencing field. Now single molecule sequencing is growing fast, do you plan to develop a python package for single molecule sequencing analysis (both Nanopore and PacBio) ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/pull/610:132,deployability,api,api,132,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:274,deployability,api,api,274,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:388,deployability,api,api,388,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:525,deployability,api,api,525,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:132,integrability,api,api,132,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:274,integrability,api,api,274,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:388,integrability,api,api,388,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:525,integrability,api,api,525,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:121,interoperability,format,formatting,121,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:132,interoperability,api,api,132,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:263,interoperability,format,formatting,263,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:274,interoperability,api,api,274,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:377,interoperability,format,formatting,377,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:388,interoperability,api,api,388,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:514,interoperability,format,formatting,514,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:525,interoperability,api,api,525,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:5,testability,simpl,simple,5,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:5,usability,simpl,simple,5,"Old, simple method for flat return sections; Everything looks like it should:. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.calculate_qc_metrics.html. Here the rst is weird. Just get rid of the `*`! - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.normalize_total.html. Short prose, looks fine. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.leiden.html. This should be converted into a definition list as well. - https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. Mixed. Looks great. No rtype interfering anywhere.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/issues/612:266,availability,cluster,cluster,266,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:442,availability,cluster,cluster,442,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:548,availability,cluster,cluster,548,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:7,deployability,updat,update,7,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:176,deployability,updat,updating,176,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:266,deployability,cluster,cluster,266,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:442,deployability,cluster,cluster,442,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:548,deployability,cluster,cluster,548,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:34,integrability,sub,subset,34,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:77,integrability,sub,subsetting,77,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:131,integrability,sub,subsetting,131,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:300,integrability,sub,subset,300,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:432,integrability,sub,subset,432,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:538,integrability,sub,subset,538,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:112,reliability,doe,does,112,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:455,reliability,doe,does,455,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:7,safety,updat,update,7,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:176,safety,updat,updating,176,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:7,security,updat,update,7,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:176,security,updat,updating,176,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:259,usability,custom,custom,259,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:758,usability,feedback,feedback,758,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:779,usability,learn,learning,779,"How to update obs-column only for subset of cells?; Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````. # get subset of cells that express GENE1. gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works! adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1. ````. Would you know what the recommended way of doing this is and why is my first solution not working? I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/613:45,integrability,compon,component,45,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:68,integrability,compon,component,68,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:182,integrability,compon,components,182,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:206,integrability,compon,components,206,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:45,interoperability,compon,component,45,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:68,interoperability,compon,component,68,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:163,interoperability,specif,specific,163,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:182,interoperability,compon,components,182,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:206,interoperability,compon,components,206,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:45,modifiability,compon,component,45,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:68,modifiability,compon,component,68,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:182,modifiability,compon,components,182,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:206,modifiability,compon,components,206,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:385,usability,help,help,385,"pl.umap/tsne/draw_graph colored by principal component or diffusion component; Dear scanpy developers,. I would like to color my tsnes, umaps and graph layouts by specific principal components or diffusion components. Any chance you could add an option to use columns from obsm objects as colors for these plots? (Or maybe that's already possible and I missed it...?). Thanks for your help. Best,. Lisa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/pull/614:42,energy efficiency,reduc,reduce,42,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:221,energy efficiency,reduc,reduce,221,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:345,energy efficiency,reduc,reduces,345,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1523,energy efficiency,adapt,adapted,1523,"are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:53,integrability,batch,batch,53,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:167,integrability,batch,batch,167,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:232,integrability,batch,batch,232,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:275,integrability,batch,batch-specific,275,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:328,integrability,batch,batch,328,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:387,integrability,batch,batch,387,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:497,integrability,batch,batch,497,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:749,integrability,batch,batches,749,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:969,integrability,batch,batches,969,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1056,integrability,batch,batches,1056,"ffects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1523,integrability,adapt,adapted,1523,"are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1744,integrability,batch,batch,1744,"y batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/564",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1828,integrability,batch,batch,1828,"anmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2086,integrability,batch,batch,2086,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2279,integrability,batch,batch,2279,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2381,integrability,batch,batch,2381,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2657,integrability,batch,batch,2657,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:121,interoperability,specif,specified,121,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:281,interoperability,specif,specific,281,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1523,interoperability,adapt,adapted,1523,"are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1523,modifiability,adapt,adapted,1523,"are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:53,performance,batch,batch,53,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:167,performance,batch,batch,167,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:232,performance,batch,batch,232,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:275,performance,batch,batch-specific,275,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:328,performance,batch,batch,328,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:387,performance,batch,batch,387,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:497,performance,batch,batch,497,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:749,performance,batch,batches,749,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:969,performance,batch,batches,969,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1056,performance,batch,batches,1056,"ffects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1744,performance,batch,batch,1744,"y batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/564",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1828,performance,batch,batch,1828,"anmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2086,performance,batch,batch,2086,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2279,performance,batch,batch,2279,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2381,performance,batch,batch,2381,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2657,performance,batch,batch,2657,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:302,reliability,doe,doesn,302,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:249,safety,avoid,avoiding,249,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:767,safety,detect,detected,767,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1074,safety,detect,detected,1074,"new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2802,safety,review,review,2802,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2823,safety,Test,Tests,2823,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:767,security,detect,detected,767,"Add a batch_key option to HVG function to reduce the batch effects; I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1074,security,detect,detected,1074,"new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2802,testability,review,review,2802,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2823,testability,Test,Tests,2823,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1873,usability,user,user-images,1873,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2131,usability,user,user-images,2131,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2426,usability,user,user-images,2426,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2702,usability,user,user-images,2702,"tter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python. sc.pp.highly_variable_genes(ad). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python. sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python. sc.pp.highly_variable_genes(ad, n_top_genes=1000). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/615:199,availability,slo,slow,199,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:837,availability,avail,available,837,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:11,deployability,updat,update,11,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:45,deployability,updat,updates,45,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:625,deployability,version,version,625,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:691,deployability,observ,observation,691,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1253,deployability,updat,updates,1253,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1353,deployability,updat,updates,1353,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:625,integrability,version,version,625,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:625,modifiability,version,version,625,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:707,modifiability,variab,variable,707,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1308,modifiability,layer,layers,1308,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:283,performance,parallel,parallel,283,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:316,performance,cach,cached,316,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:353,performance,time,time,353,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:507,performance,parallel,parallelization,507,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:549,performance,perform,performing,549,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:602,performance,cach,cached,602,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:199,reliability,slo,slow,199,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:837,reliability,availab,available,837,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:11,safety,updat,update,11,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:45,safety,updat,updates,45,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:83,safety,Test,Tests,83,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:177,safety,test,tests,177,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:362,safety,test,tests,362,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:837,safety,avail,available,837,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1253,safety,updat,updates,1253,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1353,safety,updat,updates,1353,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:11,security,updat,update,11,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:45,security,updat,updates,45,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:442,security,session,session,442,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:837,security,availab,available,837,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1253,security,updat,updates,1253,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1353,security,updat,updates,1353,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:83,testability,Test,Tests,83,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:177,testability,test,tests,177,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:362,testability,test,tests,362,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:691,testability,observ,observation,691,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:549,usability,perform,performing,549,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1264,usability,User,User,1264,"Qc metrics update; Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python. (adata. .groupby(obs=""leiden""). .apply(sc.pp.describe_var). .combine(...). ). ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`. * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/616:64,availability,state,statement,64,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:83,deployability,build,builds,83,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:64,integrability,state,statement,64,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:152,reliability,doe,doesn,152,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:199,reliability,doe,does,199,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:353,safety,test,test,353,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/616:353,testability,test,test,353,"Minor fixes grab-bag; Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right? 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`. 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/issues/617:20,availability,avail,available,20,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:39,energy efficiency,Current,Currently,39,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:98,interoperability,standard,standard,98,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:243,interoperability,standard,standard,243,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:273,interoperability,standard,standard,273,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:20,reliability,availab,available,20,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:20,safety,avail,available,20,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:20,security,availab,available,20,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:33,usability,user,user,33,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:53,usability,user,users,53,"Make plot_scatter() available to user; Currently, if users are to make scatter plots based on non-standard embeddings, they use `sc.pl.scatter()`, which is functionally similar to `plot_scatter()` but not exactly the same. Be able to plot non-standard embeddings alongside standard embeddings (e.g. using `sc.pl.umap()`) in exactly the same size and style is often desired. For this reason, do you think it's ok to export `plot_scatter()` or even export it as `sc.pl.scatter()`? Many thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/pull/618:70,interoperability,specif,specify,70,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:87,modifiability,variab,variables,87,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:116,modifiability,pac,package,116,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:243,safety,test,tests,243,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:243,testability,test,tests,243,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:20,usability,support,support,20,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:140,usability,support,support,140,Add extra covariate support to combat.; It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/619:189,energy efficiency,core,core,189,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:1011,modifiability,layer,layers,1011,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:121,reliability,doe,does,121,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:14,security,access,access,14,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:135,security,access,access,135,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:360,security,access,access,360,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:436,security,access,access,436,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:550,security,access,access,550,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:780,security,access,accessing,780,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:329,testability,plan,planned,329,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:21,usability,help,helpers,21,"Add some data access helpers to utils; This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/issues/620:1458,availability,error,error,1458,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:8,deployability,fail,fails,8,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1420,deployability,log,logreg,1420,"a = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1536,deployability,log,logreg,1536,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1581,deployability,modul,module,1581,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1657,deployability,log,logreg,1657,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2167,energy efficiency,core,core,2167,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2222,interoperability,format,formats,2222,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2369,interoperability,mismatch,mismatch,2369,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1581,modifiability,modul,module,1581,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2152,modifiability,pac,packages,2152,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1458,performance,error,error,1458,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:8,reliability,fail,fails,8,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2,safety,test,tests,2,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:68,safety,test,test,68,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:686,safety,test,test,686,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:901,safety,test,test,901,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1420,safety,log,logreg,1420,"a = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1458,safety,error,error,1458,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1536,safety,log,logreg,1536,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1555,safety,input,input-,1555,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1581,safety,modul,module,1581,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1657,safety,log,logreg,1657,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1420,security,log,logreg,1420,"a = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1536,security,log,logreg,1536,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1657,security,log,logreg,1657,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:2,testability,test,tests,2,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:68,testability,test,test,68,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:686,testability,test,test,686,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:901,testability,test,test,901,"t-tests fails when variance of both groups is 0; So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)). a[:10, 0] = 5. adata = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1420,testability,log,logreg,1420,"a = sc.AnnData(. a,. obs=pd.DataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1536,testability,log,logreg,1536,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1657,testability,log,logreg,1657,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1458,usability,error,error,1458,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1555,usability,input,input-,1555,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1690,usability,tool,tools,1690,"ataFrame(. {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},. index=[f""cell{i}"" for i in range(a.shape[0])]. ),. var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),. ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])). # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""). print(adata.uns[""rank_genes_groups""][""pvals""][""a""]). # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right. ```. `""logreg""` on the other hand, throws an error:. ```python. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). <ipython-input-7-29e46f287a31> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds). 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(. 398 [n for n in rankings_gene_scores],. --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]). 400 adata.uns[key_added]['names'] = np.rec.fromarrays(. 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder). 615 # Determine shape from data-type. 616 if len(descr) != len(arrayList):. --> 617 raise ValueError(""mismatch between the number of fields "". 618 ""and the number of arrays""). 619 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/pull/621:6,safety,test,tests,6,"Fix t-tests when variance is zero; Just a quick fix, should probably replace most of this code with `scipy.stats.ttest_ind`/ `scipy.stats.stats.ttest_ind_from_stats` with `equal_var=False`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:6,testability,test,tests,6,"Fix t-tests when variance is zero; Just a quick fix, should probably replace most of this code with `scipy.stats.ttest_ind`/ `scipy.stats.stats.ttest_ind_from_stats` with `equal_var=False`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/622:42,energy efficiency,reduc,reduce,42,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:272,energy efficiency,reduc,reduce,272,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:396,energy efficiency,reduc,reduces,396,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1411,energy efficiency,adapt,adapted,1411,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:53,integrability,batch,batch,53,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:218,integrability,batch,batch,218,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:283,integrability,batch,batch,283,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:326,integrability,batch,batch-specific,326,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:379,integrability,batch,batch,379,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:438,integrability,batch,batch,438,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:548,integrability,batch,batch,548,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:789,integrability,batch,batches,789,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:860,integrability,batch,batches,860,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:944,integrability,batch,batches,944,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1411,integrability,adapt,adapted,1411,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1632,integrability,batch,batch,1632,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1716,integrability,batch,batch,1716,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:172,interoperability,specif,specified,172,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:332,interoperability,specif,specific,332,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1411,interoperability,adapt,adapted,1411,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1411,modifiability,adapt,adapted,1411,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:53,performance,batch,batch,53,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:218,performance,batch,batch,218,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:283,performance,batch,batch,283,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:326,performance,batch,batch-specific,326,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:379,performance,batch,batch,379,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:438,performance,batch,batch,438,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:548,performance,batch,batch,548,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:789,performance,batch,batches,789,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:860,performance,batch,batches,860,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:944,performance,batch,batches,944,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1632,performance,batch,batch,1632,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1716,performance,batch,batch,1716,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:353,reliability,doe,doesn,353,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:300,safety,avoid,avoiding,300,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:807,safety,detect,detected,807,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:962,safety,detect,detected,962,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:807,security,detect,detected,807,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:962,security,detect,detected,962,"Add a batch_key option to HVG function to reduce the batch effects; (sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe. The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python. import scanpy as sc. import numpy as np. import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo. ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs). sc.pp.normalize_per_cell(ad). sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'). sc.pp.pca(ad). sc.pp.neighbors(ad). sc.tl.umap(ad). sc.pl.umap(ad, color=[""batch"", ""cell_type""]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/623:0,deployability,Log,Log,0,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:52,deployability,pipelin,pipeline,52,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:52,integrability,pipelin,pipeline,52,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:8,performance,time,time,8,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:0,safety,Log,Log,0,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:0,security,Log,Log,0,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:0,testability,Log,Log,0,"Log PCA time at info level, like other steps in the pipeline (recipes…; …, neighbors).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/624:332,availability,sli,slightly,332,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:310,deployability,version,version,310,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:423,deployability,contain,containing,423,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:587,deployability,version,version,587,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:705,deployability,version,version,705,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:768,deployability,version,version,768,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:205,energy efficiency,current,currently,205,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:302,energy efficiency,current,current,302,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:811,energy efficiency,current,current,811,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:310,integrability,version,version,310,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:587,integrability,version,version,587,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:705,integrability,version,version,705,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:768,integrability,version,version,768,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:248,modifiability,paramet,parameter,248,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:310,modifiability,version,version,310,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:587,modifiability,version,version,587,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:674,modifiability,paramet,parameter,674,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:705,modifiability,version,version,705,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:768,modifiability,version,version,768,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:332,reliability,sli,slightly,332,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/pull/624:713,reliability,doe,does,713,"Take n_bins into account for the cell_ranger flavor of highly_variable_genes; I've changed one line in the highly_variable_genes function, so that n_bins is taken into account with the cell_ranger flavor (currently only the seurat flavor uses this parameter). Additionally, I have noticed that, in the current version, the bins are slightly offset: after -INF, it starts at 10, instead of 5, which results in the first bin containing twice as many genes as the other bins. I don't know if this is intentional (for example, to exactly reproduce the results of cell ranger) or not. In the version that I suggest, I have removed this offset. As a consequence, with the default parameter of n_bins=20, my new version does not exactly reproduce the results of the previous version. In order to exactly reproduce the current results, we would have to keep this offset by doing range(2,n_bins+1) instead of range(1,n_bins).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/624
https://github.com/scverse/scanpy/issues/625:10,availability,down,downregulated,10,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:108,availability,cluster,clusters,108,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:515,availability,avail,available,515,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:108,deployability,cluster,clusters,108,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:371,deployability,version,versions,371,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:371,integrability,version,versions,371,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:386,integrability,topic,topics,386,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:355,modifiability,pac,packages,355,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:371,modifiability,version,versions,371,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:515,reliability,availab,available,515,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:233,safety,test,test,233,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:318,safety,test,test,318,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:515,safety,avail,available,515,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:515,security,availab,available,515,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:233,testability,test,test,233,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:318,testability,test,test,318,"Calculate downregulated genes?; Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers). Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks! Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/626:240,deployability,modul,module,240,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1171,deployability,log,logg,1171,"eback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1396,energy efficiency,load,load,1396,"/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2862,integrability,wrap,wrapper,2862,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2918,integrability,wrap,wrapper,2918,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2862,interoperability,wrapper,wrapper,2862,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2918,interoperability,wrapper,wrapper,2918,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:240,modifiability,modul,module,240,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:456,modifiability,pac,packages,456,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:898,modifiability,pac,packages,898,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1300,modifiability,pac,packages,1300,"FIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1593,modifiability,pac,packages,1593,"ad(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1905,modifiability,pac,packages,1905,"anpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2217,modifiability,pac,packages,2217,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2547,modifiability,pac,packages,2547,":. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr). 270 . 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr). 97 if swmr and swmr_support:. 98 flags |= h5f.ACC_SWMR_READ. ---> 99 fid = h5f.open(name, flags, fapl=fapl). 100 elif mode == 'r+':. 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:566,performance,cach,cache,566,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:743,performance,cach,cache,743,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:749,performance,cach,cache,749,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1009,performance,cach,cache,1009,"h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1396,performance,load,load,1396,"/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1417,performance,memor,memory,1417,"conda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:213,safety,input,input-,213,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:240,safety,modul,module,240,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1171,safety,log,logg,1171,"eback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1171,security,log,logg,1171,"eback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1171,testability,log,logg,1171,"eback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:213,usability,input,input-,213,"Cannot open h5ad file; ```. adata=sc.read('./CONFIDENTIAL_04022019.h5ad'). ```. ---------------------------------------------------------------------------. ```. OSErrorTraceback (most recent call last). <ipython-input-11-759ccdc7c8be> in <module>(). ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'). 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1417,usability,memor,memory,1417,"conda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 433 if ext in {'h5', 'h5ad'}:. 434 if sheet is None:. --> 435 return read_h5ad(filename, backed=backed). 436 else:. 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 442 else:. 443 # load everything into memory. --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)). 445 . 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 471 f = adata.file._file. 472 else:. --> 473 f = h5py.File(filename, 'r'). 474 for key in f.keys():. 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds). 139 userblock_size=userblock_size,. 140 swmr=swmr,. --> 141 **kwds,. 142 ). 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds). 267 with phil:. 268 fapl = make_fapl(driver, libver, **kwds). --> 269 fid = make_fid(name, mode, us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/627:144,deployability,observ,observation,144,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:474,deployability,observ,observation,474,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:545,deployability,updat,updated,545,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:230,modifiability,paramet,parameter,230,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:56,safety,input,input,56,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:545,safety,updat,updated,545,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:563,safety,test,tested,563,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:593,safety,test,test,593,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:162,security,modif,modified,162,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:545,security,updat,updated,545,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:144,testability,observ,observation,144,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:474,testability,observ,observation,474,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:563,testability,test,tested,563,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:593,testability,test,test,593,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:56,usability,input,input,56,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:100,usability,help,help,100,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:625,usability,tool,tool,625,"Enhancement; Hi, . I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks. [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/pull/628:116,usability,user,user-images,116,Multi-panel embedded density; This PR adds multiple panels to the plot embedding_density function. ![image](https://user-images.githubusercontent.com/4964309/57077777-2a0c2a00-6ced-11e9-8728-adff9c04f3b3.png).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/issues/629:25,safety,test,test,25,"Weird scipy warning in t-test when there are all-zero genes; After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python. adata = sc.datasets.paul15(). adata.X[:, 10] = 0.0. sc.tl.rank_genes_groups(adata, 'paul15_clusters'). ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:88,safety,test,test,88,"Weird scipy warning in t-test when there are all-zero genes; After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python. adata = sc.datasets.paul15(). adata.X[:, 10] = 0.0. sc.tl.rank_genes_groups(adata, 'paul15_clusters'). ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:25,testability,test,test,25,"Weird scipy warning in t-test when there are all-zero genes; After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python. adata = sc.datasets.paul15(). adata.X[:, 10] = 0.0. sc.tl.rank_genes_groups(adata, 'paul15_clusters'). ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:88,testability,test,test,88,"Weird scipy warning in t-test when there are all-zero genes; After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python. adata = sc.datasets.paul15(). adata.X[:, 10] = 0.0. sc.tl.rank_genes_groups(adata, 'paul15_clusters'). ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:339,usability,user,user-images,339,"Weird scipy warning in t-test when there are all-zero genes; After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python. adata = sc.datasets.paul15(). adata.X[:, 10] = 0.0. sc.tl.rank_genes_groups(adata, 'paul15_clusters'). ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/pull/630:59,availability,cluster,clustering,59,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:433,availability,cluster,clustering,433,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:0,deployability,updat,updated,0,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:59,deployability,cluster,clustering,59,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:336,deployability,updat,updated,336,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:433,deployability,cluster,clustering,433,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:473,deployability,updat,update,473,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:319,energy efficiency,heat,heatmap,319,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:51,performance,perform,perform,51,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:0,safety,updat,updated,0,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:193,safety,input,input,193,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:336,safety,updat,updated,336,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:473,safety,updat,update,473,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:0,security,updat,updated,0,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:336,security,updat,updated,336,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:473,security,updat,update,473,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:18,usability,support,support,18,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:51,usability,perform,perform,51,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:193,usability,input,input,193,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:421,usability,support,support,421,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:504,usability,support,support,504,"updated Scanpy to support Weighted sampled data to perform clustering…; I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/issues/631:170,usability,command,command,170,"scanpy.pp.normalize_total; Hello, I used the method normalized_total with a sparse matrix and inplace=False but it returned always None. I think that the problem is this command in the function _normalize_data: . X = sparsefuncs.inplace_row_scale(X, 1/counts). this should be replaced with:. sparsefuncs.inplace_row_scale(X, 1/counts). is it correct? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/632:6,availability,error,error,6,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:98,availability,error,error,98,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:454,availability,Error,Error,454,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:156,deployability,Version,Version,156,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:723,deployability,modul,module,723,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1049,deployability,instal,install,1049,"ror; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1069,deployability,instal,install,1069,"ror; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:57,integrability,batch,batch,57,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:156,integrability,Version,Version,156,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:539,integrability,batch,batch,539,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:156,modifiability,Version,Version,156,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:167,modifiability,pac,packages,167,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:723,modifiability,modul,module,723,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:902,modifiability,pac,packages,902,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1162,modifiability,pac,packages,1162,"ror; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1493,modifiability,pac,packages,1493,"ror; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:6,performance,error,error,6,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:57,performance,batch,batch,57,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:98,performance,error,error,98,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:454,performance,Error,Error,454,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:539,performance,batch,batch,539,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:6,safety,error,error,6,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:98,safety,error,error,98,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:454,safety,Error,Error,454,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:696,safety,input,input-,696,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:723,safety,modul,module,723,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:999,safety,except,except,999,"knn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:652,testability,Trace,Traceback,652,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:6,usability,error,error,6,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:98,usability,error,error,98,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:275,usability,learn,learn,275,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:454,usability,Error,Error,454,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:696,usability,input,input-,696,"bbknn error; Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```. scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1. pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Cmds:. ```py. import scanpy.external as sce. sce.pp.bbknn(adata, batch_key='sample', copy=False). ```. Error info:. ```pytb. sce.pp.bbknn(adata, batch_key='sample', copy=False). computing batch balanced neighbors. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-34-5b7ebd13c9e6> in <module>. 1 # Correct. 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'). ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 except ImportError:. 83 raise ImportError('Please install bbknn: `pip install bbknn`.'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn). 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], . 273 knn_indices.shape[1], bandwidth=bandwidth,. --> 274 											 local_connectivity=local_connectivity). 275 #optional trimming. 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/633:357,availability,Cluster,ClusterMap,357,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:357,deployability,Cluster,ClusterMap,357,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:156,energy efficiency,heat,heatmap,156,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:196,energy efficiency,heat,heatmap,196,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:452,integrability,messag,message,452,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:979,integrability,messag,message,979,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:452,interoperability,messag,message,452,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:979,interoperability,messag,message,979,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:616,modifiability,pac,packages,616,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:915,reliability,doe,does,915,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:248,security,ident,ident,248,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:997,testability,understand,understand,997,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:175,usability,command,command,175,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:525,usability,visual,visualized,525,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:660,usability,User,UserWarning,660,"Tha maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached; I was trying to plot a heatmap using this command:. `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/634:281,integrability,filter,filtered,281,"Losing transcripts during analyses; This may already have been discussed elsewhere... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. Very much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/634:508,integrability,filter,filtered,508,"Losing transcripts during analyses; This may already have been discussed elsewhere... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. Very much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/634:650,integrability,filter,filter,650,"Losing transcripts during analyses; This may already have been discussed elsewhere... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. Very much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/634:636,reliability,Doe,Does,636,"Losing transcripts during analyses; This may already have been discussed elsewhere... It appears that I am loosing a bunch of genes/transcripts during the Velocyto analysis, and I am trying to figure out what is going on. After running Velocyto, there are approximately 5000 final filtered transcripts (presumably ones used for the vector calculation). However, several genes/transcripts of interest that should be highly expressed in most cells (seen using software like Seurat) are not listed in the final filtered genes. Thus, I am suspecting that many genes/transcripts are, for some reason, being lost during Velocyto calculation. Does Velocyto filter out genes that are not differentially expressed (namely, genes/transcripts that are not useful to plot vector)? My cells are captured using Fluidigm. Very much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/635:23,deployability,updat,update,23,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:6,integrability,wrap,wrapper,6,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:84,integrability,wrap,wrapper,84,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:6,interoperability,wrapper,wrapper,6,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:84,interoperability,wrapper,wrapper,84,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:23,safety,updat,update,23,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/635:23,security,updat,update,23,"bbknn wrapper needs an update; Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/pull/636:53,deployability,updat,update,53,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:233,deployability,updat,update,233,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:176,integrability,wrap,wrapper,176,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:176,interoperability,wrapper,wrapper,176,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:102,reliability,doe,doesn,102,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:53,safety,updat,update,53,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:233,safety,updat,update,233,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:53,security,updat,update,53,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:233,security,updat,update,233,"BBKNN 1.3.2 obsoleted argument removal and docstring update; I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/issues/637:279,availability,Cluster,ClusterMap,279,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:279,deployability,Cluster,ClusterMap,279,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:6,energy efficiency,heat,heatmap,6,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:68,energy efficiency,heat,heatmap,68,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:505,energy efficiency,heat,heatmap,505,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:757,energy efficiency,heat,heatmap,757,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:14,reliability,doe,doesn,14,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:140,security,ident,ident,140,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:375,usability,user,user-images,375,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:608,usability,user,user-images,608,"sc.pl.heatmap doesn't align boundaries well; I ran this:. `ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident',. use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7),. var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. But the image has something weird. Here are the snapshots:. ![image](https://user-images.githubusercontent.com/4110443/57493149-b5705700-72bb-11e9-8d2d-e02c083aeda2.png). The lines don't align well with the heatmap. . Additionally, the lines don't align well with the group ID colors, either. ![image](https://user-images.githubusercontent.com/4110443/57493206-e94b7c80-72bb-11e9-9bd8-80cdaa8c0754.png). And the ID colors seem to be not aligned well with the heatmap either. Any thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/638:9,security,sign,signature,9,"Plotting signature genes; Dear all,. I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that. In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done! Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:189,security,sign,signature,189,"Plotting signature genes; Dear all,. I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that. In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done! Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:498,usability,efficien,efficiently,498,"Plotting signature genes; Dear all,. I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that. In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done! Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:520,usability,help,help,520,"Plotting signature genes; Dear all,. I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that. In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done! Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/639:134,availability,error,error,134,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:989,availability,error,error,989,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:172,deployability,stage,stage,172,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:134,performance,error,error,134,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:989,performance,error,error,989,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:134,safety,error,error,134,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:989,safety,error,error,989,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:134,usability,error,error,134,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:422,usability,user,user-images,422,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:989,usability,error,error,989,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:1020,usability,help,help,1020,"Can't find mito_genes; I am trying to follow this tutorial . https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,. but some error just happened in the initial QC stage. After running:. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. and plot the violinplot,. I just get this graph. ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:. __________________________________________________________________________________________________. [In] np.sum(adata.obs). [Out] n_genes 14918559.0. **percent_mito 0.0**. dtype: float64. __________________________________________________________________________________________________. It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/pull/640:61,usability,support,support,61,"Add outline to the legend text in scatterplots; This PR adds support to add an outline around the legend text in scatterplots. Its effect is similar to fontweight='bold' but arguably more elegant and readable. Before:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data') `. ![image](https://user-images.githubusercontent.com/1140359/57564012-448c7600-7373-11e9-8f8f-21385e560739.png). After:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data', legend_fontoutline=1, legend_fontweight='normal')`. ![image](https://user-images.githubusercontent.com/1140359/57564018-62f27180-7373-11e9-96dc-a38bc709fc2b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:305,usability,user,user-images,305,"Add outline to the legend text in scatterplots; This PR adds support to add an outline around the legend text in scatterplots. Its effect is similar to fontweight='bold' but arguably more elegant and readable. Before:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data') `. ![image](https://user-images.githubusercontent.com/1140359/57564012-448c7600-7373-11e9-8f8f-21385e560739.png). After:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data', legend_fontoutline=1, legend_fontweight='normal')`. ![image](https://user-images.githubusercontent.com/1140359/57564018-62f27180-7373-11e9-96dc-a38bc709fc2b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:542,usability,user,user-images,542,"Add outline to the legend text in scatterplots; This PR adds support to add an outline around the legend text in scatterplots. Its effect is similar to fontweight='bold' but arguably more elegant and readable. Before:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data') `. ![image](https://user-images.githubusercontent.com/1140359/57564012-448c7600-7373-11e9-8f8f-21385e560739.png). After:. `sc.pl.umap(adata, color='paul15_clusters', legend_loc='on data', legend_fontoutline=1, legend_fontweight='normal')`. ![image](https://user-images.githubusercontent.com/1140359/57564018-62f27180-7373-11e9-96dc-a38bc709fc2b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/issues/641:0,availability,Error,Error,0,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:484,availability,error,error,484,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:495,availability,error,error,495,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2425,availability,error,error,2425,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:720,deployability,modul,module,720,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1476,energy efficiency,estimat,estimator,1476,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:91,integrability,repositor,repository,91,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1027,integrability,sub,subset,1027," scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:91,interoperability,repositor,repository,91,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1613,interoperability,Standard,StandardScaler,1613,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:720,modifiability,modul,module,720,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:870,modifiability,pac,packages,870,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1333,modifiability,pac,packages,1333,"c.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dime",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1672,modifiability,pac,packages,1672,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:1895,modifiability,pac,packages,1895,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2092,modifiability,pac,packages,2092,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:0,performance,Error,Error,0,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:484,performance,error,error,484,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:495,performance,error,error,495,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2425,performance,error,error,2425,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:501,reliability,doe,does,501,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:0,safety,Error,Error,0,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:484,safety,error,error,484,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:495,safety,error,error,495,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:693,safety,input,input-,693,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:720,safety,modul,module,720,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2425,safety,error,error,2425,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:649,testability,Trace,Traceback,649,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:0,usability,Error,Error,0,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:267,usability,command,commands,267,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:484,usability,error,error,484,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:495,usability,error,error,495,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:547,usability,command,command,547,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:693,usability,input,input-,693,"Error after normalization with scran; Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`. After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```. adata.X /= adata.obs['size_factors'].values[:,None]. sc.pp.log1p(adata). ```. When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2394,usability,hint,hints,2394,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2425,usability,error,error,2425,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```. LinAlgError Traceback (most recent call last). <ipython-input-97-96c692867dde> in <module>. ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace). 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X. 102 . --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)). 104 # now actually compute the dispersion. 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X). 16 mean_sq = np.multiply(X, X).mean(axis=0). 17 # enforece R convention (unbiased estimator) for variance. ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)). 19 else:. 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other). 226 . 227 def __pow__(self, other):. --> 228 return matrix_power(self, other). 229 . 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n). 600 a = asanyarray(a). 601 _assertRankAtLeast2(a). --> 602 _assertNdSquareness(a). 603 . 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays). 213 m, n = a.shape[-2:]. 214 if m != n:. --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'). 216 . 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square. ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/642:771,performance,time,time,771,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/642:116,usability,command,command,116,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/642:516,usability,user,user-images,516,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/642:651,usability,command,command,651,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/642:824,usability,user,user-images,824,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/642:971,usability,help,help,971,"MAGIC: different results from scanpy and magic.op; Hello everyone,. I have tried MAGIC recently using the following command:. > `adata_magic=sc.pp.magic(adata,copy=True,name_list=""all_genes"",)`. After running magic when I looked to the relation of some some genes I realized nothing has happened because the plot I get is the same for both before magic and after magic:. `sc.pl.scatter(adata, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. `sc.pl.scatter(adata_magic, x=""Csf1r"",y=""Adgre1"", color=""Csf1r"")`. ![image](https://user-images.githubusercontent.com/46027703/57604082-8da31c80-7563-11e9-9ae7-3bb761c6d57f.png). I tried running magic with the original command as follow:. `magic_op = magic.MAGIC(). adata_magic= magic_op.fit_transform(adata, genes=""all_genes"")`. and this time my plot looked very different. ![image](https://user-images.githubusercontent.com/46027703/57604141-bd522480-7563-11e9-8296-e19266ae1cf7.png). I am quite confused now...can someone kindly please help me in this regard? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/642
https://github.com/scverse/scanpy/issues/643:109,availability,error,error,109,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:82,deployability,api,api,82,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:366,deployability,version,version,366,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:82,integrability,api,api,82,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:366,integrability,version,version,366,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:82,interoperability,api,api,82,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:366,modifiability,version,version,366,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:109,performance,error,error,109,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:109,safety,error,error,109,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:109,usability,error,error,109,"ImportError: cannot import name 'IndexMixin'; Hi,. When I ran. ```. import scanpy.api as sc. ```. I met this error:. ```. 19 from scipy import sparse. 20 from scipy.sparse import issparse. ---> 21 from scipy.sparse.sputils import IndexMixin. 22 from natsort import natsorted. 23 . ImportError: cannot import name 'IndexMixin'. ```. Is there any requirements for the version of scipy? Thanks in advance,. BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/pull/644:120,deployability,version,version,120,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:275,energy efficiency,heat,heatmap,275,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:120,integrability,version,version,120,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:120,modifiability,version,version,120,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:404,safety,test,test,404,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:79,security,modif,modified,79,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:392,security,modif,modified,392,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:477,security,modif,modify,477,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:404,testability,test,test,404,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:0,usability,support,support,0,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:157,usability,support,support,157,"support for weighted sampled data to rank genes and plot (dot, violin…; I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks. Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/issues/645:66,availability,error,error,66,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:612,availability,error,error,612,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:726,availability,operat,operation,726,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1050,deployability,version,version,1050,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1073,deployability,log,logging,1073,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1050,integrability,version,version,1050,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1283,integrability,sub,subset,1283,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1050,modifiability,version,version,1050,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1058,modifiability,pac,packages,1058,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1293,modifiability,variab,variables,1293,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:66,performance,error,error,66,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:112,performance,perform,perform,112,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:186,performance,perform,perform,186,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:612,performance,error,error,612,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:66,safety,error,error,66,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:612,safety,error,error,612,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1073,safety,log,logging,1073,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:952,security,modif,modify,952,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1073,security,log,logging,1073,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1073,testability,log,logging,1073,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:66,usability,error,error,66,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:112,usability,perform,perform,112,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:186,usability,perform,perform,186,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:612,usability,error,error,612,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1190,usability,learn,learn,1190,"Exporting result after cell cycle score; Hi,. I'm encountering an error when trying to write result file, after perform cell cycle score. After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]. s_genes=[g for g in cc_genes[:43] if g in adata.var_names]. g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes). `. The field 'phase' of the obs. matrix is of type object:. `adata.obs.phase.dtypes. dtype('O')`. When I write the annData object, I got the error:. `adata.write(results_file). ... storing 'phase' as categorical. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:. `adata.obs.phase.dtypes. CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again. Following my version packages:. `sc.logging.print_versions(). scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot! Raffaella.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/646:892,availability,error,error,892,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:545,deployability,api,api,545,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:703,deployability,automat,automatically,703,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:864,deployability,api,api,864,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:856,energy efficiency,current,current,856,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:143,integrability,event,eventually,143,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:545,integrability,api,api,545,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:864,integrability,api,api,864,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:545,interoperability,api,api,545,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:864,interoperability,api,api,864,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:367,modifiability,paramet,parameters,367,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:892,performance,error,error,892,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:892,safety,error,error,892,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:0,testability,Simpl,Simpler,0,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:430,testability,simpl,simple,430,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:703,testability,automat,automatically,703,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:0,usability,Simpl,Simpler,0,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:430,usability,simpl,simple,430,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:892,usability,error,error,892,"Simpler gene annotations in plotting functions; We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/647:1006,availability,sli,sliced,1006,"s and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:432,deployability,modul,module,432,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:754,deployability,observ,observations-annotation,754,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2478,deployability,contain,containing,2478,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2055,energy efficiency,core,core,2055,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2271,energy efficiency,core,core,2271,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:566,integrability,transform,transform,566,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:566,interoperability,transform,transform,566,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:432,modifiability,modul,module,432,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:878,modifiability,pac,packages,878,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1184,modifiability,pac,packages,1184,"=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_index",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1467,modifiability,pac,packages,1467,"tion of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1746,modifiability,pac,packages,1746,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2039,modifiability,pac,packages,2039,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2255,modifiability,pac,packages,2255,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1006,reliability,sli,sliced,1006,"s and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:405,safety,input,input-,405,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:432,safety,modul,module,432,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:26,testability,Trace,Traceback,26,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:361,testability,Trace,Traceback,361,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:754,testability,observ,observations-annotation,754,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:405,usability,input,input-,405,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:841,usability,user,users,841,"mito_genes and ValueError Traceback (most recent call last) ; ```python. mito_genes = adata.var_names.str.startswith('MT-'). adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1147,usability,user,users,1147," = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-31-4b64d0e9fd7f> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1430,usability,user,users,1430,"odule>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 retur",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:1709,usability,user,users,1709,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2002,usability,user,users,2002,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:2218,usability,user,users,2218,"enes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 5 # add the total counts per cell as observations-annotation to adata. 6 adata.obs['n_counts'] = adata.X.sum(axis=1).A1. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in __getitem__(self, index). 1297 def __getitem__(self, index: Index) -> 'AnnData':. 1298 """"""Returns a sliced view of the object."""""". -> 1299 return self._getitem_view(index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _getitem_view(self, index). 1300 . 1301 def _getitem_view(self, index: Index) -> 'AnnData':. -> 1302 oidx, vidx = self._normalize_indices(index). 1303 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1304 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_indices(self, index). 1277 obs, var = super()._unpack_index(index). 1278 obs = _normalize_index(obs, self.obs_names). -> 1279 var = _normalize_index(var, self.var_names). 1280 return obs, var. 1281 . c:\users\gsy\miniconda3\second\lib\site-packages\anndata\base.py in _normalize_index(index, names). 264 # incredibly faster one. 265 positions = pd.Series(index=names, data=range(len(names))). --> 266 positions = positions[index]. 267 if positions.isnull().values.any():. 268 raise KeyError(. c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\series.py in __getitem__(self, key). 906 key = list(key). 907 . --> 908 if com.is_bool_indexer(key):. 909 key = check_bool_indexer(self.index, key). 910 . c:\users\gsy\miniconda3\second\lib\site-packages\pandas\core\common.py in is_bool_indexer(key). 122 if not lib.is_bool_array(key):. 123 if isna(key).any():. --> 124 raise ValueError(na_msg). 125 return False. 126 return True. ValueError: cannot index with vector containing NA / NaN values. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/649:90,availability,error,error,90,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:120,deployability,api,api,120,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:248,deployability,modul,module,248,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:826,energy efficiency,core,core,826,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:969,energy efficiency,core,core,969,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1149,energy efficiency,core,core,1149,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1305,energy efficiency,core,core,1305,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1443,energy efficiency,core,core,1443,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:120,integrability,api,api,120,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:120,interoperability,api,api,120,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:248,modifiability,modul,module,248,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:293,modifiability,pac,packages,293,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:404,modifiability,pac,packages,404,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:529,modifiability,pac,packages,529,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:664,modifiability,pac,packages,664,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:810,modifiability,pac,packages,810,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:953,modifiability,pac,packages,953,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1133,modifiability,pac,packages,1133,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1289,modifiability,pac,packages,1289,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1427,modifiability,pac,packages,1427,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:90,performance,error,error,90,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1596,reliability,Doe,Does,1596,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:0,safety,Except,Exception,0,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:90,safety,error,error,90,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:248,safety,modul,module,248,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1511,safety,Except,Exception,1511,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:1552,safety,Except,Exception,1552,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:184,testability,Trace,Traceback,184,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/649:90,usability,error,error,90,"Exception: Data must be 1-dimensional; Hi, when importing a loom file I get the following error. ```. >>> import scanpy.api as sc. >>> adata = sc.read_loom('/path/to/loom_file.loom'). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom. dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__. filename=filename, filemode=filemode). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual. ['obs_names', 'row_names', 'smp_names']). File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe. columns=[k for k in anno.keys() if k != index_name]). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__. mgr = init_dict(data, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict. return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr. arrays = _homogenize(arrays, index, dtype). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize. raise_cast_failure=False). File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array. raise Exception('Data must be 1-dimensional'). Exception: Data must be 1-dimensional. ```. Does anybody knows the reason why? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/650:302,availability,error,error,302,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:284,integrability,filter,filter,284,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:366,modifiability,pac,packages,366,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:302,performance,error,error,302,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:302,safety,error,error,302,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:302,usability,error,error,302,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:478,usability,support,supported,478,"Type comparison issue when using read_h5ad in backed mode; If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)). `. Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```. File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:. else X > 0, axis=0):. TypeError: '>' not supported between instances of 'SparseDataset' and 'int'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/pull/651:903,availability,cluster,clustering,903,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:963,availability,cluster,clusters,963,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1003,availability,cluster,cluster,1003,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1063,availability,cluster,cluster,1063,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:167,deployability,observ,observations,167,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:476,deployability,observ,observations,476,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:513,deployability,updat,updated,513,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:552,deployability,observ,observations,552,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:892,deployability,updat,update,892,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:903,deployability,cluster,clustering,903,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:932,deployability,observ,observations,932,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:963,deployability,cluster,clusters,963,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1003,deployability,cluster,cluster,1003,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1063,deployability,cluster,cluster,1063,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:389,energy efficiency,model,model,389,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:513,safety,updat,updated,513,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:892,safety,updat,update,892,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:280,security,expos,exposes,280,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:389,security,model,model,389,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:513,security,updat,updated,513,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:892,security,updat,update,892,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:167,testability,observ,observations,167,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:476,testability,observ,observations,476,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:552,testability,observ,observations,552,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:932,testability,observ,observations,932,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:245,usability,learn,learned,245,"Ingest class; As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations. > . > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly. > . > adata_small.obsm['X_model'] = model(adata_small.X). > . > ingest.neighbors(adata_small) # adata_small with just 1000 observations. > . > now, we have the updated neighbors graph with 1,001,000 observations. > we want to do the same things as always. > . > by leveraging the neighbors of the new data within the old data, . > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors. > . > ingest.umap(adata_small). > . > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster . > . > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/issues/653:131,availability,cluster,clusters,131,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:354,availability,cluster,clustering,354,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:378,availability,cluster,clusters,378,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:446,availability,cluster,clustering,446,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:131,deployability,cluster,clusters,131,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:354,deployability,cluster,clustering,354,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:378,deployability,cluster,clusters,378,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:446,deployability,cluster,clustering,446,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:336,integrability,sub,subset,336,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:583,integrability,sub,subset,583,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:513,reliability,Doe,Does,513,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:482,usability,command,command,482,"Runtime warning by rank_genes_groups; Dear all, . I am receiving the following runtime warning when I search for markers within my clusters using. `sc.tl.rank_genes_groups`. > RuntimeWarning: invalid value encountered in log2. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). The warning only happened after I subset my initial clustering and keep few clusters and then again run PCA and HVG analysis on them and do the clustering. It still though run the command and I get the results. Does anyone know why is it happening right after I want to analyze my subset? and is it something that I should worry about ? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/654:156,availability,cluster,clustering,156,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:431,availability,down,downgrading,431,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:578,availability,cluster,clustering,578,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:156,deployability,cluster,clustering,156,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:578,deployability,cluster,clustering,578,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:67,reliability,doe,does,67,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:41,usability,learn,learn,41,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:120,usability,learn,learn,120,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:333,usability,user,user-images,333,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:450,usability,learn,learn,450,"Scanpy not working correctly with scikit-learn 0.21.1; Hey! Scanpy does not seem to work correctly together with scikit-learn 0.21.1. When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference. ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine. The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/655:7,availability,error,error,7,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:146,deployability,log,logging,146,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:783,deployability,modul,module,783,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:783,modifiability,modul,module,783,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:7,performance,error,error,7,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:7,safety,error,error,7,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:146,safety,log,logging,146,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:756,safety,input,input-,756,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:783,safety,modul,module,783,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:146,security,log,logging,146,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:146,testability,log,logging,146,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:712,testability,Trace,Traceback,712,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:7,usability,error,error,7,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:35,usability,Minim,Minimal,35,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:534,usability,learn,learn,534,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:756,usability,input,input-,756,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:987,usability,tool,tools,987,"Import error when running cyclone; Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):. ```python. import scanpy as sc. sc.logging.print_versions(). from pypairs import __version__. print('pypairs==', __version__). adata = sc.datasets.blobs(). marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ```. ```python. scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . pypairs== v3.1.0. ---------------------------------------------------------------------------. ImportError Traceback (most recent call last). <ipython-input-20-33f7b7cad989> in <module>. 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. 8 . ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs). 132 . 133 from pypairs.pairs import cyclone. --> 134 from . import settings. 135 from pypairs import settings as pp_settings. 136 . ImportError: cannot import name 'settings'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/657:147,security,sign,signature,147,"sandbag docs inconsistent; `sc.external.tl.sandbag` mentions `categories` as a paramter but `annotation` should be given according to the function signature. According to the [pypairs documentation](https://pypairs.readthedocs.io/en/latest/pypairs.pairs.sandbag.html#pypairs.pairs.sandbag) annotation should not be passed if data is an AnnData object. Instead, pypairs expects this information in `data.vars['category']`. So, I do not know how to fix this best. If you have an idea, I can add it to #656.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/657
https://github.com/scverse/scanpy/issues/657:184,usability,document,documentation,184,"sandbag docs inconsistent; `sc.external.tl.sandbag` mentions `categories` as a paramter but `annotation` should be given according to the function signature. According to the [pypairs documentation](https://pypairs.readthedocs.io/en/latest/pypairs.pairs.sandbag.html#pypairs.pairs.sandbag) annotation should not be passed if data is an AnnData object. Instead, pypairs expects this information in `data.vars['category']`. So, I do not know how to fix this best. If you have an idea, I can add it to #656.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/657
https://github.com/scverse/scanpy/issues/658:200,availability,redund,redundant,200,"Linking patient data with cells; Hey! I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:200,deployability,redundan,redundant,200,"Linking patient data with cells; Hey! I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:200,reliability,redundan,redundant,200,"Linking patient data with cells; Hey! I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:200,safety,redund,redundant,200,"Linking patient data with cells; Hey! I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/pull/659:16,deployability,depend,dependency,16,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:88,deployability,releas,released,88,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:311,deployability,manag,manager,311,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:311,energy efficiency,manag,manager,311,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:482,energy efficiency,core,core,482,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:947,energy efficiency,core,cores,947,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:16,integrability,depend,dependency,16,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:248,integrability,wrap,wrap,248,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:16,modifiability,depend,dependency,16,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:124,performance,multi-thread,multi-threading,124,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:188,performance,multi-thread,multi-threading,188,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:16,safety,depend,dependency,16,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:311,safety,manag,manager,311,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:16,testability,depend,dependency,16,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:303,testability,context,context,303,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:30,usability,support,support,30,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:112,usability,support,support,112,"Use pynndescent dependency to support threaded nearest neighbors; Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=16):. sc.pp.neighbors(adata). ```. Running on the 130K dataset on a 16 core machine before the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:01:31.54). ```. and with the change:. ```. computing neighbors. using 'X_pca' with n_pcs = 50. finished (0:00:32.02). ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/issues/660:325,usability,user,user-images,325,"scanpy.pp.regress_out(adata) scipy issue; Hello, . I have an issue using this function. It was working perfectly until today. Now, when i try to run regress_out i have an issue with scipy:. --> from scipy.misc import factorial. cannot import name 'factorial'. Any idea to solve this problem ? . Thank you. . ![image](https://user-images.githubusercontent.com/47348139/58240057-737af280-7d4a-11e9-9df5-aaa8b8da432c.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660
https://github.com/scverse/scanpy/pull/661:1227,deployability,Updat,Update,1227,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1247,deployability,depend,dependencies,1247,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1309,deployability,fail,failing,1309,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:111,energy efficiency,heat,heatmap,111,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1247,integrability,depend,dependencies,1247,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1344,interoperability,convers,conversation,1344,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:999,modifiability,paramet,parameter,999,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1088,modifiability,extens,extension,1088,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1186,modifiability,exten,extend,1186,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1247,modifiability,depend,dependencies,1247,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1309,reliability,fail,failing,1309,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1161,safety,avoid,avoid,1161,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1227,safety,Updat,Update,1227,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1247,safety,depend,dependencies,1247,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1317,safety,test,tests,1317,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1227,security,Updat,Update,1227,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1247,testability,depend,dependencies,1247,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1317,testability,test,tests,1317,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:810,usability,user,user-images,810,"Dict param to plots #646; This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON. marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], . 'T-cell': 'CD3D',. 'T-cell CD8+': ['CD8A', 'CD8B'],. 'NK': ['GNLY', 'NKG7'],. 'Myeloid': ['CST3', 'LYZ'],. 'Monocytes': ['FCGR3A'],. 'Dendritic': ['FCER1A']}. # use marker genes as dict to group them. ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'). ```. ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/issues/662:406,availability,error,error,406,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:383,deployability,fail,fails,383,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:589,interoperability,specif,specify,589,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:370,modifiability,scal,scaling,370,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:406,performance,error,error,406,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:383,reliability,fail,fails,383,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:406,safety,error,error,406,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:120,usability,command,command,120,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:406,usability,error,error,406,"HVG by cell_ranger flavor, n_top_genes not working; Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:. _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/663:639,availability,error,error,639,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:35,deployability,updat,updates,35,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:313,deployability,updat,update,313,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:578,deployability,updat,updating,578,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:829,deployability,roll,roll,829,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:855,deployability,version,version,855,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:930,deployability,version,version,930,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:855,integrability,version,version,855,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:930,integrability,version,version,930,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:27,modifiability,pac,package,27,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:331,modifiability,pac,packages,331,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:394,modifiability,pac,packages,394,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:598,modifiability,pac,packages,598,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:855,modifiability,version,version,855,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:922,modifiability,pac,package,922,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:930,modifiability,version,version,930,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:983,modifiability,pac,packages,983,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:639,performance,error,error,639,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:35,safety,updat,updates,35,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:313,safety,updat,update,313,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:578,safety,updat,updating,578,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:639,safety,error,error,639,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:35,security,updat,updates,35,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:313,security,updat,update,313,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:578,security,updat,updating,578,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:486,usability,learn,learn,486,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:639,usability,error,error,639,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:739,usability,learn,learn,739,"Problem with 3D UMAP after package updates; Dear all. I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error! > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems? Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/665:0,deployability,Upgrad,Upgrading,0,Upgrading scikit-learn from 0.20.1 to 0.21.1 messes up paul15 example; Need to investigate what's going wrong.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/665
https://github.com/scverse/scanpy/issues/665:0,modifiability,Upgrad,Upgrading,0,Upgrading scikit-learn from 0.20.1 to 0.21.1 messes up paul15 example; Need to investigate what's going wrong.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/665
https://github.com/scverse/scanpy/issues/665:17,usability,learn,learn,17,Upgrading scikit-learn from 0.20.1 to 0.21.1 messes up paul15 example; Need to investigate what's going wrong.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/665
https://github.com/scverse/scanpy/issues/666:17,availability,error,error,17,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:80,availability,error,error,80,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:166,availability,error,error,166,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:240,availability,error,error,240,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:330,availability,error,error,330,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1626,availability,error,errors,1626,"_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3144,availability,error,errors,3144,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3335,availability,error,error,3335,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:590,deployability,modul,module,590,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1039,deployability,log,logg,1039,"t_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2117,deployability,Fail,Failed,2117,"_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2141,deployability,pipelin,pipeline,2141,"/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2945,deployability,releas,release,2945,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3524,deployability,log,logarithmization,3524,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2197,energy efficiency,CPU,CPUDispatcher,2197,"ial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2449,energy efficiency,CPU,CPUDispatcher,2449,"/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2731,energy efficiency,current,current,2731,"onda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? B",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2141,integrability,pipelin,pipeline,2141,"/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3341,integrability,messag,message,3341,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1022,interoperability,coordinat,coordinates,1022,"used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3341,interoperability,messag,message,3341,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:590,modifiability,modul,module,590,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:673,modifiability,pac,packages,673,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1171,modifiability,pac,packages,1171,"e below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1476,modifiability,pac,packages,1476,"-------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1759,modifiability,pac,packages,1759,"ha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1935,modifiability,pac,packages,1935,"x(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2253,modifiability,paramet,parameters,2253,"lpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2394,modifiability,paramet,parameterized,2394,"ate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2561,modifiability,pac,packages,2561,"e(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2635,modifiability,pac,packages,2635,"ortedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., wo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:17,performance,error,error,17,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:80,performance,error,error,80,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:166,performance,error,error,166,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:240,performance,error,error,240,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:330,performance,error,error,330,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1062,performance,time,time,1062," UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.wit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1626,performance,error,errors,1626,"_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2197,performance,CPU,CPUDispatcher,2197,"ial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2449,performance,CPU,CPUDispatcher,2449,"/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3144,performance,error,errors,3144,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3335,performance,error,error,3335,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2117,reliability,Fail,Failed,2117,"_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3253,reliability,doe,doesn-t-compile,3253,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3683,reliability,doe,does,3683,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:17,safety,error,error,17,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:80,safety,error,error,80,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:166,safety,error,error,166,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:240,safety,error,error,240,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:330,safety,error,error,330,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:563,safety,input,input-,563,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:590,safety,modul,module,590,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1039,safety,log,logg,1039,"t_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1619,safety,except,except,1619,"a, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1626,safety,error,errors,1626,"_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3144,safety,error,errors,3144,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3335,safety,error,error,3335,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3524,safety,log,logarithmization,3524,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1039,security,log,logg,1039,"t_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2318,security,sign,signatures,2318,"tate, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3524,security,log,logarithmization,3524,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:519,testability,Trace,Traceback,519,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1039,testability,log,logg,1039,"t_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3354,testability,trace,traceback,3354,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3524,testability,log,logarithmization,3524,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:17,usability,error,error,17,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:80,usability,error,error,80,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:166,usability,error,error,166,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:240,usability,error,error,240,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:330,usability,error,error,330,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:563,usability,input,input-,563,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:689,usability,tool,tools,689,"sc.tl.umap numba error when used with init_pos=""paga""; Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb. sc.tl.umap(adata, init_pos='paga'). computing UMAP. using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-35-924452b37e5b> in <module>. ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1626,usability,error,errors,1626,"_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1703,usability,user,user,1703,"n umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1718,usability,help,help,1718,"in_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy). 137 neigh_params.get('metric', 'euclidean'),. 138 neigh_params.get('metric_kwds', {}),. --> 139 verbose=max(0, verbosity-3)). 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates. 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose). 984 initial_alpha,. 985 negative_sample_rate,. --> 986 verbose=verbose,. 987 ). 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 348 e.patch_message(msg). 349 . --> 350 error_rewrite(e, 'typing'). 351 except errors.UnsupportedError as e:. 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2921,usability,support,supported,2921,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3144,usability,error,errors,3144,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3222,usability,user,user,3222,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3335,usability,error,error,3335,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3378,usability,minim,minimal,3378,"ython3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 315 raise e. 316 else:. --> 317 reraise(type(e), e, None). 318 . 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb). 656 value = tp(). 657 if value.__traceback__ is not tb:. --> 658 raise value.with_traceback(tb). 659 raise value. 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)). [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/dev/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization. 2. PCA, bbknn, louvain. 3. combat, HVG, PCA, UMAP (works well). 4. Paga (with louvain from 2., works well). 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed? Best,. Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/667:114,deployability,version,versions,114,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:509,deployability,continu,continuing,509,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:652,deployability,fail,fails,652,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:114,integrability,version,versions,114,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:65,modifiability,variab,variable,65,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:114,modifiability,version,versions,114,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:487,modifiability,variab,variable,487,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:652,reliability,fail,fails,652,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:1109,reliability,pra,practice,1109,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:346,usability,workflow,workflow,346,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:954,usability,workflow,workflow,954,"Issue with regress_out() and tagging rather than removing highly variable genes?; One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```. sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3). ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/668:0,availability,Error,Error,0,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:456,availability,error,error,456,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:6,integrability,messag,message,6,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:6,interoperability,messag,message,6,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:77,modifiability,pac,package,77,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:0,performance,Error,Error,0,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:376,performance,perform,perform,376,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:456,performance,error,error,456,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:0,safety,Error,Error,0,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:456,safety,error,error,456,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:0,usability,Error,Error,0,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:376,usability,perform,perform,376,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:456,usability,error,error,456,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:512,usability,user,user-images,512,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:682,usability,help,help,682,"Error message in tl.diffmap / why n_comps must be > 2 ; Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`. `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help! Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/669:506,deployability,api,api,506,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:938,deployability,log,logreg,938,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1020,deployability,log,logistic,1020,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1143,deployability,stack,stackoverflow,1143,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:59,integrability,batch,batch,59,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:175,integrability,batch,batch,175,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:296,integrability,batch,batch,296,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:339,integrability,batch,batch,339,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:378,integrability,transform,transform,378,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:506,integrability,api,api,506,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1315,integrability,wrap,wrapper,1315,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1434,integrability,batch,batch,1434,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:378,interoperability,transform,transform,378,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:506,interoperability,api,api,506,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1315,interoperability,wrapper,wrapper,1315,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:59,performance,batch,batch,59,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:175,performance,batch,batch,175,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:296,performance,batch,batch,296,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:339,performance,batch,batch,339,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:679,performance,content,content,679,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1434,performance,batch,batch,1434,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:938,safety,log,logreg,938,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1020,safety,log,logistic,1020,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:938,security,log,logreg,938,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1020,security,log,logistic,1020,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:413,testability,understand,understand,413,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:938,testability,log,logreg,938,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1020,testability,log,logistic,1020,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1029,testability,regress,regression,1029,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1632,testability,understand,understanding,1632,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:783,usability,interact,interaction,783,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:995,usability,interact,interactions,995,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1191,usability,interact,interaction-term-in-python-sklearn,1191,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:1353,usability,interact,interactions,1353,"Proper way to calculate differential gene expression after batch alignment?; Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets? After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:. `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`. I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper. Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect? Do you think this is the right way to do this and could you point me in the right direction to solve this? I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
