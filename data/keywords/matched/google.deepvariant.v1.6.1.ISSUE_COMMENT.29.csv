id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/741:555,deployability,version,version,555,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:618,deployability,stage,stage,618,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:481,integrability,version,version,481,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:555,integrability,version,version,555,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:5,interoperability,share,shared,5,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:481,modifiability,version,version,481,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:555,modifiability,version,version,555,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:678,modifiability,deco,decode,678,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:737,performance,error,error,737,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:14,reliability,sli,slice,14,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:263,reliability,sli,slicing,263,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:330,reliability,sli,slicing,330,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:523,reliability,sli,slice,523,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:670,reliability,slo,slow,670,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:737,safety,error,error,737,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:720,usability,command,command,720,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:737,usability,error,error,737,"I've shared a slice of the [cram](https://polybox.ethz.ch/index.php/s/gs77cgkairybovm) and [index](https://polybox.ethz.ch/index.php/s/4TddHufT6wsGMaB) with the [reference](https://polybox.ethz.ch/index.php/s/koYmv6FLL59ahI7). This is definitely strange, because slicing out part of the first chromosome reproduces the crash, but slicing out chromosome 29 (this is on cattle) appears to run fine, so this must be some bizarre edge case. Even more strangely, if I use the container version of samtools to extract this exact slice with `--output-fmt-option version=3.1 `, it makes it further to the candidate generation stage but then potentially freezes (or is just very slow to decode the cram). I've attached the exact command and full error I get to reproduce this on v1.6. [fulllog.txt](https://github.com/google/deepvariant/files/13497112/fulllog.txt).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:699,availability,down,download,699,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:166,deployability,version,version,166,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:227,deployability,Updat,Updating,227,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:249,deployability,version,version,249,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:313,deployability,updat,update,313,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:360,deployability,updat,updating,360,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:526,deployability,BUILD,BUILD,526,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:690,deployability,releas,releases,690,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1054,deployability,updat,updating,1054,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1113,deployability,releas,release,1113,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:965,energy efficiency,current,currently,965,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:166,integrability,version,version,166,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:249,integrability,version,version,249,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:166,modifiability,version,version,166,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:249,modifiability,version,version,249,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:346,performance,time,time,346,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:227,safety,Updat,Updating,227,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:313,safety,updat,update,313,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:360,safety,updat,updating,360,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1054,safety,updat,updating,1054,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:227,security,Updat,Updating,227,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:313,security,updat,update,313,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:360,security,updat,updating,360,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1054,security,updat,updating,1054,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:207,usability,support,supports,207,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:809,usability,workflow,workflows,809,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:991,usability,workflow,workflow,991,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32). ```. http_archive(. name = ""htslib"",. build_file = ""//:third_party/htslib.BUILD"",. sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",. strip_prefix = ""htslib-1.18"",. urls = [. ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",. ],. ). ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:56,deployability,build,build,56,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:120,deployability,version,version,120,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,deployability,version,version,184,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:214,deployability,contain,container,214,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:282,deployability,version,version,282,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:430,deployability,version,versions,430,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:96,energy efficiency,current,currently,96,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:120,integrability,version,version,120,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,integrability,version,version,184,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:282,integrability,version,version,282,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:430,integrability,version,versions,430,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:66,interoperability,share,shared,66,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:120,modifiability,version,version,120,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:184,modifiability,version,version,184,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:282,modifiability,version,version,282,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:430,modifiability,version,versions,430,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:192,security,access,accessible,192,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:398,usability,interact,interaction,398,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:525,usability,close,close,525,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```. Apptainer> samtools --version. samtools 1.10. Using htslib 1.18. Copyright (C) 2019 Genome Research Ltd. ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:104,deployability,releas,release,104,"Alex,. Thank you again for pointing out this issue with Cram v3.1. I'm happy to report that our next DV release will include htslib 1.18, which seems to handle these files without issue. I'll mark this issue as closed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:211,usability,close,closed,211,"Alex,. Thank you again for pointing out this issue with Cram v3.1. I'm happy to report that our next DV release will include htslib 1.18, which seems to handle these files without issue. I'll mark this issue as closed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:42,availability,error,error,42,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:284,availability,Failur,Failure,284,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:302,availability,sli,slice,302,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:352,availability,state,state,352,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:552,availability,state,state,552,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:779,availability,state,state,779,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:976,availability,state,state,976,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1170,availability,state,state,1170,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:284,deployability,Fail,Failure,284,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1394,deployability,Fail,Failed,1394,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:352,integrability,state,state,352,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:552,integrability,state,state,552,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:779,integrability,state,state,779,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:976,integrability,state,state,976,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1170,integrability,state,state,1170,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:101,interoperability,mismatch,mismatch,101,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:295,modifiability,deco,decode,295,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:42,performance,error,error,42,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:284,performance,Failur,Failure,284,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:284,reliability,Fail,Failure,284,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:302,reliability,sli,slice,302,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:1394,reliability,Fail,Failed,1394,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:42,safety,error,error,42,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:82,security,checksum,checksum,82,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:309,testability,Trace,Traceback,309,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/741:42,usability,error,error,42,"Was this issue fixed? I'm having the same error:. ```. [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777. [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800. [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854. [E::cram_next_slice] Failure to decode slice. Traceback (most recent call last):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. reads = reservoir_sample_reads(. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. return utils.reservoir_sample(iterable_of_reads, k, random). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. for i, item in enumerate(iterable):. File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. record, not_done = self._raw_next(). File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. not_done = self._cc_iterable.PythonNext(record). ValueError: DATA_LOSS: Failed to parse SAM record. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/741
https://github.com/google/deepvariant/issues/742:27,interoperability,share,share,27,"@aheravi ,. Can you please share the command you are using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:37,usability,command,command,37,"@aheravi ,. Can you please share the command you are using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:228,modifiability,PAC,PACBIO,228,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:308,safety,input,input,308,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:380,safety,input,input,380,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:452,safety,input,input,452,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:33,usability,command,command,33,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:308,usability,input,input,308,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:380,usability,input,input,380,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:452,usability,input,input,452,"Hi @kishwarshafin ,. Here is the command I used:. ```. BIN_VERSION=""1.6.0"". singularity run -B /deepvariant/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref ./reference/GRCh38_no_alt_analysis_set.fasta \. --reads_child ./input/HG002.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent1 ./input/HG003.pfda_challenge.grch38.phased.chr20.bam \. --reads_parent2 ./input/HG004.pfda_challenge.grch38.phased.chr20.bam \. --output_vcf_child ./output/HG002.output.vcf.gz \. --output_vcf_parent1 ./output/HG003.output.vcf.gz \. --output_vcf_parent2 ./output/HG004.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards $(nproc) \. --intermediate_results_dir ./output/intermediate_results_dir \. --output_gvcf_child ./output/HG002.g.vcf.gz \. --output_gvcf_parent1 ./output/HG003.g.vcf.gz \. --output_gvcf_parent2 ./output/HG004.g.vcf.gz \. --regions chr20. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/742:60,usability,command,command,60,"@aheravi ,. Just to see if it works, can you please run the command without using:. ```bash. --intermediate_results_dir ./output/intermediate_results_dir. ```. And see if it works? Meanwhile, I will try to reproduce it locally.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/742
https://github.com/google/deepvariant/issues/743:16,availability,error,error,16,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:67,availability,error,error,67,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:100,energy efficiency,current,currently,100,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:406,energy efficiency,model,model,406,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:415,energy efficiency,model,model,415,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:526,energy efficiency,model,model,526,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:532,energy efficiency,model,model,532,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:16,performance,error,error,16,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:67,performance,error,error,67,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:16,safety,error,error,16,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:67,safety,error,error,67,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:229,safety,input,input,229,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:249,safety,input,input,249,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:599,safety,input,input,599,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:406,security,model,model,406,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:415,security,model,model,415,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:526,security,model,model,526,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:532,security,model,model,532,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:16,usability,error,error,16,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:67,usability,error,error,67,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:88,usability,command,command,88,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:229,usability,input,input,229,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:249,usability,input,input,249,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:599,usability,input,input,599,"I am getting an error with channel being different. Find below the error coming and the command I'm currently using-. docker run -u $(id -u):$(id -g) \. -v ""${PWD}/quickstart-output"":""/quickstart-output"" \. -v ""${PWD}/quickstart-input"":""/quickstart-input"" \. -v ""/BXRX_ANALYSIS/HRD_MeetaS/projects/vijay/UMI_pipeline/cicero_ref/reference_hg38_noalt/Homo_sapiens/GRCh38_no_alt/FASTA"":""/FASTA"" \. -v ""${PWD}/model"":""/model"" \. -w $(pwd) \. google/deepvariant:latest \. run_deepvariant \. --model_type=WES \. --customized_model=/model/model.ckpt \. --ref=/FASTA/GRCh38_no_alt.fa \. --reads=/quickstart-input/Aligned.sortedByCoord.out.bam \. --output_vcf=/quickstart-output/output.vcf.gz \. --num_shards=30 \. --intermediate_results_dir /quickstart-output/intermediate_results_dir. ![image (2)](https://github.com/google/deepvariant/assets/45894456/c1961bb4-6e7d-477c-bab6-b267ba31eb33).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:68,energy efficiency,model,model,68,"Hi @YogitaVerma123 ,. Can you tell me where you got your customized model? It seems like your model only has 6 channels. Our default WES channels have 7 channels. The 7th channel is insert_size.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:94,energy efficiency,model,model,94,"Hi @YogitaVerma123 ,. Can you tell me where you got your customized model? It seems like your model only has 6 channels. Our default WES channels have 7 channels. The 7th channel is insert_size.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:68,security,model,model,68,"Hi @YogitaVerma123 ,. Can you tell me where you got your customized model? It seems like your model only has 6 channels. Our default WES channels have 7 channels. The 7th channel is insert_size.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:94,security,model,model,94,"Hi @YogitaVerma123 ,. Can you tell me where you got your customized model? It seems like your model only has 6 channels. Our default WES channels have 7 channels. The 7th channel is insert_size.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:57,usability,custom,customized,57,"Hi @YogitaVerma123 ,. Can you tell me where you got your customized model? It seems like your model only has 6 channels. Our default WES channels have 7 channels. The 7th channel is insert_size.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:111,availability,down,downloaded,111,"Hi @pichuan ,. Im running this for RNA seq study . In the documentation itself the model is there, I have just downloaded those files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:83,energy efficiency,model,model,83,"Hi @pichuan ,. Im running this for RNA seq study . In the documentation itself the model is there, I have just downloaded those files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:83,security,model,model,83,"Hi @pichuan ,. Im running this for RNA seq study . In the documentation itself the model is there, I have just downloaded those files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:58,usability,document,documentation,58,"Hi @pichuan ,. Im running this for RNA seq study . In the documentation itself the model is there, I have just downloaded those files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:372,deployability,version,version,372,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:177,energy efficiency,cpu,cpu-only-machine,177,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:372,integrability,version,version,372,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:372,modifiability,version,version,372,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:177,performance,cpu,cpu-only-machine,177,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:341,usability,document,documentation,341,"If you're running RNA seq study, when you run this part:. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine. Can you make sure you added this line:. ```. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. ```. And, if you're running RNAseq documentation, you have to use version `1.4.0`. `latest` won't work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:110,deployability,version,version,110,"Hi @YogitaVerma123 , please feel free to reach out if you have more questions. . Hopefully using the matching version (1.4.0) and and adding the flag will resolve your issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:110,integrability,version,version,110,"Hi @YogitaVerma123 , please feel free to reach out if you have more questions. . Hopefully using the matching version (1.4.0) and and adding the flag will resolve your issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/743:110,modifiability,version,version,110,"Hi @YogitaVerma123 , please feel free to reach out if you have more questions. . Hopefully using the matching version (1.4.0) and and adding the flag will resolve your issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/743
https://github.com/google/deepvariant/issues/744:1115,availability,checkpoint,checkpoint,1115,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1145,availability,checkpoint,checkpoint,1145,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:143,deployability,pipelin,pipeline,143,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1139,energy efficiency,model,model,1139,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1165,energy efficiency,model,model,1165,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:143,integrability,pipelin,pipeline,143,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:404,interoperability,specif,specifying,404,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:525,interoperability,specif,specifies,525,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:327,performance,parallel,parallelized,327,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1115,reliability,checkpoint,checkpoint,1115,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1145,reliability,checkpoint,checkpoint,1145,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:979,safety,Input,Input,979,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1315,safety,Input,Input,1315,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1139,security,model,model,1139,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1165,security,model,model,1165,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:397,testability,simpl,simply,397,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:397,usability,simpl,simply,397,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:979,usability,Input,Input,979,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1315,usability,Input,Input,1315,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. . * call_variants will be run with the same number of shards. * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```. bin/make_examples \. --examples /tmn/your_examples.tfrecord@200.gz \. --mode calling \. --reads /tmp/your_input_bam.bam \. --realign_reads \. --ref=/tmp/your_reference.fna \. --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:. bin/call_variants.par \. --batch_size=32 \. --checkpoint <Path to the model checkpoint or saved model>.ckpt \. --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:. /tmp/your_call_variants_output.cvo.tfrecord@200.gz. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:385,reliability,doe,does,385,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:419,safety,input,inputs,419,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:176,usability,workflow,workflow,176,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:203,usability,user,users,203,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:216,usability,workflow,workflow,216,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:419,usability,input,inputs,419,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:642,usability,clear,clear,642,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:654,usability,close,close,654,"Hi @themkdemiiir ,. @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow. (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:97,reliability,doe,does,97,"Would there be any difference if I gave it all chromosomes or only one in vcf outputs? Also, how does it shard the BAM file according to chromosomes or regions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:205,performance,parallel,parallelize,205,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:741,performance,parallel,parallelizing,741,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:21,safety,input,input,21,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:118,safety,input,input,118,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:280,safety,input,input,280,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:598,safety,input,input,598,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:685,safety,input,input,685,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:789,security,control,controlled,789,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:789,testability,control,controlled,789,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:21,usability,input,input,21,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:118,usability,input,input,118,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:280,usability,input,input,280,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:598,usability,input,input,598,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:628,usability,tool,tools,628,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:685,usability,input,input,685,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1230,usability,help,helpful,1230,"@themkdemiiir if the input BAM has only one chromosome, then the output VCF will only be able to call variants in the input BAM (therefore will only have calls from one chromosome). But that allows you to parallelize. For example, you can run 22 separate DeepVariant runs from 22 input BAMs, each of them are chr1-chr22. Then at the end, you'll have 22 VCFs, each of them have calls from chr1-chr22, and you can combine the output VCFs at the end if you want to have one VCF. What I describe above will be the way you split by chromosome when using DeepVariant. (Basically you'll need to split the input BAMs by yourself, using tools like `samtools`). Once you provide DeepVariant one input BAM, DeepVariant has some internal way of further parallelizing/sharding its computation. This is controlled by the `--num_shards` flag of `run_deepvariant`. We recommend not using more than the number of processors when using `--num_shards`. The way that @akolesnikov described about the `--task` flag is a more advanced way. I actually don't necessarily recommend you trying to use the `--task` flag directly because it can be confusing. . The `--num_shards` flag of `run_deepvariant` is something that you can change. Hopefully this is helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1154,availability,checkpoint,checkpoint,1154,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1184,availability,checkpoint,checkpoint,1184,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:151,deployability,pipelin,pipeline,151,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1178,energy efficiency,model,model,1178,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1204,energy efficiency,model,model,1204,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:151,integrability,pipelin,pipeline,151,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:418,interoperability,specif,specifying,418,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:524,interoperability,specif,specifies,524,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:341,performance,parallel,parallelized,341,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1154,reliability,checkpoint,checkpoint,1154,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1184,reliability,checkpoint,checkpoint,1184,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1012,safety,Input,Input,1012,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1364,safety,Input,Input,1364,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1178,security,model,model,1178,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1204,security,model,model,1204,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:411,testability,simpl,simply,411,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:411,usability,simpl,simply,411,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1012,usability,Input,Input,1012,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/744:1364,usability,Input,Input,1364,"> Hi @themkdemiiir,. > . > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. > . > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard. > * call_variants will be run with the same number of shards. > * postprocess_variants has to be run in a single process. > . > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. > . > ```. > bin/make_examples \. > --examples /tmn/your_examples.tfrecord@200.gz \. > --mode calling \. > --reads /tmp/your_input_bam.bam \. > --realign_reads \. > --ref=/tmp/your_reference.fna \. > --task=11. > . > # Input for each instance of call_variants is the output of one instance of make_examples:. > bin/call_variants.par \. > --batch_size=32 \. > --checkpoint <Path to the model checkpoint or saved model>.ckpt \. > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \. > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. > . > # Input for for postprocess would be the output of all instances of call_variants:. > /tmp/your_call_variants_output.cvo.tfrecord@200.gz. > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/744
https://github.com/google/deepvariant/issues/745:104,energy efficiency,gpu,gpu,104,"Hi @alanlamsiu ,. Can you provide the full context? Like, where did your `deepvariant_deeptrio-1.6.0rc2-gpu.sif` came from? You can also check out https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity to see if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:104,performance,gpu,gpu,104,"Hi @alanlamsiu ,. Can you provide the full context? Like, where did your `deepvariant_deeptrio-1.6.0rc2-gpu.sif` came from? You can also check out https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity to see if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:43,testability,context,context,43,"Hi @alanlamsiu ,. Can you provide the full context? Like, where did your `deepvariant_deeptrio-1.6.0rc2-gpu.sif` came from? You can also check out https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity to see if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:307,deployability,contain,container,307,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:86,energy efficiency,gpu,gpu,86,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:133,energy efficiency,gpu,gpu,133,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:277,energy efficiency,gpu,gpu,277,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:86,performance,gpu,gpu,86,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:133,performance,gpu,gpu,133,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:277,performance,gpu,gpu,277,"Hi @pichuan,. I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. Then I did `singularity shell --nv -B /usr/lib/locale/:/usr/lib/locale/ /sc/arion/work/linx19/utilities/deepvariant_deeptrio-1.6.0rc2-gpu.sif` and shelled into the container. When I checked `ls /opt/deepvariant/bin/`, I saw the following files. call_variants labeled_examples_to_vcf make_examples_somatic postprocess_variants run_deepsomatic.py runtime_by_region_vis.zip train. call_variants.zip labeled_examples_to_vcf.zip make_examples_somatic.zip postprocess_variants.zip run_deepvariant settings.sh train.zip. call_variants_slim make_examples multisample_make_examples run-prereq.sh run_deepvariant.py show_examples vcf_stats_report. call_variants_slim.zip make_examples.zip multisample_make_examples.zip run_deepsomatic runtime_by_region_vis show_examples.zip vcf_stats_report.zip. Do you think the run_deeptrio file is missing here? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:265,energy efficiency,gpu,gpu,265,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:312,energy efficiency,gpu,gpu,312,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:368,interoperability,share,share,368,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:265,performance,gpu,gpu,265,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:312,performance,gpu,gpu,312,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:338,testability,understand,understand,338,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:384,usability,command,commands,384,"Hi @alanlamsiu . Can you try the steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and see if that works for you? > Hi @pichuan,. > . > I pulled the image using `docker pull google/deepvariant:deeptrio-1.6.0-gpu`, which got `deepvariant_deeptrio-1.6.0rc2-gpu.sif`. I don't think I understand this part. Can you share the exact commands you ran here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:512,availability,error,error,512,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:218,deployability,contain,container,218,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:288,deployability,contain,container,288,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:518,integrability,messag,message,518,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:518,interoperability,messag,message,518,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:512,performance,error,error,512,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:512,safety,error,error,512,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:69,usability,clear,clear,69,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:512,usability,error,error,512,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:430,availability,mainten,maintenance-policy,430,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2887,availability,avail,available,2887,"ng from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3194,availability,operat,operations,3194,"le, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see hel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3248,availability,operat,operations,3248,"```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:97,deployability,version,version,97,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:829,deployability,instal,installed,829,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1116,deployability,instal,install,1116,"alk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, ple",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1910,deployability,version,version,1910,"t/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2484,deployability,Version,Version,2484," docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2500,deployability,Contain,Container,2500,"deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2600,deployability,contain,container,2600,"u.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2674,deployability,Contain,Container,2674," ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2718,deployability,contain,container,2718," -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_util",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2837,deployability,contain,container-license,2837,"d like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2905,deployability,contain,container,2905,"version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2926,deployability,CONTAIN,CONTAINER-LICENSE,2926,"u should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3897,deployability,instal,installed,3897,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4327,deployability,continu,continue,4327,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4512,deployability,version,version,4512,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4780,deployability,updat,update,4780,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:290,energy efficiency,GPU,GPU,290,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:367,energy efficiency,gpu,gpu,367,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:409,energy efficiency,cloud,cloud-platform,409,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:575,energy efficiency,cloud,cloud,575,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:675,energy efficiency,cpu,cpu-platform,675,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:759,energy efficiency,gpu,gpu,759,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1542,energy efficiency,gpu,gpu,1542," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1603,energy efficiency,gpu,gpu,1603,"tandard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1673,energy efficiency,gpu,gpu,1673,"cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1714,energy efficiency,gpu,gpu,1714,"to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1796,energy efficiency,gpu,gpu,1796,"ine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://develop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2350,energy efficiency,gpu,gpu,2350,"com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3007,energy efficiency,core,core,3007,"ommand you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3073,energy efficiency,optim,optimized,3073,"evious, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3153,energy efficiency,CPU,CPU,3153,".0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is require",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3431,energy efficiency,load,load,3431,"me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3819,energy efficiency,GPU,GPU,3819,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:97,integrability,version,version,97,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1910,integrability,version,version,1910,"t/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2484,integrability,Version,Version,2484," docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4091,integrability,messag,message,4091,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4512,integrability,version,version,4512,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:415,interoperability,platform,platform,415,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:604,interoperability,standard,standard-,604,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:679,interoperability,platform,platform,679,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3012,interoperability,platform,platform,3012,"you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expect",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3386,interoperability,platform,platform,3386,"/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3516,interoperability,share,shared,3516,"yright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4091,interoperability,messag,message,4091,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:97,modifiability,version,version,97,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1910,modifiability,version,version,1910,"t/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2484,modifiability,Version,Version,2484," docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4512,modifiability,version,version,4512,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:290,performance,GPU,GPU,290,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:367,performance,gpu,gpu,367,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:627,performance,disk,disk-size,627,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:675,performance,cpu,cpu-platform,675,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:759,performance,gpu,gpu,759,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1542,performance,gpu,gpu,1542," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1603,performance,gpu,gpu,1603,"tandard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1673,performance,gpu,gpu,1673,"cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1714,performance,gpu,gpu,1714,"to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1796,performance,gpu,gpu,1796,"ine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://develop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2350,performance,gpu,gpu,2350,"com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2624,performance,content,contents,2624," checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3073,performance,optimiz,optimized,3073,"evious, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3107,performance,Network,Network,3107,"f so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3153,performance,CPU,CPU,3153,".0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is require",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3173,performance,perform,performance-critical,3173,"used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --hel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3431,performance,load,load,3431,"me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3819,performance,GPU,GPU,3819,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:430,reliability,mainten,maintenance-policy,430,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:802,reliability,doe,doesn,802,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2887,reliability,availab,available,2887,"ng from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:142,safety,test,tested,142,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:305,safety,test,test,305,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1096,safety,test,test,1096,"ersion? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2887,safety,avail,available,2887,"ng from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4340,safety,test,test,4340,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4780,safety,updat,update,4780,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:442,security,polic,policy,442,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:712,security,ssh,ssh,712,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:747,security,ssh,ssh,747,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2637,security,govern,governed,2637,"size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2887,security,availab,available,2887,"ng from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3107,security,Network,Network,3107,"f so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4780,security,updat,update,4780,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:23,testability,understand,understand,23,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:142,testability,test,tested,142,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:305,testability,test,test,305,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:1096,testability,test,test,1096,"ersion? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4340,testability,test,test,4340,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:211,usability,experien,experience,211,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:361,usability,USER,USER,361,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version? I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash. gcloud compute instances create ""${USER}-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh. sudo bash -x install_nvidia_docker.sh . ```. (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2008,usability,command,command,2008,"part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash. wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh. sudo bash -x install_singularity.sh . ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2414,usability,command,command,2414,"s-on-singularity. I ran:. ```bash. BIN_VERSION=1.6.0. singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"". ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash. pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2665,usability,Learn,Learning,2665,"uan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif . -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif. ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:2828,usability,learn,learning-container-license,2828,"ng I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, ple",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3173,usability,perform,performance-critical,3173,"used the .sif file, I'll also do something similar:. So, then I ran:. ```bash. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant_deeptrio-1.6.0-gpu.sif \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. The command above gave me:. ```. ==========. == CUDA ==. ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license:. https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --hel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3950,usability,help,helpshort,3950,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3965,usability,help,helpfull,3965,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:3981,usability,help,help,3981,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4165,usability,help,helpshort,4165,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4180,usability,help,helpfull,4180,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4196,usability,help,help,4196,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4439,usability,confirm,confirm,4439,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:4716,usability,document,documentation,4716,"s://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs. 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```. --model_type is required. Pass --helpshort or --helpfull to see help on flags. ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,. Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one). 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:84,deployability,version,version,84,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:313,deployability,contain,container,313,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:482,deployability,version,version,482,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:478,energy efficiency,gpu,gpu,478,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:510,energy efficiency,gpu,gpu,510,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:84,integrability,version,version,84,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:482,integrability,version,version,482,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:84,modifiability,version,version,84,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:482,modifiability,version,version,482,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:478,performance,gpu,gpu,478,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/745:510,performance,gpu,gpu,510,"Hi @pichuan ,. Thanks for checking it out. I later found out that I did use the rc2 version before. Now I have the one without rc2 as well. ![image](https://github.com/google/deepvariant/assets/34832128/ce620abc-ccbe-4ef6-ad04-1a76e23e896a). Now I see that `/opt/deepvariant/bin/deeptrio/run_deeptrio` is in this container. ![image](https://github.com/google/deepvariant/assets/34832128/9c479716-af62-4915-ab71-4a13cad772fa). Thanks for pointing it out and I will use the 1.6.0-gpu version instead of 1.6.0rc2-gpu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/745
https://github.com/google/deepvariant/issues/746:245,interoperability,incompatib,incompatible,245,"Hi @Kwondo87 ,. I think this might be similar to https://github.com/google/deepvariant/issues/640. If I remember correctly, the issue was that Singularity uses the Numpy from your computer (instead of the docker image), and in your setting it's incompatible. Please look at the discussion in https://github.com/google/deepvariant/issues/640 and see if it's helpful for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:104,safety,reme,remember,104,"Hi @Kwondo87 ,. I think this might be similar to https://github.com/google/deepvariant/issues/640. If I remember correctly, the issue was that Singularity uses the Numpy from your computer (instead of the docker image), and in your setting it's incompatible. Please look at the discussion in https://github.com/google/deepvariant/issues/640 and see if it's helpful for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:357,usability,help,helpful,357,"Hi @Kwondo87 ,. I think this might be similar to https://github.com/google/deepvariant/issues/640. If I remember correctly, the issue was that Singularity uses the Numpy from your computer (instead of the docker image), and in your setting it's incompatible. Please look at the discussion in https://github.com/google/deepvariant/issues/640 and see if it's helpful for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:149,deployability,version,version,149,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:201,deployability,version,version,201,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:353,deployability,version,version,353,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:149,integrability,version,version,149,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:201,integrability,version,version,201,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:353,integrability,version,version,353,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:149,modifiability,version,version,149,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:201,modifiability,version,version,201,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:353,modifiability,version,version,353,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:172,reliability,doe,doesn,172,"Thank you for the reply, @pichuan . I took a look at the discussion. It looks like the issue was not resolved for deepvariant 1.6.0. Also, the numpy version on my computer doesn't matter, as the numpy version inside the singularity image is being used as you've shown in discussion #640. 1.4.0 image worked on setting but I would like to use the latest version of singularity image if possible. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:520,availability,error,error,520,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:669,deployability,version,versions,669,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:669,integrability,version,versions,669,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:669,modifiability,version,versions,669,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:520,performance,error,error,520,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:520,safety,error,error,520,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:520,usability,error,error,520,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:752,usability,help,help,752,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:919,usability,help,helpful,919,"@Kwondo87 I wasn't able to reproduce the Singularity issue on my side, so I'll need more information from you. I also noticed that your original post said you used https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md . Can you use https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md instead in case there's differences? (Although, I just quickly checked and they don't look different on the Singularity section). @Kwondo87 given that I'm not able to reproduce the error, can you give me your step-by-step, including how you get the image, how you run singularity, etc. What your OS is, etc. Maybe check the numpy versions in anywhere that might be relevant and just print them? . I'll be able to help more when I can reproduce on my side. (Something similar to what I did in https://github.com/google/deepvariant/issues/745#issuecomment-1840177389 will be really helpful. Although, in that case I also wasn't able to reproduce the issue).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:154,deployability,version,version,154,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:175,deployability,version,version,175,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:366,deployability,version,version,366,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:27,energy efficiency,CPU,CPU,27,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:133,energy efficiency,cpu,cpu,133,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:206,energy efficiency,cpu,cpu,206,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:237,energy efficiency,cpu,cpu,237,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:404,energy efficiency,cpu,cpu,404,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:506,energy efficiency,cpu,cpu,506,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:717,energy efficiency,cpu,cpu,717,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:154,integrability,version,version,154,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:175,integrability,version,version,175,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:366,integrability,version,version,366,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:743,interoperability,bind,bind,743,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:154,modifiability,version,version,154,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:175,modifiability,version,version,175,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:366,modifiability,version,version,366,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:743,modifiability,bind,bind,743,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:27,performance,CPU,CPU,27,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:133,performance,cpu,cpu,133,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:206,performance,cpu,cpu,206,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:237,performance,cpu,cpu,237,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:404,performance,cpu,cpu,404,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:506,performance,cpu,cpu,506,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:717,performance,cpu,cpu,717,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:765,safety,input,input,765,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:12,usability,confirm,confirmed,12,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:765,usability,input,input,765,"Btw, I have confirmed on a CPU machine , I was able to run the Quick Start with Singularity without any issues. ```. pichuan@pichuan-cpu:~$ singularity --version. singularity version 3.7.0. pichuan@pichuan-cpu:~$ uname -a. Linux pichuan-cpu 5.15.0-1047-gcp #55~20.04.1-Ubuntu SMP Wed Nov 15 11:38:25 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux. ```. And a few different version checks:. ```. pichuan@pichuan-cpu:~$ singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.24.4. pichuan@pichuan-cpu:~$ singularity shell deepvariant_1.6.0.sif. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.24.4. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.24.4. pichuan@pichuan-cpu:~$ singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.24.4. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:187,availability,error,error,187,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:276,availability,down,download,276,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:321,availability,down,downloading,321,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:357,availability,error,error,357,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2136,deployability,version,versions,2136,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2165,deployability,version,version,2165,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2186,deployability,version,version,2186,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2136,integrability,version,versions,2136,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2165,integrability,version,version,2165,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2186,integrability,version,version,2186,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2663,interoperability,bind,bind,2663,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2136,modifiability,version,versions,2136,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2165,modifiability,version,version,2165,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2186,modifiability,version,version,2186,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2663,modifiability,bind,bind,2663,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:187,performance,error,error,187,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:357,performance,error,error,357,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:187,safety,error,error,187,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:357,safety,error,error,357,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:530,safety,test,testdata,530,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:610,safety,test,testdata,610,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2686,safety,test,testdata,2686,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:530,testability,test,testdata,530,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:610,testability,test,testdata,610,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1098,testability,unit,unittest,1098,"/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1170,testability,unit,unittest,1170,"ves the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1246,testability,unit,unittest,1246,".0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1321,testability,unit,unittest,1321,"loading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1400,testability,unit,unittest,1400,"bove to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:1789,testability,unit,unittest,1789,"get -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:2686,testability,test,testdata,2686,"bp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. And I did same things as you to check the versions. ```. singularity --version. singularity version 3.8.5-2.el7. uname -a. Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy. numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0. Singularity> python -c ""import numpy; print(numpy.__version__)"". 1.22.1. Singularity> python3 -c ""import numpy; print(numpy.__version__)"". 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)"". 1.22.1. ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:187,usability,error,error,187,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:357,usability,error,error,357,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error. 2. Follow the instructions in the link above to run the program. Here is the script that I used . ```. #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ""${OUTPUT_DIR}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:28,deployability,updat,updating,28,"Hi @Kwondo87 ,. Can you try updating your numpy to 1.24.4 and see if it changes anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:28,safety,updat,updating,28,"Hi @Kwondo87 ,. Can you try updating your numpy to 1.24.4 and see if it changes anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:28,security,updat,updating,28,"Hi @Kwondo87 ,. Can you try updating your numpy to 1.24.4 and see if it changes anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:17,deployability,upgrad,upgraded,17,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:99,deployability,upgrad,upgraded,99,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:132,deployability,contain,containers,132,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:154,deployability,instal,install,154,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:170,deployability,upgrad,upgrade,170,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:207,deployability,contain,container,207,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:227,deployability,contain,container,227,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:273,deployability,contain,container,273,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:17,modifiability,upgrad,upgraded,17,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:99,modifiability,upgrad,upgraded,99,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:170,modifiability,upgrad,upgrade,170,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:218,reliability,Doe,Does,218,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!! But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container? Thanks a million anyway! ```. Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy. numpy==1.24.4. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:167,usability,help,help,167,"Glad to hear that it worked. I might have mentioned this before (maybe in the previous thread) - I'm not super familiar with Singularity myself, so I won't be able to help more on that. Given that your use case works, I'll close this issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/746:223,usability,close,close,223,"Glad to hear that it worked. I might have mentioned this before (maybe in the previous thread) - I'm not super familiar with Singularity myself, so I won't be able to help more on that. Given that your use case works, I'll close this issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/746
https://github.com/google/deepvariant/issues/747:387,deployability,build,build,387,"Hi @pioneer-pi ,. 1. . After you built from source, you can run the binaries with commands like:. ```. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. (like this line here: https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh#L64 ). 2. Yes you need to recompile. . Usually, in the same client, I'd do something like:. ```bash. source settings. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. (similar to https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh#L104-L110).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:82,usability,command,commands,82,"Hi @pioneer-pi ,. 1. . After you built from source, you can run the binaries with commands like:. ```. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. (like this line here: https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh#L64 ). 2. Yes you need to recompile. . Usually, in the same client, I'd do something like:. ```bash. source settings. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. (similar to https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh#L104-L110).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:164,usability,help,help,164,"Hi @pioneer-pi ,. 1. . After you built from source, you can run the binaries with commands like:. ```. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. (like this line here: https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh#L64 ). 2. Yes you need to recompile. . Usually, in the same client, I'd do something like:. ```bash. source settings. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. (similar to https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh#L104-L110).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:65,deployability,build,build,65,"You should be able to use either of them in this setup after you build. The zip files were built here https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh. When you built it on the machine, I think both versions should run fine (and the behaviors are the same)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:224,deployability,version,versions,224,"You should be able to use either of them in this setup after you build. The zip files were built here https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh. When you built it on the machine, I think both versions should run fine (and the behaviors are the same)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:224,integrability,version,versions,224,"You should be able to use either of them in this setup after you build. The zip files were built here https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh. When you built it on the machine, I think both versions should run fine (and the behaviors are the same)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:224,modifiability,version,versions,224,"You should be able to use either of them in this setup after you build. The zip files were built here https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh. When you built it on the machine, I think both versions should run fine (and the behaviors are the same)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:258,usability,behavi,behaviors,258,"You should be able to use either of them in this setup after you build. The zip files were built here https://github.com/google/deepvariant/blob/r1.6/build_release_binaries.sh. When you built it on the machine, I think both versions should run fine (and the behaviors are the same)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/747:114,usability,close,close,114,"Hi @pioneer-pi , https://github.com/google/deepvariant/issues/756#issuecomment-1865388872 can also relevant. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/747
https://github.com/google/deepvariant/issues/748:142,energy efficiency,cpu,cpus,142,"Hi @ASLeonard ,. @kishwarshafin will take a closer look later. I have one question : if you try running postprocess_variants with an extra `--cpus 0` flag, what happens?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:142,performance,cpu,cpus,142,"Hi @ASLeonard ,. @kishwarshafin will take a closer look later. I have one question : if you try running postprocess_variants with an extra `--cpus 0` flag, what happens?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:44,usability,close,closer,44,"Hi @ASLeonard ,. @kishwarshafin will take a closer look later. I have one question : if you try running postprocess_variants with an extra `--cpus 0` flag, what happens?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:107,availability,slo,slower,107,"Same issue, with `--cpus 0` (or 1) I still get endless printing of reading the bed. It appears to run much slower as well, so not sure if it is really having to re-read the bed file for every variant candidate being genotyped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:20,energy efficiency,cpu,cpus,20,"Same issue, with `--cpus 0` (or 1) I still get endless printing of reading the bed. It appears to run much slower as well, so not sure if it is really having to re-read the bed file for every variant candidate being genotyped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:20,performance,cpu,cpus,20,"Same issue, with `--cpus 0` (or 1) I still get endless printing of reading the bed. It appears to run much slower as well, so not sure if it is really having to re-read the bed file for every variant candidate being genotyped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:107,reliability,slo,slower,107,"Same issue, with `--cpus 0` (or 1) I still get endless printing of reading the bed. It appears to run much slower as well, so not sure if it is really having to re-read the bed file for every variant candidate being genotyped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:250,energy efficiency,CPU,CPU,250,"For completeness, I let one of the jobs for longer until finished, and the calls inside the PAR can be heterozygous and outside are always homozygous ref/alt, so the regions are being respected correctly. On this one sample however, it took around 1 CPU to postprocess without the haploid contigs/PAR and 3 hours to postprocess with, so a substantial increase.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:339,integrability,sub,substantial,339,"For completeness, I let one of the jobs for longer until finished, and the calls inside the PAR can be heterozygous and outside are always homozygous ref/alt, so the regions are being respected correctly. On this one sample however, it took around 1 CPU to postprocess without the haploid contigs/PAR and 3 hours to postprocess with, so a substantial increase.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:250,performance,CPU,CPU,250,"For completeness, I let one of the jobs for longer until finished, and the calls inside the PAR can be heterozygous and outside are always homozygous ref/alt, so the regions are being respected correctly. On this one sample however, it took around 1 CPU to postprocess without the haploid contigs/PAR and 3 hours to postprocess with, so a substantial increase.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:4,safety,compl,completeness,4,"For completeness, I let one of the jobs for longer until finished, and the calls inside the PAR can be heterozygous and outside are always homozygous ref/alt, so the regions are being respected correctly. On this one sample however, it took around 1 CPU to postprocess without the haploid contigs/PAR and 3 hours to postprocess with, so a substantial increase.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:4,security,compl,completeness,4,"For completeness, I let one of the jobs for longer until finished, and the calls inside the PAR can be heterozygous and outside are always homozygous ref/alt, so the regions are being respected correctly. On this one sample however, it took around 1 CPU to postprocess without the haploid contigs/PAR and 3 hours to postprocess with, so a substantial increase.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:89,deployability,log,log,89,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:116,deployability,patch,patch,116,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:50,energy efficiency,Current,Currently,50,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:78,safety,avoid,avoid,78,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:89,safety,log,log,89,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:116,safety,patch,patch,116,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:89,security,log,log,89,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:116,security,patch,patch,116,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:89,testability,log,log,89,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:31,deployability,updat,updated,31,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:222,deployability,updat,updating,222,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:138,energy efficiency,gpu,gpu,138,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:235,energy efficiency,current,current,235,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:138,performance,gpu,gpu,138,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:31,safety,updat,updated,31,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:222,safety,updat,updating,222,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:31,security,updat,updated,31,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:196,security,assess,assess,196,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:222,security,updat,updating,222,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/748:335,usability,help,helpful,335,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash. google/deepvariant:CL590726281. google/deepvariant:CL590726281-gpu. ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/748
https://github.com/google/deepvariant/issues/749:1343,deployability,pipelin,pipeline,1343,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1739,deployability,updat,update,1739,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1502,energy efficiency,current,currently,1502,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:437,integrability,wrap,wrapper,437,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1343,integrability,pipelin,pipeline,1343,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:437,interoperability,wrapper,wrapper,437,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:488,interoperability,specif,specify,488,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:938,modifiability,interm,intermediate,938,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1279,performance,workload,workload,1279,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1645,safety,review,reviewing,1645,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1739,safety,updat,update,1739,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:331,security,access,access,331,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:580,security,access,access,580,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:680,security,access,accessible,680,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:895,security,control,control,895,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1393,security,control,control,1393,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1448,security,control,control,1448,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1531,security,team,team,1531,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1739,security,updat,update,1739,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:895,testability,control,control,895,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1049,testability,understand,understand,1049,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1393,testability,control,control,1393,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1448,testability,control,control,1448,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1460,testability,understand,understand,1460,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1645,testability,review,reviewing,1645,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:788,usability,command,commands,788,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1109,usability,resum,resuming,1109,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1214,usability,resum,resuming,1214,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1567,usability,help,helping,1567,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:1767,usability,progress,progress,1767,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team. If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests! Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress. If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:224,deployability,depend,depending,224,"Thanks for response @pichuan,. I think i will definitely strive to implement this, but wont be before February 2024. I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:224,integrability,depend,depending,224,"Thanks for response @pichuan,. I think i will definitely strive to implement this, but wont be before February 2024. I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:224,modifiability,depend,depending,224,"Thanks for response @pichuan,. I think i will definitely strive to implement this, but wont be before February 2024. I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:224,safety,depend,depending,224,"Thanks for response @pichuan,. I think i will definitely strive to implement this, but wont be before February 2024. I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:224,testability,depend,depending,224,"Thanks for response @pichuan,. I think i will definitely strive to implement this, but wont be before February 2024. I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:636,availability,recov,recovery,636,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:225,deployability,resourc,resource,225,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:636,deployability,recov,recovery,636,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:200,energy efficiency,predict,predictably,200,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:225,energy efficiency,resourc,resource,225,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:225,performance,resourc,resource,225,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:636,reliability,recov,recovery,636,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:200,safety,predict,predictably,200,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:225,safety,resourc,resource,225,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:636,safety,recov,recovery,636,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:636,security,recov,recovery,636,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:225,testability,resourc,resource,225,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:87,usability,workflow,workflows,87,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/749:611,usability,resum,resuming,611,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks! Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/749
https://github.com/google/deepvariant/issues/750:137,availability,down,downsample,137,"Hi @gushenweiz,. A `RefCall` can happen for several reasons. Seems like you have a very high depth in this data. Would it be possible to downsample it to something between 90-100x and try again? It seems like the model does not think it's a variant based on the evidence. If you want to manually change the RefCall to pass then you have to do it on the VCF as a post-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:213,energy efficiency,model,model,213,"Hi @gushenweiz,. A `RefCall` can happen for several reasons. Seems like you have a very high depth in this data. Would it be possible to downsample it to something between 90-100x and try again? It seems like the model does not think it's a variant based on the evidence. If you want to manually change the RefCall to pass then you have to do it on the VCF as a post-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:219,reliability,doe,does,219,"Hi @gushenweiz,. A `RefCall` can happen for several reasons. Seems like you have a very high depth in this data. Would it be possible to downsample it to something between 90-100x and try again? It seems like the model does not think it's a variant based on the evidence. If you want to manually change the RefCall to pass then you have to do it on the VCF as a post-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:213,security,model,model,213,"Hi @gushenweiz,. A `RefCall` can happen for several reasons. Seems like you have a very high depth in this data. Would it be possible to downsample it to something between 90-100x and try again? It seems like the model does not think it's a variant based on the evidence. If you want to manually change the RefCall to pass then you have to do it on the VCF as a post-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:448,deployability,observ,observe,448,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:219,energy efficiency,predict,predictive,219,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:603,energy efficiency,model,model,603,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:783,energy efficiency,model,model,783,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:840,energy efficiency,model,model,840,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:243,interoperability,specif,specific,243,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:671,interoperability,standard,standard,671,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:91,performance,network,network,91,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:219,safety,predict,predictive,219,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:91,security,network,network,91,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:603,security,model,model,603,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:783,security,model,model,783,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:840,security,model,model,840,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:427,testability,coverag,coverage,427,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:448,testability,observ,observe,448,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:658,testability,coverag,coverage,658,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:78,usability,help,helpful,78,"Hi @gushenweiz , hopefully Kishwar and Andrew's answers from before have been helpful. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/750:92,usability,close,close,92,"Hi @gushenweiz , hopefully Kishwar and Andrew's answers from before have been helpful. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/750
https://github.com/google/deepvariant/issues/751:247,availability,down,downstream,247,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:458,availability,mainten,maintenance,458,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:294,integrability,sub,subjective,294,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:458,reliability,mainten,maintenance,458,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:33,usability,feedback,feedback,33,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:473,usability,support,support,473,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:55,usability,help,helpful,55,"Hi @dmarkie ,. Hopefully Andrew's response earlier was helpful. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/751:69,usability,close,close,69,"Hi @dmarkie ,. Hopefully Andrew's response earlier was helpful. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/751
https://github.com/google/deepvariant/issues/752:15,availability,error,error,15,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:266,deployability,version,version,266,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:66,energy efficiency,current,current,66,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:266,integrability,version,version,266,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:266,modifiability,version,version,266,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:15,performance,error,error,15,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:297,performance,perform,perform,297,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:15,safety,error,error,15,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:15,usability,error,error,15,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:186,usability,help,help,186,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:297,usability,perform,perform,297,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). . * Can you provide any details on how it was aligned? * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:37,usability,help,helpful,37,"The comments from Andrew Carrol were helpful, for some reason the script I was using had ""--outSAMmode NoQS"" in the STAR command, removing this fixes the issue. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/752:121,usability,command,command,121,"The comments from Andrew Carrol were helpful, for some reason the script I was using had ""--outSAMmode NoQS"" in the STAR command, removing this fixes the issue. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/752
https://github.com/google/deepvariant/issues/753:36,deployability,build,build,36,"Hi @one-matrix ,. Are you trying to build deepvariant locally? If that's the case, then please follow the steps here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:183,deployability,build,build-test,183,"Hi @one-matrix ,. Are you trying to build deepvariant locally? If that's the case, then please follow the steps here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:189,safety,test,test,189,"Hi @one-matrix ,. Are you trying to build deepvariant locally? If that's the case, then please follow the steps here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:189,testability,test,test,189,"Hi @one-matrix ,. Are you trying to build deepvariant locally? If that's the case, then please follow the steps here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:46,availability,error,error,46,i have excute build_and_test.sh but the other error come. `. (py38) root@30634345fd4a:/deepvariant# ./build_and_test.sh. + source settings.sh. ++ export DV_USE_PREINSTALLED_TF=0. ++ DV_USE_PREINSTALLED_TF=0. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3459,availability,Avail,Available,3459,"RL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3912,availability,state,state,3912,"t-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8249,availability,error,errors,8249,"/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10280,availability,ERROR,ERROR,10280," --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10434,availability,error,error,10434,"/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13902,availability,INCID,INCIDENTAL,13902," met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14042,availability,SERVIC,SERVICES,14042,"ing disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3406,deployability,releas,release,3406,"ariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3505,deployability,build,build,3505,"ariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3589,deployability,build,build,3589,"SION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3595,deployability,Build,Builds,3595,".19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3872,deployability,configurat,configurations,3872,"IB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4170,deployability,instal,install,4170,"uild_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Readin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4178,deployability,Instal,Installs,4178,"ting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4295,deployability,depend,dependency,4295,"-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4449,deployability,Build,Builds,4449,"s> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_hea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4493,deployability,version,version,4493,"nalyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4508,deployability,version,version,4508,"rofile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5373,deployability,build,build,5373,"erver. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7411,deployability,build,build,7411,"s/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Foun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7696,deployability,build,build,7696,"k/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_seri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8309,deployability,build,build,8309,"(05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8445,deployability,build,build,8445,"IN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --defi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8820,deployability,build,build,8820,"no-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:9047,deployability,build,build,9047,"onfigure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:9736,deployability,build,build,9736,"k-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10332,deployability,BUILD,BUILD,10332,"nds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Iba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10401,deployability,fail,failed,10401,"IX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibaz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10426,deployability,fail,failed,10426,"(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/externa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12516,deployability,Configurat,Configuration,12516,"oogle_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14042,deployability,SERVIC,SERVICES,14042,"ing disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3488,energy efficiency,profil,profile,3488,"leapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3511,energy efficiency,profil,profile,3511,"packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3818,energy efficiency,Load,Loads,3818,"ATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info comma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6602,energy efficiency,core,core,6602,"es=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6635,energy efficiency,core,core,6635,"egacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6679,energy efficiency,core,core,6679,"ic_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6719,energy efficiency,core,core,6719," --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6759,energy efficiency,core,core,6759,"e=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6800,energy efficiency,core,core,6800,"pport=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6838,energy efficiency,core,core,6838,"brary --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6866,energy efficiency,core,core,6866,"atic_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-fun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6893,energy efficiency,core,core,6893,"-deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6918,energy efficiency,cpu,cpu,6918,"low/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6933,energy efficiency,core,core,6933,"ir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtim",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6958,energy efficiency,gpu,gpu,6958,"er/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6973,energy efficiency,core,core,6973,"nchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7013,energy efficiency,core,core,7013,"it/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc option",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7063,energy efficiency,core,core,7063,"t/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7073,energy efficiency,gpu,gpu,7073,"ms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7088,energy efficiency,core,core,7088,"ompiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --fla",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7133,energy efficiency,core,core,7133,"piler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7162,energy efficiency,core,core,7162,"low/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Read",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7195,energy efficiency,core,core,7195,"ensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7231,energy efficiency,core,core,7231,"nalysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7270,energy efficiency,core,core,7270,"ests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7295,energy efficiency,core,core,7295,"ler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8686,energy efficiency,gpu,gpu,8686,"erited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8755,energy efficiency,gpu,gpu,8755,"mps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:9877,energy efficiency,Current,Current,9877,"work_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10180,energy efficiency,load,loaded,10180,"-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3872,integrability,configur,configurations,3872,"IB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3912,integrability,state,state,3912,"t-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3970,integrability,repositor,repositories,3970,"packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4295,integrability,depend,dependency,4295,"-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4398,integrability,repositor,repositories,4398,"l release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4493,integrability,version,version,4493,"nalyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4508,integrability,version,version,4508,"rofile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6067,integrability,transform,transforms,6067,"by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6559,integrability,transform,transforms,6559,"-c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10070,integrability,repositor,repository,10070,"low/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10198,integrability,configur,configured,10198,"=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/exter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10350,integrability,wrap,wrapping,10350,"-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12516,integrability,Configur,Configuration,12516,"oogle_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13809,integrability,EVENT,EVENT,13809,"or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14022,integrability,SUB,SUBSTITUTE,14022,"ions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14042,integrability,SERVIC,SERVICES,14042,"ing disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14183,integrability,CONTRACT,CONTRACT,14183,"nd the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. 40: def `Log10PErrorToRoundedPhred` as log10_perror_to_rounded_phred(log10_perror: float) -> int\n. 41: def `Log10ToReal` as log10_perror_to_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3606,interoperability,specif,specified,3606,"ort DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3787,interoperability,specif,specified,3787,"in/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3851,interoperability,specif,specified,3851,"3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3970,interoperability,repositor,repositories,3970,"packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4332,interoperability,specif,specified,4332,"ld_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4398,interoperability,repositor,repositories,4398,"l release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4411,interoperability,specif,specified,4411,"3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4469,interoperability,specif,specified,4469,"nds:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4733,interoperability,specif,specifying,4733,"ver. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6067,interoperability,transform,transforms,6067,"by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6559,interoperability,transform,transforms,6559,"-c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6657,interoperability,convers,conversion,6657,"ble_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10070,interoperability,repositor,repository,10070,"low/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12609,interoperability,platform,platform,12609,"ternal/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12654,interoperability,platform,platform,12654,"xternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13285,interoperability,distribut,distribution,13285,"al/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF TH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13487,interoperability,specif,specific,13487,"til/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14183,interoperability,CONTRACT,CONTRACT,14183,"nd the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. 40: def `Log10PErrorToRoundedPhred` as log10_perror_to_rounded_phred(log10_perror: float) -> int\n. 41: def `Log10ToReal` as log10_perror_to_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:1193,modifiability,pac,packages,1193,LLED_TF=0. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:1246,modifiability,pac,packages,1246, ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packa,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:1330,modifiability,pac,packages,1330,lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. +,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:1407,modifiability,pac,packages,1407,ot/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepva,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2245,modifiability,pac,packages,2245,kages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2312,modifiability,pac,packages,2312,m/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2417,modifiability,pac,packages,2417,xport DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0].,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2515,modifiability,pac,packages,2515,W_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2917,modifiability,pac,packages,2917,"+ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:2976,modifiability,pac,packages,2976,"IZED_TF_WHL=1. ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositori",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3872,modifiability,configur,configurations,3872,"IB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4295,modifiability,depend,dependency,4295,"-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4493,modifiability,version,version,4493,"nalyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4508,modifiability,version,version,4508,"rofile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5088,modifiability,Inherit,Inherited,5088," about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/comp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5226,modifiability,Inherit,Inherited,5226,"ts the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5362,modifiability,Inherit,Inherited,5362,"e bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7400,modifiability,Inherit,Inherited,7400,"tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7550,modifiability,pac,packages,7550,"tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7685,modifiability,Inherit,Inherited,7685,"e_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:9274,modifiability,paramet,parameter,9274," applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10171,modifiability,pac,packages,10171,"precated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10198,modifiability,configur,configured,10198,"=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/exter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12516,modifiability,Configur,Configuration,12516,"oogle_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14042,modifiability,SERVIC,SERVICES,14042,"ing disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:46,performance,error,error,46,i have excute build_and_test.sh but the other error come. `. (py38) root@30634345fd4a:/deepvariant# ./build_and_test.sh. + source settings.sh. ++ export DV_USE_PREINSTALLED_TF=0. ++ DV_USE_PREINSTALLED_TF=0. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3488,performance,profil,profile,3488,"leapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3511,performance,profil,profile,3511,"packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3818,performance,Load,Loads,3818,"ATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info comma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6918,performance,cpu,cpu,6918,"low/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6958,performance,gpu,gpu,6958,"er/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7073,performance,gpu,gpu,7073,"ms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8249,performance,error,errors,8249,"/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8686,performance,gpu,gpu,8686,"erited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8755,performance,gpu,gpu,8755,"mps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:9930,performance,cach,cache,9930,"r_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10180,performance,load,loaded,10180,"-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10280,performance,ERROR,ERROR,10280," --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10434,performance,error,error,10434,"/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10471,performance,cach,cache,10471,"clude --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3459,reliability,Availab,Available,3459,"RL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10401,reliability,fail,failed,10401,"IX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibaz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10426,reliability,fail,failed,10426,"(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/externa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13902,reliability,INCID,INCIDENTAL,13902," met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:46,safety,error,error,46,i have excute build_and_test.sh but the other error come. `. (py38) root@30634345fd4a:/deepvariant# ./build_and_test.sh. + source settings.sh. ++ export DV_USE_PREINSTALLED_TF=0. ++ DV_USE_PREINSTALLED_TF=0. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3459,safety,Avail,Available,3459,"RL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3797,safety,test,test,3797,"on3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4295,safety,depend,dependency,4295,"-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4444,safety,test,test,4444,"options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4479,safety,test,test,4479,"nalyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4849,safety,test,test,4849,"specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --exp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5192,safety,test,test,5192," to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5328,safety,test,test,5328,"e specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6151,safety,test,tests,6151,":40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6187,safety,test,tests,6187," 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6226,safety,test,tests,6226,"erited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6271,safety,test,tests,6271,"remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6311,safety,test,tests,6311," options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6360,safety,test,tests,6360,"nherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6410,safety,test,tests,6410,"d_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6459,safety,test,tests,6459,"rue --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6511,safety,test,tests,6511,"_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7253,safety,test,tests,7253,"iler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7353,safety,test,test,7353,"tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7650,safety,test,test,7650,"back/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8022,safety,test,test,8022,"ager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8070,safety,test,test,8070,"/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8184,safety,test,test,8184,"nsorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8220,safety,test,test,8220,"nsorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8249,safety,error,errors,8249,"/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8591,safety,test,test,8591,"/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8672,safety,test,test,8672,".bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8741,safety,test,test,8741,"--show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10253,safety,test,test,10253," --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10280,safety,ERROR,ERROR,10280," --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10434,safety,error,error,10434,"/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13510,safety,permiss,permission,13510," Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3119,security,sign,sign-compare,3119,"NAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3280,security,sign,sign-compare,3280,"WHL_PATH=gs://deepvariant/packages/tensorflow. ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3459,security,Availab,Available,3459,"RL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3872,security,configur,configurations,3872,"IB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4914,security,sign,sign-compare,4914," the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10198,security,configur,configured,10198,"=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/exter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12516,security,Configur,Configuration,12516,"oogle_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:12830,security,modif,modification,12830,"lif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:14052,security,LOSS,LOSS,14052,"laimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n. 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n. 28:# POSSIBILITY OF SUCH DAMAGE.\n. 29:\n. 30:from ""third_party/nucleus/util/math.h"":\n. 31: namespace `nucleus`:\n. 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n. 33: log10_ptrue: float, value_if_not_finite: float) -> float\n. 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n. 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n. 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n. 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n. 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n. 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n. 40: def ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3743,testability,coverag,coverage,3743,"export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying tar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3767,testability,coverag,coverage,3767,"opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3797,testability,test,test,3797,"on3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4295,testability,depend,dependency,4295,"-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) IN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4444,testability,test,test,4444,"options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4479,testability,test,test,4479,"nalyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4849,testability,test,test,4849,"specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --exp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5192,testability,test,test,5192," to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:5328,testability,test,test,5328,"e specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6151,testability,test,tests,6151,":40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6187,testability,test,tests,6187," 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6226,testability,test,tests,6226,"erited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6271,testability,test,tests,6271,"remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6311,testability,test,tests,6311," options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6360,testability,test,tests,6360,"nherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6410,testability,test,tests,6410,"d_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6459,testability,test,tests,6459,"rue --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:6511,testability,test,tests,6511,"_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7253,testability,test,tests,7253,"iler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7353,testability,test,test,7353,"tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:7650,testability,test,test,7650,"back/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8022,testability,test,test,8022,"ager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8070,testability,test,test,8070,"/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8184,testability,test,test,8184,"nsorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8220,testability,test,test,8220,"nsorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8591,testability,test,test,8591,"/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8672,testability,test,test,8672,".bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8741,testability,test,test,8741,"--show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10253,testability,test,test,10253," --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:46,usability,error,error,46,i have excute build_and_test.sh but the other error come. `. (py38) root@30634345fd4a:/deepvariant# ./build_and_test.sh. + source settings.sh. ++ export DV_USE_PREINSTALLED_TF=0. ++ DV_USE_PREINSTALLED_TF=0. ++ export TF_NEED_GCP=1. ++ TF_NEED_GCP=1. ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu. ++ DV_BAZEL_VERSION=5.3.0. ++ export PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ PATH=/root/bin:/root/.vscode-server/bin/0ee08df0cf4527e40edc9aa28f4b5bd38bbff2b2/bin/remote-cli:/opt/conda/envs/py38/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/envs/bio/bin:/opt/deepvariant/bin:/root/bin. ++ export DEEPVARIANT_BUCKET=gs://deepvariant. ++ DEEPVARIANT_BUCKET=gs://deepvariant. ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages. ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages. ++ export DV_TF_NIGHTLY_BUILD=0. ++ DV_TF_NIGHTLY_BUILD=0. ++ [[ 0 = \1 ]]. ++ export DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ DV_CPP_TENSORFLOW_TAG=v2.11.0. ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=2.11.0. ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=2.11.0. ++ export DV_GPU_BUILD=1. ++ DV_GPU_BUILD=1. ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1. ++ ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3436,usability,command,command,3436," GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3469,usability,command,commands,3469,"ttps://storage.googleapis.com/deepvariant/packages/tensorflow. ++ export DV_TF_NUMPY_VERSION=1.19.2. ++ DV_TF_NUMPY_VERSION=1.19.2. ++ export DV_INSTALL_GPU_DRIVERS=0. ++ DV_INSTALL_GPU_DRIVERS=0. ++ export PYTHON_VERSION=3.8. ++ PYTHON_VERSION=3.8. +++ which python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the spec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:3725,usability,stop,stops,3725,"hich python3.8. ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8. ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages. ++ export USE_DEFAULT_PYTHON_LIB_PATH=1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4022,usability,help,help,4022,"1. ++ USE_DEFAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4034,usability,help,help,4034,"FAULT_PYTHON_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4043,usability,command,commands,4043,"N_LIB_PATH=1. ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4238,usability,command,command,4238,"AGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'commo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4359,usability,Stop,Stops,4359,"time_version=remotejdk_11'. + bazel. [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4552,usability,help,help,4552,"gets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4565,usability,help,help,4565,"ies the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4571,usability,command,command,4571,"action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4588,usability,help,help,4588,"uild Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --def",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4610,usability,command,command,4610,"d targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4626,usability,help,help,4626,"onicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4689,usability,help,help,4689,"moves output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4759,usability,help,help,4759," code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --defin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:4816,usability,command,command,4816,"ads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. + [[ 1 = \1 ]]. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/... (05:40:22) INFO: Options provided by the client:. Inherited 'common' options: --isatty=1 --terminal_columns=166. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'common' options: --experimental_repo_remote_exec. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:. Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --exper",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:8249,usability,error,errors,8249,"/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:. 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium. (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:. 'test' options: --test_output=errors. (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false. (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10280,usability,ERROR,ERROR,10280," --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10434,usability,error,error,10434,"/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:10450,usability,command,command,10450,"LUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. (05:40:23) INFO: Current date is 2023-12-18. (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: . Warning: skipping import of repository 'com_google_protobuf' because it already exists. (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured). (05:40:23) INFO: Found 141 targets and 48 test targets... (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command . (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \. exec env - \. bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Ie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/753:13230,usability,document,documentation,13230,"/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif). # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46. # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789. 1:# Copyright 2018 Google LLC.\n. 2:#\n. 3:# Redistribution and use in source and binary forms, with or without\n. 4:# modification, are permitted provided that the following conditions\n. 5:# are met:\n. 6:#\n. 7:# 1. Redistributions of source code must retain the above copyright notice,\n. 8:# this list of conditions and the following disclaimer.\n. 9:#\n. 10:# 2. Redistributions in binary form must reproduce the above copyright\n. 11:# notice, this list of conditions and the following disclaimer in the\n. 12:# documentation and/or other materials provided with the distribution.\n. 13:#\n. 14:# 3. Neither the name of the copyright holder nor the names of its\n. 15:# contributors may be used to endorse or promote products derived from this\n. 16:# software without specific prior written permission.\n. 17:#\n. 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n. 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n. 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n. 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n. 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n. 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n. 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n. 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n. 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/753
https://github.com/google/deepvariant/issues/754:61,deployability,log,logs,61,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:61,safety,log,logs,61,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:61,security,log,logs,61,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:61,testability,log,logs,61,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:88,testability,understand,understand,88,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:45,usability,command,command,45,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:45,usability,close,close,45,"Hi @gulkhan007 ,. Given the inactivity, I'll close this issue for now. Please feel free to open with more information if you still need help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/754:136,usability,help,help,136,"Hi @gulkhan007 ,. Given the inactivity, I'll close this issue for now. Please feel free to open with more information if you still need help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/754
https://github.com/google/deepvariant/issues/755:651,availability,avail,available,651,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:40,deployability,version,version,40,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:66,deployability,contain,contained,66,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:129,deployability,version,version,129,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:168,deployability,version,versions,168,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:235,deployability,depend,dependencies,235,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:325,deployability,depend,dependencies,325,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:358,deployability,updat,updating,358,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:495,deployability,updat,updated,495,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:574,deployability,updat,updates,574,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:611,deployability,updat,updates,611,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:40,integrability,version,version,40,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:129,integrability,version,version,129,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:168,integrability,version,versions,168,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:235,integrability,depend,dependencies,235,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:325,integrability,depend,dependencies,325,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:40,modifiability,version,version,40,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:129,modifiability,version,version,129,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:168,modifiability,version,versions,168,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:235,modifiability,depend,dependencies,235,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:325,modifiability,depend,dependencies,325,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:462,performance,time,time,462,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:651,reliability,availab,available,651,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:235,safety,depend,dependencies,235,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:325,safety,depend,dependencies,325,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:358,safety,updat,updating,358,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:495,safety,updat,updated,495,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:574,safety,updat,updates,574,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:611,safety,updat,updates,611,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:651,safety,avail,available,651,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:358,security,updat,updating,358,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:495,security,updat,updated,495,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:574,security,updat,updates,574,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:611,security,updat,updates,611,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:651,security,availab,available,651,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:235,testability,depend,dependencies,235,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:325,testability,depend,dependencies,325,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:57,usability,help,helpful,57,"Hi @dbrami , . hopefully Andrew's answer from before was helpful. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/755:71,usability,close,close,71,"Hi @dbrami , . hopefully Andrew's answer from before was helpful. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/755
https://github.com/google/deepvariant/issues/756:23,deployability,build,building,23,@one-matrix details on building from source are here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Are you using Ubuntu 20.04? Can you tell me a little bit more about what you are trying to do?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:119,deployability,build,build-test,119,@one-matrix details on building from source are here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Are you using Ubuntu 20.04? Can you tell me a little bit more about what you are trying to do?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:125,safety,test,test,125,@one-matrix details on building from source are here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Are you using Ubuntu 20.04? Can you tell me a little bit more about what you are trying to do?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:125,testability,test,test,125,@one-matrix details on building from source are here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Are you using Ubuntu 20.04? Can you tell me a little bit more about what you are trying to do?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:95,deployability,build,build-prereq,95,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:458,deployability,modul,module,458,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:577,deployability,modul,module,577,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:716,deployability,modul,module,716,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:59,energy efficiency,model,model,59,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:458,modifiability,modul,module,458,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:577,modifiability,modul,module,577,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:716,modifiability,modul,module,716,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:458,safety,modul,module,458,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:577,safety,modul,module,577,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:716,safety,modul,module,716,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:59,security,model,model,59,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:351,testability,Trace,Traceback,351,"@danielecook hi danielecook ,I want to try to speed up the model inference. Have been executed build-prereq.sh and build_and_test.sh. debug in system docker image: google/deepvariant:1.6.0, Ubuntu 20.04 . How to reference cilf files also have some problems. `. root@7065ad26b62a:/deepvariant# python ./deepvariant/python/variant_calling_wrap_test.py. Traceback (most recent call last):. File ""./deepvariant/python/variant_calling_wrap_test.py"", line 36, in <module>. from third_party.nucleus.io import fasta. File ""/deepvariant/./third_party/nucleus/io/fasta.py"", line 60, in <module>. from third_party.nucleus.io import genomics_reader. File ""/deepvariant/./third_party/nucleus/io/genomics_reader.py"", line 84, in <module>. from third_party.nucleus.io.python import tfrecord_reader. ImportError: cannot import name 'tfrecord_reader' from 'third_party.nucleus.io.python' (/deepvariant/./third_party/nucleus/io/python/__init__.py). `.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:26,deployability,version,version,26,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:138,deployability,build,build-prereq,138,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:26,integrability,version,version,26,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:26,modifiability,version,version,26,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:232,reliability,doe,doesn,232,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:20,safety,avoid,avoid,20,@danielecook yesto avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:501,availability,down,down,501,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:380,deployability,build,build-test,380,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:560,deployability,build,build,560,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1502,deployability,build,build-test,1502,"elow for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1576,deployability,build,build-prereq,1576,"Variant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1723,deployability,build,build,1723,"/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2294,deployability,continu,continue,2294,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2334,deployability,build,build,2334,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2566,deployability,build,build,2566,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2623,deployability,build,build,2623,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2731,deployability,build,build,2731,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2794,deployability,build,build,2794,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:791,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,791,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:884,energy efficiency,cpu,cpu,884,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:926,energy efficiency,cloud,cloud-platform,926,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1008,energy efficiency,cloud,cloud,1008,"rix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1108,energy efficiency,cpu,cpu-platform,1108," work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Some",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1271,energy efficiency,cpu,cpu,1271,"e it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:824,interoperability,platform,platform,824,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:932,interoperability,platform,platform,932,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1037,interoperability,standard,standard-,1037," you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1112,interoperability,platform,platform,1112,"ork in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Someth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:791,performance,cpu,cpu-only-machine-on-google-cloud-platform,791,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:884,performance,cpu,cpu,884,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1060,performance,disk,disk-size,1060,"`python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1108,performance,cpu,cpu-platform,1108," work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Some",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1271,performance,cpu,cpu,1271,"e it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1608,reliability,doe,does,1608,"get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:386,safety,test,test,386,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1508,safety,test,test,1508,"w for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1763,safety,test,tested,1763,"details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run some",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2663,safety,test,test,2663,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2681,safety,test,tests,2681,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1145,security,ssh,ssh,1145,"eepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1212,security,ssh,ssh,1212," pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1259,security,ssh,ssh,1259,"won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2315,security,modif,modify,2315,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:234,testability,simpl,simply,234,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:386,testability,test,test,386,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1508,testability,test,test,1508,"w for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1763,testability,test,tested,1763,"details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run some",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2663,testability,test,test,2663,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2676,testability,unit,unit,2676,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2681,testability,test,tests,2681,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:234,usability,simpl,simply,234,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:297,usability,document,documented,297,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:450,usability,clarit,clarity,450,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:682,usability,command,command,682,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:777,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,777,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:878,usability,USER,USER,878,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:1265,usability,USER,USER,1265,"execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash. gcloud compute instances create ""${USER}-cpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-64"" \. --boot-disk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2197,usability,help,help,2197,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:2480,usability,help,help,2480,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:3034,usability,help,help,3034,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:3055,usability,help,helps,3055,"isk-size ""300"" \. --zone ""us-west1-b"" \. --min-cpu-platform ""Intel Skylake"". ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash. gcloud compute ssh ${USER}-cpu --zone us-west1-b. ```. Then I get the repo:. ```bash. git clone https://github.com/google/deepvariant.git. cd deepvariant. ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash. sudo su. ```. and then:. ```bash. ./build-prereq.sh. ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash. ./build_and_test.sh. ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash. python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help. ```. should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash. source settings.sh. ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/... ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash. bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants. ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash. bazel-bin/deepvariant/call_variants --help. ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:200,deployability,version,versions,200,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:364,energy efficiency,Reduc,Reduce,364,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:165,integrability,coupl,coupled,165,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:200,integrability,version,versions,200,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:165,modifiability,coupl,coupled,165,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:200,modifiability,version,versions,200,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:375,performance,I/O,I/O,375,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:444,performance,memor,memory,444,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:479,performance,memor,memory,479,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:112,safety,compl,complicated,112,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:112,security,compl,complicated,112,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:165,testability,coupl,coupled,165,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:68,usability,help,help,68,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:444,usability,memor,memory,444,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:479,usability,memor,memory,479,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people. This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. . I have another idea, and here's how it works. make-example-->file-->callvariant-->file-->postprocess. Can we consider this approachReduce the I/O read and write and data serialization. make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:206,deployability,releas,release,206,"Hi @one-matrix ,. In the past few months, our team member @akolesnikov has been experimenting with something conceptually similar to what you described. The work is not finalized, but hopefully in our next release in 2024 , some improved implementation will come out. Thanks for the suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:46,security,team,team,46,"Hi @one-matrix ,. In the past few months, our team member @akolesnikov has been experimenting with something conceptually similar to what you described. The work is not finalized, but hopefully in our next release in 2024 , some improved implementation will come out. Thanks for the suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:5,usability,close,close,5,"I'll close this issue now. If you have more questions, feel free to open new issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:48,deployability,build,build-test,48,"By the way, I'll add a pointer from deepvariant-build-test.md to https://github.com/google/deepvariant/issues/756#issuecomment-1865388872. The change will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:176,deployability,releas,release,176,"By the way, I'll add a pointer from deepvariant-build-test.md to https://github.com/google/deepvariant/issues/756#issuecomment-1865388872. The change will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:54,safety,test,test,54,"By the way, I'll add a pointer from deepvariant-build-test.md to https://github.com/google/deepvariant/issues/756#issuecomment-1865388872. The change will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/756:54,testability,test,test,54,"By the way, I'll add a pointer from deepvariant-build-test.md to https://github.com/google/deepvariant/issues/756#issuecomment-1865388872. The change will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/756
https://github.com/google/deepvariant/issues/757:4,availability,error,error,4,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:375,availability,operat,operations,375,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:429,availability,operat,operations,429,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:668,availability,servic,service,668,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:676,availability,servic,service,676,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:696,availability,servic,service,696,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:857,availability,servic,service,857,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:865,availability,servic,service,865,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1884,availability,Cluster,ClusterSpec,1884,"cutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 13",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:2671,availability,Cluster,ClusterSpec,2671,": None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3428,availability,slo,sloppy,3428,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remaind",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3964,availability,slo,sloppy,3964,"ults_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6003,availability,Restor,Restoring,6003,"sed implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6113,availability,Restor,Restoring,6113," Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7181,availability,restor,restore,7181,"1:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11446,availability,sli,slices,11446,"ile ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12345,availability,checkpoint,checkpoint,12345,"training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12559,availability,restor,restore,12559,"ore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13251,availability,checkpoint,checkpoint,13251,"ompat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15649,availability,restor,restore,15649,"verableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15792,availability,restor,restore,15792,"1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15916,availability,Restor,Restoring,15916,"rflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15931,availability,checkpoint,checkpoint,15931,"aining/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16037,availability,checkpoint,checkpoint,16037,"ile ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 49",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16121,availability,checkpoint,checkpoint,16121,"sion.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16142,availability,error,error,16142," in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19769,availability,sli,slices,19769,"ile ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21264,availability,checkpoint,checkpoint,21264,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21352,availability,error,error,21352,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:668,deployability,servic,service,668,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:676,deployability,servic,service,676,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:696,deployability,servic,service,696,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:857,deployability,servic,service,857,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:865,deployability,servic,service,865,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:922,deployability,Version,Version,922,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1884,deployability,Cluster,ClusterSpec,1884,"cutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 13",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:2671,deployability,Cluster,ClusterSpec,2671,": None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3258,deployability,version,version,3258,": None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be rem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3284,deployability,updat,updating,3284,"_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3794,deployability,version,version,3794,"is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3820,deployability,updat,updating,3820,"replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updatin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4279,deployability,version,version,4279," updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.bas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4305,deployability,updat,updating,4305,"ataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4791,deployability,version,version,4791,"ersion. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4817,deployability,updat,updating,4817,"pdating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 13969469949312",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5341,deployability,version,version,5341,"func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-000",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5367,deployability,updat,updating,5367," followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5665,deployability,version,version,5665,":381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5691,deployability,updat,updating,5691,"tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6257,deployability,fail,failed,6257,"python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8050,deployability,stack,stack,8050,"back (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8198,deployability,modul,module,8198,"_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10558,deployability,build,build,10558,"create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10662,deployability,build,build,10662,"ion.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13480,deployability,modul,module,13480,"python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_per",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15942,deployability,fail,failed,15942,"nitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16373,deployability,stack,stack,16373," File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16521,deployability,modul,module,16521,"sr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18881,deployability,build,build,18881,"create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18985,deployability,build,build,18985,"ion.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20557,deployability,modul,module,20557,"ine 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:189,energy efficiency,core,core,189,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:255,energy efficiency,optim,optimized,255,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:334,energy efficiency,CPU,CPU,334,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:538,energy efficiency,core,core,538,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:584,energy efficiency,CPU,CPU,584,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:588,energy efficiency,Frequenc,Frequency,588,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:972,energy efficiency,core,core,972,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1179,energy efficiency,model,model,1179,"orflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1252,energy efficiency,estimat,estimator,1252,"timized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1297,energy efficiency,model,model,1297,"rary (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:2117,energy efficiency,estimat,estimator,2117,"erformance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4452,energy efficiency,optim,optimizations,4452,"e `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4964,energy efficiency,optim,optimizations,4964,"execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5100,energy efficiency,estimat,estimator,5100,"com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5823,energy efficiency,estimat,estimator,5823,"Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6034,energy efficiency,model,models,6034,"orflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6048,energy efficiency,model,model,6048," model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6144,energy efficiency,model,models,6144,"nsorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6158,energy efficiency,model,model,6158,"/usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6211,energy efficiency,core,core,6211,"s/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filenam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6312,energy efficiency,model,models,6312,"ll be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/sess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6326,energy efficiency,model,model,6326,"in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6873,energy efficiency,model,models,6873,"flow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6887,energy efficiency,model,model,6887," finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7832,energy efficiency,model,models,7832,"ework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7846,energy efficiency,model,model,7846,"mpl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8009,energy efficiency,estimat,estimator,8009,"ion, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8019,energy efficiency,estimat,estimator,8019,"er exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8880,energy efficiency,predict,prediction,8880," file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8898,energy efficiency,predict,predictions,8898," 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8984,energy efficiency,estimat,estimator,8984,"ow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8994,energy efficiency,estimat,estimator,8994,"or/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9022,energy efficiency,predict,predict,9022,"or.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14167,energy efficiency,predict,prediction,14167,"ework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14185,energy efficiency,predict,predictions,14185,"otFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._sessi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14272,energy efficiency,estimat,estimator,14272,"g of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14282,energy efficiency,estimat,estimator,14282,"bove exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14310,energy efficiency,predict,predict,14310,"eption occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16155,energy efficiency,model,models,16155,"ssion. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16169,energy efficiency,model,model,16169,"_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16332,energy efficiency,estimat,estimator,16332,"ssion. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16342,energy efficiency,estimat,estimator,16342,"t_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17203,energy efficiency,predict,prediction,17203," file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17221,energy efficiency,predict,predictions,17221," 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17307,energy efficiency,estimat,estimator,17307,"ow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17317,energy efficiency,estimat,estimator,17317,"or/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17345,energy efficiency,predict,predict,17345,"or.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21281,energy efficiency,model,models,21281,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21295,energy efficiency,model,model,21295,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21384,energy efficiency,model,model,21384,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21402,energy efficiency,model,models,21402,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21416,energy efficiency,model,model,21416,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21605,energy efficiency,model,models,21605,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21619,energy efficiency,model,model,21619,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21800,energy efficiency,model,models,21800,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21816,energy efficiency,model,model,21816,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21847,energy efficiency,model,model,21847,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21864,energy efficiency,model,model,21864,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21926,energy efficiency,model,models,21926,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21940,energy efficiency,model,model,21940,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21977,energy efficiency,model,models,21977,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21991,energy efficiency,model,model,21991,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:668,integrability,servic,service,668,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:676,integrability,servic,service,676,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:696,integrability,servic,service,696,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:857,integrability,servic,service,857,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:865,integrability,servic,service,865,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:922,integrability,Version,Version,922,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3258,integrability,version,version,3258,": None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be rem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3794,integrability,version,version,3794,"is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4223,integrability,batch,batching,4223,"nd will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4279,integrability,version,version,4279," updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.bas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4401,integrability,batch,batch,4401,".AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4735,integrability,batch,batching,4735,"ave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4791,integrability,version,version,4791,"ersion. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4913,integrability,batch,batch,4913,"lls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5341,integrability,version,version,5341,"func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-000",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5665,integrability,version,version,5665,":381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7762,integrability,messag,message,7762,"e 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20853,integrability,sub,subprocess,20853,"_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20946,integrability,sub,subprocess,20946,"ensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21027,integrability,sub,subprocess,21027,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:194,interoperability,platform,platform,194,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:543,interoperability,platform,platform,543,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:730,interoperability,platform,platform,730,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7762,interoperability,messag,message,7762,"e 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8293,interoperability,platform,platform,8293,"python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/trainin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13576,interoperability,platform,platform,13576,"s = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16616,interoperability,platform,platform,16616,"n _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/trainin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:668,modifiability,servic,service,668,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:676,modifiability,servic,service,676,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:696,modifiability,servic,service,696,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:857,modifiability,servic,service,857,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:865,modifiability,servic,service,865,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:922,modifiability,Version,Version,922,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3258,modifiability,version,version,3258,": None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be rem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3794,modifiability,version,version,3794,"is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4279,modifiability,version,version,4279," updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.bas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4791,modifiability,version,version,4791,"ersion. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5191,modifiability,pac,packages,5191,"ython.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.71187",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5208,modifiability,layer,layers,5208,"imental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5215,modifiability,layer,layers,5215,".ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/fr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5231,modifiability,Layer,Layer,5231,"s deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kerne",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5341,modifiability,version,version,5341,"func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-000",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5390,modifiability,layer,layer,5390,"Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5515,modifiability,pac,packages,5515,"19 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5532,modifiability,layer,layers,5532,"21 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5539,modifiability,layer,layers,5539,"94699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*arg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5555,modifiability,Layer,Layer,5555,"ecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5665,modifiability,version,version,5665,":381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5714,modifiability,layer,layer,5714,".experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6013,modifiability,paramet,parameters,6013,"entation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6041,modifiability,pac,pacbio,6041,"Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6123,modifiability,paramet,parameters,6123,"odel_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6151,modifiability,pac,pacbio,6151,"w:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/trainin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6319,modifiability,pac,pacbio,6319,"emoved in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6457,modifiability,pac,packages,6457,"eprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6582,modifiability,pac,packages,6582,"thon.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6716,modifiability,pac,packages,6716,"__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6880,modifiability,pac,pacbio,6880,"aph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No su",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7120,modifiability,pac,packages,7120,"arameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7277,modifiability,pac,packages,7277,"2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7397,modifiability,pac,packages,7397,"recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7542,modifiability,pac,packages,7542,"File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7663,modifiability,pac,packages,7663,"n_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7839,modifiability,pac,pacbio,7839,"rrors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7972,modifiability,pac,packages,7972,"]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8198,modifiability,modul,module,8198,"_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8266,modifiability,pac,packages,8266,"6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8947,modifiability,pac,packages,8947,"l/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9097,modifiability,pac,packages,9097,"l.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9263,modifiability,pac,packages,9263,"ackages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9441,modifiability,pac,packages,9441,"_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9615,modifiability,pac,packages,9615," ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9784,modifiability,pac,packages,9784,"/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9962,modifiability,pac,packages,9962,"_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10112,modifiability,pac,packages,10112,"/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShard",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10318,modifiability,pac,packages,10318,"line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10483,modifiability,pac,packages,10483,"session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10602,modifiability,pac,packages,10602,"3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10770,modifiability,pac,packages,10770,"n3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10903,modifiability,pac,packages,10903,"r.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11047,modifiability,pac,packages,11047,"elf._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11189,modifiability,pac,packages,11189," self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11323,modifiability,pac,packages,11323,"tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_G",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11498,modifiability,pac,packages,11498,"/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11618,modifiability,pac,packages,11618,"python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11776,modifiability,pac,packages,11776,"ensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11908,modifiability,pac,packages,11908,"tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12163,modifiability,pac,packages,12163,"al/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12498,modifiability,pac,packages,12498,"ages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12657,modifiability,pac,packages,12657,"_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_fla",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12855,modifiability,pac,packages,12855,"_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12998,modifiability,pac,packages,12998,"ck.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13480,modifiability,modul,module,13480,"python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_per",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13549,modifiability,pac,packages,13549,"9, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14235,modifiability,pac,packages,14235,"ot found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14386,modifiability,pac,packages,14386,"les_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14553,modifiability,pac,packages,14553,"/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14732,modifiability,pac,packages,14732,"l/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14907,modifiability,pac,packages,14907,"Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15077,modifiability,pac,packages,15077,"les/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15256,modifiability,pac,packages,15256,"ator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15413,modifiability,pac,packages,15413,"ning/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15552,modifiability,pac,packages,15552,"kages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). Fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15731,modifiability,pac,packages,15731,"kages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15809,modifiability,Variab,Variable,15809,". _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15979,modifiability,Variab,Variable,15979,"eate_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16162,modifiability,pac,pacbio,16162,"self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16295,modifiability,pac,packages,16295,"ed_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16521,modifiability,modul,module,16521,"sr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16589,modifiability,pac,packages,16589,"on_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17270,modifiability,pac,packages,17270,"l/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17420,modifiability,pac,packages,17420,"l.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17586,modifiability,pac,packages,17586,"ackages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17764,modifiability,pac,packages,17764,"_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17938,modifiability,pac,packages,17938," ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18107,modifiability,pac,packages,18107,"/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18285,modifiability,pac,packages,18285,"_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18435,modifiability,pac,packages,18435,"/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShard",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18641,modifiability,pac,packages,18641,"line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18806,modifiability,pac,packages,18806,"session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18925,modifiability,pac,packages,18925,"3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19093,modifiability,pac,packages,19093,"n3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19226,modifiability,pac,packages,19226,"r.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19370,modifiability,pac,packages,19370,"elf._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19512,modifiability,pac,packages,19512," self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19646,modifiability,pac,packages,19646,"tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19821,modifiability,pac,packages,19821,"/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19941,modifiability,pac,packages,19941,"python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20099,modifiability,pac,packages,20099,"ensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20231,modifiability,pac,packages,20231,"tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20557,modifiability,modul,module,20557,"ine 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20617,modifiability,pac,packages,20617,"local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20717,modifiability,pac,packages,20717,"eturn io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21288,modifiability,pac,pacbio,21288,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21409,modifiability,pac,pacbio,21409,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21612,modifiability,pac,pacbio,21612,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21807,modifiability,pac,pacbio,21807,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21933,modifiability,pac,pacbio,21933,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21984,modifiability,pac,pacbio,21984,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4,performance,error,error,4,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:255,performance,optimiz,optimized,255,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:289,performance,Network,Network,289,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:334,performance,CPU,CPU,334,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:354,performance,perform,performance-critical,354,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:584,performance,CPU,CPU,584,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1072,performance,Tune,Tune,1072,"94699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1121,performance,perform,performance,1121,"ples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4223,performance,batch,batching,4223,"nd will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4401,performance,batch,batch,4401,".AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4452,performance,optimiz,optimizations,4452,"e `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4735,performance,batch,batching,4735,"ave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4913,performance,batch,batch,4913,"lls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4964,performance,optimiz,optimizations,4964,"execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16142,performance,error,error,16142," in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21069,performance,time,time,21069,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21352,performance,error,error,21352,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:750,reliability,doe,does,750,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3428,reliability,slo,sloppy,3428,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remaind",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3964,reliability,slo,sloppy,3964,"ults_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6003,reliability,Restor,Restoring,6003,"sed implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6113,reliability,Restor,Restoring,6113," Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6257,reliability,fail,failed,6257,"python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7181,reliability,restor,restore,7181,"1:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:11446,reliability,sli,slices,11446,"ile ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12345,reliability,checkpoint,checkpoint,12345,"training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12559,reliability,restor,restore,12559,"ore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13251,reliability,checkpoint,checkpoint,13251,"ompat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15649,reliability,restor,restore,15649,"verableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15792,reliability,restor,restore,15792,"1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15916,reliability,Restor,Restoring,15916,"rflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15931,reliability,checkpoint,checkpoint,15931,"aining/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:15942,reliability,fail,failed,15942,"nitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16037,reliability,checkpoint,checkpoint,16037,"ile ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 49",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16121,reliability,checkpoint,checkpoint,16121,"sion.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:19769,reliability,sli,slices,19769,"ile ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21264,reliability,checkpoint,checkpoint,21264,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21449,reliability,doe,does,21449,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4,safety,error,error,4,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:117,safety,input,input,117,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3284,safety,updat,updating,3284,"_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3820,safety,updat,updating,3820,"replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updatin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4305,safety,updat,updating,4305,"ataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4817,safety,updat,updating,4817,"pdating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 13969469949312",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5367,safety,updat,updating,5367," followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5691,safety,updat,updating,5691,"tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7008,safety,except,exception,7008,"parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/esti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7027,safety,except,exception,7027,"t/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8198,safety,modul,module,8198,"_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8880,safety,predict,prediction,8880," file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8898,safety,predict,predictions,8898," 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9022,safety,predict,predict,9022,"or.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12051,safety,except,exception,12051,"tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12070,safety,except,exception,12070,"raining/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in err",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12386,safety,except,exception,12386,"tore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12405,safety,except,exception,12405,".restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13292,safety,except,exception,13292,": Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13311,safety,except,exception,13311,"LE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13480,safety,modul,module,13480,"python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_per",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14167,safety,predict,prediction,14167,"ework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14185,safety,predict,predictions,14185,"otFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._sessi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14310,safety,predict,predict,14310,"eption occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16142,safety,error,error,16142," in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16521,safety,modul,module,16521,"sr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_sec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17203,safety,predict,prediction,17203," file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17221,safety,predict,predictions,17221," 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17345,safety,predict,predict,17345,"or.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20557,safety,modul,module,20557,"ine 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21352,safety,error,error,21352,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:289,security,Network,Network,289,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1179,security,model,model,1179,"orflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1297,security,model,model,1297,"rary (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3284,security,updat,updating,3284,"_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:3820,security,updat,updating,3820,"replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updatin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4305,security,updat,updating,4305,"ataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4817,security,updat,updating,4817,"pdating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`. WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 13969469949312",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5367,security,updat,updating,5367," followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:5691,security,updat,updating,5691,"tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. INFO:tensorflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6034,security,model,models,6034,"orflow:Calling model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6048,security,model,model,6048," model_fn. I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6144,security,model,models,6144,"nsorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6158,security,model,model,6158,"/usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6312,security,model,models,6312,"ll be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/sess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6326,security,model,model,6326,"in a future version. Instructions for updating:. Please use `layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6491,security,session,session,6491,"/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6616,security,session,session,6616,"s deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_meta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6750,security,session,session,6750,"sorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6873,security,model,models,6873,"flow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6887,security,model,model,6887," finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7311,security,session,session,7311,"els/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7431,security,session,session,7431,"al/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7576,security,session,session,7576,"t-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7697,security,session,session,7697,"python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7832,security,model,models,7832,"ework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7846,security,model,model,7846,"mpl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:10275,security,access,access,10275,"flow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_seque",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16155,security,model,models,16155,"ssion. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16169,security,model,model,16169,"_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:18598,security,access,access,18598,"flow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session. self._scaffold.finalize(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__. self.build(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build. self._build(self._filename, build_save=True, build_restore=True). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build. build_restore=build_restore). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal. restore_sequentially, reshape). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_seque",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21281,security,model,models,21281,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21295,security,model,model,21295,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21384,security,model,model,21384,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21402,security,model,models,21402,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21416,security,model,model,21416,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21605,security,model,models,21605,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21619,security,model,model,21619,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21800,security,model,models,21800,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21816,security,model,model,21816,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21847,security,model,model,21847,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21864,security,model,model,21864,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21926,security,model,models,21926,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21940,security,model,model,21940,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21977,security,model,models,21977,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21991,security,model,model,21991,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:6385,testability,Trace,Traceback,6385,"layer.__call__` method instead. W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. INFO:tensorflow:Done calling model_fn. I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn. INFO:tensorflow:Graph was finalized. I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:7048,testability,Trace,Traceback,7048,".ckpt. I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt. 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do_call. return fn(*args). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _run_fn. target_list, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1443, in _call_tf_sessionrun. run_metadata). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:8056,testability,trace,trace,8056,"most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run. run_metadata_ptr). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run. feed_dict_tensor, options, run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run. run_metadata). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:9031,testability,hook,hooks,9031,"9) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12091,testability,Trace,Traceback,12091,"ne 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:12426,testability,Trace,Traceback,12426,"tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor. self, compat.as_bytes(tensor_str)). RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:13332,testability,Trace,Traceback,13332,"ound in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore. names_to_keys = object_graph_key_mapping(save_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping. object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor. error_translator(e). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator. raise errors_impl.NotFoundError(None, None, error_message). tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:14319,testability,hook,hooks,14319,"ccurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 66",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16379,testability,trace,trace,16379,"""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:17354,testability,hook,hooks,17354,"9) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants. prediction = next(predictions). File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict. hooks=all_hooks) as mon_sess:. File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session. return self._sess_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session. self.tf_sess = self._session_creator.create_session(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20459,testability,Trace,Traceback,20459,"re_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:4,usability,error,error,4,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:117,usability,input,input,117,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:354,usability,perform,performance-critical,354,"The error reported in the `docx` file is:. ```. I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:1121,usability,perform,performance,1121,"ples: [100, 221, 9]. 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz. 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:. 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version. 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf. W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I1219 05:41:38.123045 139694699493120 estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:16142,usability,error,error,16142," in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session. config=config). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore. err, ""a Variable name or other graph key that is missing""). tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory. 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>. tf.compat.v1.app.run(). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run. _run_main(main, args). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main. use_tpu=FLAGS.use_tpu,. File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20361,usability,user,user,20361,"6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps. name=""restore_shard"")). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps. restore_sequentially). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:20875,usability,command,command,20875,"in restore_v2. name=name). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21058,usability,Command,Command,21058,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21333,usability,statu,status,21333,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21352,usability,error,error,21352,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:21499,usability,command,command,21499,"r. attrs=attr_protos, op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal. op_def=op_def). File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__. self._traceback = tf_stack.extract_stack(). real	0m19.667s. user	0m15.445s. sys	0m3.603s. I1219 05:41:49.700348 140338112591616 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command '( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" )' returned non-zero exit status 1. ```. The error seems to suggest that the model file (`/opt/models/pacbio/model.ckpt.data-00000-of-00001`) does not exist. Can you try running the following command:. ```. BIN_VERSION=""1.1.0"". docker run -it google/deepvariant:""${BIN_VERSION}"" /bin/bash. ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```. You should see the following output:. ```.  docker run -it google/deepvariant:1.1.0 /bin/bash. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta. $root@df876e3a15f7:/opt/deepvariant# ls /opt/models/pacbio/model.ckpt.data-00000-of-00001. /opt/models/pacbio/model.ckpt.data-00000-of-00001. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:160,deployability,instal,install,160,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:103,energy efficiency,model,models,103,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:117,energy efficiency,model,model,117,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:110,modifiability,pac,pacbio,110,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:103,security,model,models,103,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:117,security,model,model,117,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:27,usability,command,command,27,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:191,usability,help,help,191,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help. ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:65,deployability,version,version,65,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:201,energy efficiency,model,models,201,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:217,energy efficiency,model,model,217,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:249,energy efficiency,model,model,249,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:267,energy efficiency,model,model,267,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:65,integrability,version,version,65,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:65,modifiability,version,version,65,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:208,modifiability,pac,pacbio,208,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:201,security,model,models,201,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:217,security,model,model,217,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:249,security,model,model,249,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:267,security,model,model,267,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:22,usability,confirm,confirm,22,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:126,usability,confirm,confirmed,126,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:306,usability,command,command,306,"Hi @hcsun1 ,. Can you confirm that you're using the 1.1.0 docker version? It should be inside the Docker image itself. I just confirmed by:. ```bash. $ sudo docker run google/deepvariant:1.1.0 ls /opt/models/pacbio/. model.ckpt.data-00000-of-00001. model.ckpt.index. model.ckpt.meta. ```. Can you run that command in your environment and see what you see?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/757:69,usability,close,close,69,"Hi @hcsun1 , . This issue has been inactive for a few weeks, so I'll close it. Please feel free to follow up if you still have questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/757
https://github.com/google/deepvariant/issues/758:305,usability,user,users,305,"Hi @husamia , Given you're using `kishwars/pepper_deepvariant` , the candidate finding part of that codebase is separate from DeepVariant. I'll tag @kishwarshafin to see if he has any comments. . But it might also be better to move this discussion to the pepper_deepvariant repo, so it's easier for other users to see this thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/758
https://github.com/google/deepvariant/issues/759:164,deployability,version,version,164,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:18,energy efficiency,load,load,18,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:33,energy efficiency,model,model,33,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:44,energy efficiency,GPU,GPU,44,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:156,energy efficiency,current,current,156,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:164,integrability,version,version,164,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:164,modifiability,version,version,164,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:18,performance,load,load,18,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:44,performance,GPU,GPU,44,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:68,performance,memor,memory,68,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:204,performance,memor,memory,204,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:33,security,model,model,33,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:68,usability,memor,memory,68,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:204,usability,memor,memory,204,"Hi @husamia ,. To load the Keras model with GPU, you'll need enough memory. . In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:63,energy efficiency,load,load,63,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:88,energy efficiency,load,loaded,88,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:98,energy efficiency,GPU,GPU,98,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:157,energy efficiency,load,load,157,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:166,energy efficiency,model,model,166,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:63,performance,load,load,63,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:88,performance,load,loaded,88,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:98,performance,GPU,GPU,98,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:134,performance,memor,memory,134,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:157,performance,load,load,157,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:166,security,model,model,166,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:134,usability,memor,memory,134,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/759:40,usability,close,close,40,"Thanks @kishwarshafin ! @husamia , I'll close this issue now. Feel free to follow up if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/759
https://github.com/google/deepvariant/issues/761:20,usability,help,helping,20,"Hi @melop , I'll be helping you with this, but I'm a bit behind on my tasks. I'll assign this myself and I'll try to take a look soon (hopefully by end of tomorrow!)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:114,energy efficiency,GPU,GPU,114,"Hi @melop , sorry that it took me a while. I have a question for you: When call_variants step is running, is your GPU being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:114,performance,GPU,GPU,114,"Hi @melop , sorry that it took me a while. I have a question for you: When call_variants step is running, is your GPU being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:274,availability,error,error,274,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:311,availability,error,error,311,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:193,deployability,fail,failed,193,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:392,deployability,stack,stackoverflow,392,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:54,energy efficiency,GPU,GPU,54,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:280,integrability,messag,message,280,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:317,integrability,messag,message,317,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:280,interoperability,messag,message,280,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:317,interoperability,messag,message,317,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:421,interoperability,Specif,Specifically,421,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:29,performance,time,time,29,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:54,performance,GPU,GPU,54,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:274,performance,error,error,274,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:311,performance,error,error,311,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:193,reliability,fail,failed,193,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:264,safety,detect,detected,264,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:274,safety,error,error,274,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:311,safety,error,error,311,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:264,security,detect,detected,264,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:274,usability,error,error,274,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:311,usability,error,error,311,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:72,usability,close,close,72,"Hi @melop , let me know if you have a chance to try and follow up. I'll close this because it has been 2 weeks. Please feel free to follow up and reopen if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:15,availability,error,error,15,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:584,availability,error,error,584,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:644,availability,error,error,644,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1197,availability,error,error,1197,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1300,availability,error,error,1300,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:50,deployability,modul,modules,50,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:92,deployability,instal,installed,92,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:929,deployability,version,version,929,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1403,deployability,modul,module,1403,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:479,energy efficiency,load,load,479,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1123,energy efficiency,load,load,1123,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1348,energy efficiency,gpu,gpu,1348,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:929,integrability,version,version,929,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:564,interoperability,share,shared,564,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1418,interoperability,conflict,conflict,1418,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:50,modifiability,modul,modules,50,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:207,modifiability,pac,packages,207,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:264,modifiability,pac,packages,264,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:325,modifiability,pac,packages,325,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:388,modifiability,pac,packages,388,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:929,modifiability,version,version,929,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1057,modifiability,pac,packages,1057,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1403,modifiability,modul,module,1403,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:15,performance,error,error,15,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:479,performance,load,load,479,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:584,performance,error,error,584,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:644,performance,error,error,644,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1123,performance,load,load,1123,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1197,performance,error,error,1197,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1300,performance,error,error,1300,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1348,performance,gpu,gpu,1348,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:15,safety,error,error,15,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:50,safety,modul,modules,50,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:584,safety,error,error,584,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:644,safety,error,error,644,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1197,safety,error,error,1197,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1300,safety,error,error,1300,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1403,safety,modul,module,1403,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1564,safety,test,test,1564,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1564,testability,test,test,1564,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:15,usability,error,error,15,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:458,usability,stop,stops,458,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:584,usability,error,error,584,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:644,usability,error,error,644,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1197,usability,error,error,1197,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/761:1300,usability,error,error,1300,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though). Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/761
https://github.com/google/deepvariant/issues/762:282,modifiability,Pac,PacBio,282,"Hi @elcortegano ,. `--normalize_reads` is designed for short reads in which case the mapping wouldn't accurately left align all of the indels. For long reads, I do not believe that would be the case as most of the reads would span your INDELs accurately, so `--normalize_reads` for PacBio should not be required. In this, we are assuming you are using minimap2 as the mapper. On the other hand, if you are trying to normalize the VCF then you can do it post variant calling. Please see `bcftools norm` https://vcftools.github.io/htslib.html#norm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/762
https://github.com/google/deepvariant/issues/762:84,modifiability,scenario,scenario,84,"Thank you for the clarifications,. So, if I understand correctly, in a multi-sample scenario I could left-align the deepvariant-produced VCFs with `bcftools norm` and then use GLnexus to combine them. For merging, should the gVCFs also be left-aligned? (those are required by GLnexus I believe)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/762
https://github.com/google/deepvariant/issues/762:44,testability,understand,understand,44,"Thank you for the clarifications,. So, if I understand correctly, in a multi-sample scenario I could left-align the deepvariant-produced VCFs with `bcftools norm` and then use GLnexus to combine them. For merging, should the gVCFs also be left-aligned? (those are required by GLnexus I believe)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/762
https://github.com/google/deepvariant/issues/763:99,deployability,updat,updates,99,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:99,safety,updat,updates,99,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:99,security,updat,updates,99,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:79,usability,close,closely,79,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:118,usability,progress,progress,118,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:192,usability,help,helpful,192,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:183,interoperability,mismatch,mismatching,183,"Local realignment algorithm works as following: . * Candidate windows across the genome are selected for reassembly by looking for any evidence of possible genetic variation, such as mismatching or soft clipped bases. Window cannot exceed maximum size, usually 1000 bases. * De Bruijn graph is constructed using multiple fixed k-mer sizes (from 20 to 75, inclusive, with increments of 5) out of the reference genome bases for the candidate window, as well as all overlapping reads. * Candidate haplotypes are generated by traversing the assembly graphs and the top two most likely haplotypes are selected that best explain the read evidence. * Finally, each read is then realigned to its most likely haplotype. To answer your question. Reads can move either direction as a result of the realignment. To investigate it further it would be helpful to see what haplotype was created as a result of the realignment. Would you be able to provide the reference file that you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:838,usability,help,helpful,838,"Local realignment algorithm works as following: . * Candidate windows across the genome are selected for reassembly by looking for any evidence of possible genetic variation, such as mismatching or soft clipped bases. Window cannot exceed maximum size, usually 1000 bases. * De Bruijn graph is constructed using multiple fixed k-mer sizes (from 20 to 75, inclusive, with increments of 5) out of the reference genome bases for the candidate window, as well as all overlapping reads. * Candidate haplotypes are generated by traversing the assembly graphs and the top two most likely haplotypes are selected that best explain the read evidence. * Finally, each read is then realigned to its most likely haplotype. To answer your question. Reads can move either direction as a result of the realignment. To investigate it further it would be helpful to see what haplotype was created as a result of the realignment. Would you be able to provide the reference file that you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:93,availability,sli,slightly,93,"Thank you for these explanations. Please find below the link for the reference genome (hg19, slightly modified). https://www.dropbox.com/scl/fi/e9imci4i4rkv5vcpgujus/genome_PAR_masked.fasta.gz?rlkey=5fxpz3fbzy70glish6xwhckc9&dl=0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:93,reliability,sli,slightly,93,"Thank you for these explanations. Please find below the link for the reference genome (hg19, slightly modified). https://www.dropbox.com/scl/fi/e9imci4i4rkv5vcpgujus/genome_PAR_masked.fasta.gz?rlkey=5fxpz3fbzy70glish6xwhckc9&dl=0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:102,security,modif,modified,102,"Thank you for these explanations. Please find below the link for the reference genome (hg19, slightly modified). https://www.dropbox.com/scl/fi/e9imci4i4rkv5vcpgujus/genome_PAR_masked.fasta.gz?rlkey=5fxpz3fbzy70glish6xwhckc9&dl=0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:144,interoperability,specif,specific,144,Thank you for providing the reference. We were able to reproduce the issue locally. Meanwhile one workaround I'd like to suggest to analyze the specific position or small region where local realignment doesn't work is to run DeepVariant without the local realignment. It can be done by adding `--realign_reads=false` to `--make_examples_extra_args` flag and specifying a region with `--regions` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:358,interoperability,specif,specifying,358,Thank you for providing the reference. We were able to reproduce the issue locally. Meanwhile one workaround I'd like to suggest to analyze the specific position or small region where local realignment doesn't work is to run DeepVariant without the local realignment. It can be done by adding `--realign_reads=false` to `--make_examples_extra_args` flag and specifying a region with `--regions` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:202,reliability,doe,doesn,202,Thank you for providing the reference. We were able to reproduce the issue locally. Meanwhile one workaround I'd like to suggest to analyze the specific position or small region where local realignment doesn't work is to run DeepVariant without the local realignment. It can be done by adding `--realign_reads=false` to `--make_examples_extra_args` flag and specifying a region with `--regions` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:616,availability,replic,replication,616,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:628,availability,error,error,628,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:440,interoperability,mismatch,mismatch,440,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:594,interoperability,standard,standard,594,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:694,interoperability,specif,specifically,694,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:255,modifiability,exten,extend,255,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:349,modifiability,exten,extend,349,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:723,modifiability,exten,extended,723,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:817,modifiability,exten,extend,817,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:977,modifiability,extens,extensive,977,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:628,performance,error,error,628,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:564,safety,compl,complexity,564,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:628,safety,error,error,628,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:564,security,compl,complexity,564,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:661,testability,context,context,661,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:628,usability,error,error,628,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:1091,usability,satisfa,satisfactory,1091,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/763:105,usability,close,close,105,"Hi @Fred-07 , given that @akolesnikov and @AndrewCarroll have investigated and answered this issue, I'll close it now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/763
https://github.com/google/deepvariant/issues/764:43,safety,input,input,43,"@ColinR01 ,. Is it possible to provide the input files to reproduce this result?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:43,usability,input,input,43,"@ColinR01 ,. Is it possible to provide the input files to reproduce this result?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:64,energy efficiency,current,currently,64,I believe @kishwarshafin was able to reproduce the issue and is currently looking into a fix. Keeping this open for now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:61,deployability,patch,patch,61,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:205,energy efficiency,gpu,gpu,205,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:205,performance,gpu,gpu,205,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:61,safety,patch,patch,61,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/764:61,security,patch,patch,61,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/764
https://github.com/google/deepvariant/issues/765:1721,availability,down,downsides,1721,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1539,deployability,manag,manage,1539,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1752,deployability,manag,manage,1752,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1539,energy efficiency,manag,manage,1539,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1615,energy efficiency,model,model,1615,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1686,energy efficiency,model,model,1686,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1752,energy efficiency,manag,manage,1752,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1179,integrability,sub,substitute,1179,"nt to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1401,integrability,sub,sub-sampling,1401,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:137,interoperability,share,share,137,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:550,performance,perform,perform,550,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:575,performance,time,time,575,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1340,performance,time,time,1340," the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the qua",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1348,performance,perform,perform,1348,"study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:222,safety,input,input,222,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1270,safety,input,inputs,1270,"n WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accura",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1539,safety,manag,manage,1539,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1752,safety,manag,manage,1752,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1615,security,model,model,1615,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1686,security,model,model,1686,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:154,usability,command,commands,154,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:222,usability,input,input,222,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:291,usability,command,commands,291,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:472,usability,command,command,472,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:550,usability,perform,perform,550,"@dbrami have you seen the case study? https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1270,usability,input,inputs,1270,"n WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accura",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:1348,usability,perform,perform,1348,"study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:2019,usability,help,helpful,2019,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:149,availability,down,downsampling,149,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:261,availability,down,downsampling,261,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:247,modifiability,paramet,parameter,247,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:187,performance,time,times,187,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:199,performance,perform,perform,199,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:199,usability,perform,perform,199,"Thanks for the added code. Here are my follow-up questions:. - what is the pattern to use for file naming if Im processing multiple BAM files? - If downsampling same source BAM multiple times, do I perform loop function myself? - Is there a seed parameter for downsampling fraction? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:214,availability,down,downsampling,214,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:396,availability,down,downsampling,396,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:157,interoperability,specif,specify,157,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:382,modifiability,paramet,parameter,382,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:126,performance,perform,perform,126,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:252,performance,time,times,252,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:264,performance,perform,perform,264,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:126,usability,perform,perform,126,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:264,usability,perform,perform,264,"__what is the pattern to use for file naming if Im processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information? __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:105,availability,down,downsample,105,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:146,availability,down,downsampled,146,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:358,availability,down,downsampling,358,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:345,modifiability,paramet,parameter,345,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:197,performance,perform,perform,197,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:390,performance,time,times,390,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:436,performance,time,time,436,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:197,usability,perform,perform,197,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:456,usability,clear,clears,456,"I guess the naming pattern is related to second question. Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x. Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:32,availability,down,downsampling,32,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:136,availability,down,downsample,136,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:82,energy efficiency,current,currently,82,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:639,energy efficiency,model,model,639,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:745,energy efficiency,model,model,745,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:218,integrability,sub,subsample,218,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:358,integrability,sub,subsample,358,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:649,integrability,Sub,Subsampling,649,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:194,interoperability,specif,specify,194,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:18,modifiability,paramet,parameter,18,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:670,performance,improve perform,improve performance,670,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:466,safety,input,input,466,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:478,safety,input,input,478,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:639,security,model,model,639,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:745,security,model,model,745,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:622,testability,plan,plan,622,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:699,testability,coverag,coverage,699,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:728,testability,plan,plan,728,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:767,testability,coverag,coverage,767,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:466,usability,input,input,466,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:478,usability,input,input,478,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:665,usability,help,help,665,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:678,usability,perform,performance,678,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:. ```. -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the. fraction of templates/read pairs to keep; INT part sets seed). ```. So you can subsample each bam like this:. ```bash. for i in `seq 1 5`; do. # i sets the seed. samtools view -s ${i}.20 input.bam > input.${i}.20.bam. ```. Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/765:19,usability,close,close,19,"Hi @dbrami ,. I'll close this issue now. Feel free to follow up if you have more questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/765
https://github.com/google/deepvariant/issues/766:84,deployability,resourc,resource-constrained,84,@bcantarel what type of machine are you running this on? I wonder if DeepVariant is resource-constrained. What do you get when you type `nproc` ? Can you try to lower the value of `--num_shards` to this value if it is lower?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:84,energy efficiency,resourc,resource-constrained,84,@bcantarel what type of machine are you running this on? I wonder if DeepVariant is resource-constrained. What do you get when you type `nproc` ? Can you try to lower the value of `--num_shards` to this value if it is lower?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:84,performance,resourc,resource-constrained,84,@bcantarel what type of machine are you running this on? I wonder if DeepVariant is resource-constrained. What do you get when you type `nproc` ? Can you try to lower the value of `--num_shards` to this value if it is lower?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:84,safety,resourc,resource-constrained,84,@bcantarel what type of machine are you running this on? I wonder if DeepVariant is resource-constrained. What do you get when you type `nproc` ? Can you try to lower the value of `--num_shards` to this value if it is lower?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:84,testability,resourc,resource-constrained,84,@bcantarel what type of machine are you running this on? I wonder if DeepVariant is resource-constrained. What do you get when you type `nproc` ? Can you try to lower the value of `--num_shards` to this value if it is lower?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:73,energy efficiency,cpu,cpus,73,"I tried on both an e2-highmem-16 and nd2-highmem-96, so that's 16 and 96 cpus, I saw this in both instances. when running deep-variant I set num_shards to $(nproc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:73,performance,cpu,cpus,73,"I tried on both an e2-highmem-16 and nd2-highmem-96, so that's 16 and 96 cpus, I saw this in both instances. when running deep-variant I set num_shards to $(nproc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:236,deployability,releas,released,236,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:270,deployability,continu,continue,270,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:206,energy efficiency,model,model,206,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:257,energy efficiency,model,model,257,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:156,safety,test,tested,156,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:206,security,model,model,206,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:257,security,model,model,257,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:156,testability,test,tested,156,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:77,availability,sli,slim,77,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:83,deployability,Version,Version,83,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:49,energy efficiency,model,model,49,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:119,energy efficiency,model,models,119,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:83,integrability,Version,Version,83,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:83,modifiability,Version,Version,83,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:77,reliability,sli,slim,77,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:49,security,model,model,49,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:119,security,model,models,119,"@bcantarel ,. Please use ""1.5.0"" for the RNA-seq model as it is trained with slim. Version 1.6.0 only works with Keras models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:40,deployability,version,version,40,"Hi @bcantarel ,. I hope using the right version resolved your issue. Closing this now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:40,integrability,version,version,40,"Hi @bcantarel ,. I hope using the right version resolved your issue. Closing this now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/766:40,modifiability,version,version,40,"Hi @bcantarel ,. I hope using the right version resolved your issue. Closing this now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/766
https://github.com/google/deepvariant/issues/767:141,interoperability,specif,specified,141,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:89,reliability,doe,does,89,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:31,safety,input,input,31,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:122,safety,input,input,122,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:190,safety,input,input,190,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:109,security,ident,identify,109,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:31,usability,input,input,31,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:122,usability,input,input,122,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:190,usability,input,input,190,"If `sra.fq.paths` is a list of input files, it should be entered as `@sra.fq.paths`. KMC does not attempt to identify the input type. If not specified otherwise, it seems to assume that the input is (plain or gzipped) fastq.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/767:58,performance,time,time,58,"You need to prefix your paths file with `@`. So try. ```. time ./kmc -k29 -okff -t8 @sra.fq.paths ./sra.fq $TMPDIR. ```. (edit: oops, sorry for repeated post -- somehow didn't notice reply above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/767
https://github.com/google/deepvariant/issues/768:119,deployability,contain,container,119,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:207,deployability,contain,containers,207,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:57,energy efficiency,cpu,cpus,57,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:101,energy efficiency,CPU,CPU,101,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:240,energy efficiency,cpu,cpu,240,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:57,performance,cpu,cpus,57,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:101,performance,CPU,CPU,101,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:240,performance,cpu,cpu,240,"@Luosanmu , . Can you please add:. ```bash. docker run --cpus=$num_shards. ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/768:18,energy efficiency,cool,cool,18,"@kishwarshafin ,. cool It is useful work !!! thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/768
https://github.com/google/deepvariant/issues/769:103,deployability,patch,patch,103,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:125,deployability,releas,release,125,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:103,safety,patch,patch,103,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:103,security,patch,patch,103,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:117,testability,plan,plan,117,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:65,deployability,patch,patch,65,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:209,energy efficiency,gpu,gpu,209,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:209,performance,gpu,gpu,209,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:65,safety,patch,patch,65,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/769:65,security,patch,patch,65,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/769
https://github.com/google/deepvariant/issues/770:46,usability,close,closed,46,Notethis issue's code is the same as #768 in closed issue,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:137,deployability,observ,observe,137,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:24,integrability,filter,filtering,24,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:183,integrability,filter,filters,183,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
