id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/249:75,safety,updat,update,75,"Hi @se2cheeese . I will close this issue for now because there's no reason update from you. But please feel free to follow up with more questions, either reopening this issue, or another one. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/249:75,security,updat,update,75,"Hi @se2cheeese . I will close this issue for now because there's no reason update from you. But please feel free to follow up with more questions, either reopening this issue, or another one. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/249:24,usability,close,close,24,"Hi @se2cheeese . I will close this issue for now because there's no reason update from you. But please feel free to follow up with more questions, either reopening this issue, or another one. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/250:90,safety,input,input,90,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:141,safety,input,input,141,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:90,usability,input,input,90,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:141,usability,input,input,141,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:323,usability,command,commands,323,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:503,usability,command,command,503,"Hi @PlatonB . The stderr output of DeepVariant for the make_examples line has: . ""--ref ""/input/SRR062634.filt.fastq"". This is providing the input reads in position of the reference genome. Instead, you will need to provide the bgzipped, indexed file for Homo_sapiens.GRCh38.dna.primary_assembly.fa. You can do so with the commands:. bgzip -i Homo_sapiens.GRCh38.dna.primary_assembly.fa && samtools faidx Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz. and then replace the ref location to match in the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/251:326,deployability,scale,scale,326,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:76,energy efficiency,reduc,reduced,76,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:250,energy efficiency,model,model,250,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:326,energy efficiency,scale,scale,326,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:326,modifiability,scal,scale,326,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:326,performance,scale,scale,326,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:250,security,model,model,250,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:314,testability,Plan,Planning,314,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:84,usability,learn,learning,84,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:307,usability,learn,learn,307,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:89,energy efficiency,current,currently,89,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:243,performance,perform,perform,243,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:216,usability,clear,clear,216,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:243,usability,perform,perform,243,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:257,usability,close,close,257,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:131,usability,help,help,131,"I'm not sure if you are still bothered by this issue, but I have posted a similar one before and got perfectly solved. Hope it can help you, too! [https://github.com/google/deepvariant/issues/185](url)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/252:16,availability,error,error,16,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:79,energy efficiency,Current,Currently,79,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:22,integrability,messag,message,22,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:22,interoperability,messag,message,22,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:152,modifiability,maintain,maintains,152,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:16,performance,error,error,16,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:16,safety,error,error,16,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:152,safety,maintain,maintains,152,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:16,usability,error,error,16,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:112,usability,support,supports,112,"Looking at this error message, I wonder if it's related to Python3 vs Python2. Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:150,deployability,instal,install,150,"Thanks for the question. I'm agreed with Pi-Chuan, trying in a python 2.7 conda environment should hopefully resolve these issues. Hope that gets the install figured out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:79,deployability,instal,installed,79,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2972,deployability,modul,modules,2972,1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0 conda-forge. libssh2: 1.8.2-h22169c7_2 conda-forge. libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge. linecache2: 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4257,deployability,instal,installed,4257," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4299,deployability,instal,installed,4299," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4358,deployability,instal,install,4358," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:310,energy efficiency,gpu,gpu,310,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1230,energy efficiency,cloud,cloud-sdk,1230,ill be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:3786,energy efficiency,estimat,estimator,3786," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:70,modifiability,pac,packages,70,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:127,modifiability,Pac,Package,127,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:144,modifiability,pac,package,144,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:225,modifiability,pac,packages,225,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2972,modifiability,modul,modules,2972,1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0 conda-forge. libssh2: 1.8.2-h22169c7_2 conda-forge. libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge. linecache2: 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4271,modifiability,pac,package,4271," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4325,modifiability,pac,package,4325," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:310,performance,gpu,gpu,310,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2641,performance,parallel,parallel,2641,_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0 conda-forge. libssh2: 1.8.2-h22169c7_2 conda-forge. libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge. linecache2: 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4284,performance,time,time,4284," mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2972,safety,modul,modules,2972,1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0 conda-forge. libssh2: 1.8.2-h22169c7_2 conda-forge. libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge. linecache2: 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:633,security,certif,certificates,633,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:682,security,certif,certifi,682,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:939,security,cryptograph,cryptography,939,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:3341,security,rsa,rsa,3341," 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-forge. setuptools: 42.0.2-py27_0 conda-forge. six: 1.13.0-py27_0 conda-forge. sortedcontainers: 2.1.0-py_0 conda-forge. sqlite: 3.30.1-hcee41ef_0 conda-forge. stevedore: 1.30.1-py_0 conda-forge. subprocess32: 3.5.4-py27h516909a_0 conda-forge. tensorboard: 1.12.0-py27_1000 conda-forge. tensorflow: 1.12.0-gpu_py27h2a0f108_0 . tensorflow-base: 1.12.0-gpu_py27had579c0_0 . tensorflow-estimator: 1.13.0-py_0 . termcolor: 1.1.0-py_2 conda-forge. tk: 8.6.10-hed695b0_0 conda-forge. traceback2: 1.4.0-py27_0 conda-forge. unicodecsv: 0.14.1-py_1 conda-forge. unittest2: 1.1.0-py_0 conda-forge. urllib3: 1.25.7-py27_0 conda-forge. wcwidth: 0.1.7-py_1 conda-forge. werkzeug: 0.16.0-py_0 conda-forge. wheel: 0.33.6-py27_0 conda-forge. xz: 5.2.4-h14c3975_1001 conda-forge. yaml: 0.2.2-h516909a_1 conda-forge. zlib: 1.2.11-h516909a_1006 conda-forge. ```. I've also installed one package at a time, and then installed the deepvariant package with the c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:135,testability,plan,plan,135,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:. ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge. _tflow_select: 2.1.0-gpu . absl-py: 0.8.1-py27_0 conda-forge. astor: 0.7.1-py_0 conda-forge. backports: 1.0-py_2 conda-forge. backports.weakref: 1.0.post1-py27_1000 conda-forge. boost: 1.70.0-py27h9de70de_1 conda-forge. boost-cpp: 1.70.0-h8e57a91_2 conda-forge. bzip2: 1.0.8-h516909a_2 conda-forge. c-ares: 1.15.0-h516909a_1001 conda-forge. ca-certificates: 2019.11.28-hecc5488_0 conda-forge. certifi: 2019.11.28-py27_0 conda-forge. cffi: 1.13.2-py27h8022711_0 conda-forge. chardet: 3.0.4-py27_1003 conda-forge. cliff: 2.15.0-py27_0 conda-forge. cmd2: 0.8.6-py27_0 conda-forge. contextlib2: 0.6.0-py_0 conda-forge. crcmod: 1.7-py27_1002 conda-forge. cryptography: 2.8-py27h72c5cf5_1 conda-forge. cudatoolkit: 9.2-0 . cudnn: 7.6.4-cuda9.2_0 . cupti: 9.2.148-0 . curl: 7.65.3-hf8cf82a_0 conda-forge. enum34: 1.1.6-py27_1002 conda-forge. funcsigs: 1.0.2-py_3 conda-forge. futures: 3.3.0-py27_0 conda-forge. gast: 0.3.2-py_0 conda-forge. google-cloud-sdk: 166.0.0-py27_0 bioconda . grpcio: 1.23.0-py27he9ae1f9_0 conda-forge. h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge. hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge. htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2402,testability,mock,mock,2402, htslib: 1.9-h244ad75_9 bioconda . httplib2: 0.14.0-py27_0 conda-forge. icu: 64.2-he1b5a44_1 conda-forge. idna: 2.8-py27_1000 conda-forge. intervaltree: 3.0.2-py_0 conda-forge. ipaddress: 1.0.23-py_0 conda-forge. keras-applications: 1.0.8-py_1 conda-forge. keras-preprocessing: 1.1.0-py_0 conda-forge. krb5: 1.16.4-h2fd8d38_0 conda-forge. libblas: 3.8.0-14_openblas conda-forge. libcblas: 3.8.0-14_openblas conda-forge. libcurl: 7.65.3-hda55be3_0 conda-forge. libdeflate: 1.3-h516909a_0 conda-forge. libedit: 3.1.20170329-hf8c457e_1001 conda-forge. libffi: 3.2.1-he1b5a44_1006 conda-forge. libgcc-ng: 9.2.0-hdf63c60_0 conda-forge. libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge. liblapack: 3.8.0-14_openblas conda-forge. libopenblas: 0.3.7-h5ec1e0e_5 conda-forge. libpng: 1.6.37-hed695b0_0 conda-forge. libprotobuf: 3.11.1-h8b12597_0 conda-forge. libssh2: 1.8.2-h22169c7_2 conda-forge. libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge. linecache2: 1.0.0-py_1 conda-forge. markdown: 3.1.1-py_0 conda-forge. mock: 3.0.5-py27_0 conda-forge. ncurses: 6.1-hf484d3e_1002 conda-forge. numpy: 1.14.6-py27h95a1406_1201 conda-forge. oauth2client: 1.5.2-py27_0 bioconda . openjdk: 8.0.192-h14c3975_1003 conda-forge. openssl: 1.1.1d-h516909a_0 conda-forge. parallel: 20160622-1 bioconda . pbr: 5.4.2-py_0 conda-forge. perl: 5.26.2-h516909a_1006 conda-forge. perl-threaded: 5.26.0-0 bioconda . pip: 19.3.1-py27_0 conda-forge. prettytable: 0.7.2-py_3 conda-forge. protobuf: 3.11.1-py27he1b5a44_0 conda-forge. psutil: 5.6.7-py27h516909a_0 conda-forge. pyasn1: 0.4.8-py_0 conda-forge. pyasn1-modules: 0.2.7-py_0 conda-forge. pycparser: 2.19-py27_1 conda-forge. pyopenssl: 19.1.0-py27_0 conda-forge. pyparsing: 2.4.5-py_0 conda-forge. pyperclip: 1.7.0-py_0 conda-forge. pysocks: 1.7.0-py27_0 conda-forge. python: 2.7.15-h5a48372_1009 conda-forge. pyyaml: 5.2-py27h516909a_0 conda-forge. readline: 8.0-hf8c457e_0 conda-forge. requests: 2.22.0-py27_1 conda-forge. rsa: 3.1.4-py27_0 bioconda . scipy: 1.2.1-py27h921218d_2 conda-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:119,availability,error,error,119,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:596,availability,error,errors,596,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:646,availability,down,downloading,646,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:151,deployability,instal,installed,151,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:314,deployability,instal,installing,314,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:486,energy efficiency,cloud,cloud-sdk,486,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:670,energy efficiency,model,model,670,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:125,integrability,messag,messages,125,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:125,interoperability,messag,messages,125,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:380,interoperability,conflict,conflicts,380,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:119,performance,error,error,119,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:596,performance,error,errors,596,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:119,safety,error,error,119,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:370,safety,avoid,avoid,370,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:596,safety,error,errors,596,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:670,security,model,model,670,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:119,usability,error,error,119,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:263,usability,indicat,indicative,263,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:596,usability,error,errors,596,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:894,usability,help,helps,894,"Andrea;. Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:. ```. conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'. ```. If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:330,availability,error,error,330,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:259,deployability,instal,installed,259,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:631,deployability,modul,module,631,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:776,deployability,modul,module,776,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:984,deployability,modul,module,984,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1170,deployability,modul,module,1170,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1345,deployability,modul,module,1345,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1429,deployability,version,version,1429,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1588,deployability,instal,install,1588,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1599,deployability,updat,updated,1599,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1607,deployability,version,version,1607,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1647,deployability,instal,install,1647,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:221,energy efficiency,cloud,cloud-sdk,221,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1009,energy efficiency,core,core,1009,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1128,energy efficiency,core,core,1128,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,integrability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1429,integrability,version,version,1429,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1607,integrability,version,version,1607,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:389,interoperability,share,share,389,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,modifiability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:631,modifiability,modul,module,631,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:729,modifiability,pac,packages,729,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:776,modifiability,modul,module,776,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:930,modifiability,pac,packages,930,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:984,modifiability,modul,module,984,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1108,modifiability,pac,packages,1108,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1170,modifiability,modul,module,1170,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1345,modifiability,modul,module,1345,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1429,modifiability,version,version,1429,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1607,modifiability,version,version,1607,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:330,performance,error,error,330,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:330,safety,error,error,330,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:631,safety,modul,module,631,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:776,safety,modul,module,776,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:984,safety,modul,module,984,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1170,safety,modul,module,1170,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1345,safety,modul,module,1345,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1599,safety,updat,updated,1599,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1599,security,updat,updated,1599,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,testability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:486,testability,Trace,Traceback,486,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:74,usability,command,command,74,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:330,usability,error,error,330,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:480,usability,help,help,480,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1728,usability,support,support,1728,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. However, when I try to run it I get the following error:. ```. python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>. import tensorflow as tf. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>. from tensorflow.core.framework.graph_pb2 import *. File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>. from google.protobuf import descriptor as _descriptor. File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>. from google.protobuf.pyext import _message. ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so). ```. I've tried to install an updated version of the library using . ```conda install librosa```. but it didn't work. Any suggestion? Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:259,deployability,instal,installed,259,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:319,deployability,instal,installed,319,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:221,energy efficiency,cloud,cloud-sdk,221,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,integrability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,modifiability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:49,testability,coupl,couple,49,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:74,usability,command,command,74,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:291,usability,guid,guide,291,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:412,usability,support,support,412,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. ```. conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. ```. Everything is installed correctly. Is there a guide to follow for locally installed variant caller? I'm not sure I've been able to find it. . Thank you again for your support,. Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:33,availability,error,error,33,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:103,deployability,instal,installation,103,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:426,deployability,instal,installed,426,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:486,deployability,instal,installed,486,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:380,energy efficiency,cloud,cloud-sdk,380,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:200,integrability,coupl,couple,200,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:200,modifiability,coupl,couple,200,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:33,performance,error,error,33,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:33,safety,error,error,33,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:200,testability,coupl,couple,200,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:33,usability,error,error,33,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:225,usability,command,command,225,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:458,usability,guid,guide,458,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:585,usability,support,support,585,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it? > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:. > . > ```. > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0. > ```. > . > Everything is installed correctly. Is there a guide to follow for locally installed variant caller? > I'm not sure I've been able to find it. > . > Thank you again for your support,. > Andrea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1086,availability,error,error,1086,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:19,deployability,instal,installing,19,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:65,energy efficiency,model,models,65,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:258,energy efficiency,model,models,258,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:222,interoperability,share,share,222,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:623,interoperability,share,share,623,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:792,interoperability,share,share,792,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1076,interoperability,share,share,1076,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1086,performance,error,error,1086,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1086,safety,error,error,1086,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:65,security,model,models,65,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:258,security,model,models,258,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:905,usability,command,command,905,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1086,usability,error,error,1086,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```. $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. binaries models . ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```. $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1129,availability,error,error,1129,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1178,availability,error,error,1178,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:21,deployability,instal,installing,21,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1276,deployability,version,version,1276,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:67,energy efficiency,model,models,67,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:270,energy efficiency,model,models,270,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1276,integrability,version,version,1276,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:232,interoperability,share,share,232,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:651,interoperability,share,share,651,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:830,interoperability,share,share,830,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1119,interoperability,share,share,1119,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1276,modifiability,version,version,1276,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1129,performance,error,error,1129,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1178,performance,error,error,1178,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1129,safety,error,error,1129,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1178,safety,error,error,1178,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:67,security,model,models,67,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:270,security,model,models,270,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:942,usability,command,command,942,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1129,usability,error,error,1129,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1178,usability,error,error,1178,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. > . > ```. > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0. > binaries models . > ```. > . > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. > . > ```. > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D. > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>. > ```. > . > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions! > . > @prabal97 could you share the error you are seeing? I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:12,availability,error,error,12,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:60,availability,operat,operating,60,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:337,availability,operat,operating,337,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:469,energy efficiency,cloud,cloud,469,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:12,performance,error,error,12,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:94,reliability,doe,doesn,94,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:12,safety,error,error,12,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:389,security,access,access,389,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:12,usability,error,error,12,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:18,usability,indicat,indicates,18,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:102,usability,support,support,102,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:520,usability,help,helps,520,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:54,availability,cluster,cluster,54,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:54,deployability,cluster,cluster,54,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:118,deployability,updat,updates,118,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:197,energy efficiency,cloud,cloud,197,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:118,safety,updat,updates,118,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:118,security,updat,updates,118,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:238,usability,support,support,238,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2,energy efficiency,Current,Currently,2,> Currently DeepVariant still only supports Python2. Is there a plan to migrate to Python 3?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:64,testability,plan,plan,64,> Currently DeepVariant still only supports Python2. Is there a plan to migrate to Python 3?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:35,usability,support,supports,35,> Currently DeepVariant still only supports Python2. Is there a plan to migrate to Python 3?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:92,deployability,releas,release,92,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:121,deployability,updat,update,121,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:121,safety,updat,update,121,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:121,security,updat,update,121,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/253:238,availability,operat,operate,238,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:295,interoperability,specif,specifed,295,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:57,performance,content,contents,57,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:192,performance,content,contents,192,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:122,reliability,doe,does,122,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:14,testability,understand,understand,14,"@PlatonB if I understand correctly, you want to pipe the contents of the final VCF to another program? If so, DeepVariant does not write the VCF to stdout, so you cannot directly pipe the VCF contents to another program. However, you can operate on the final VCF, which gets written to the path specifed by `--output_vcf`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:50,performance,content,contents,50,"> if I understand correctly, you want to pipe the contents of the final VCF to another program? Yes. > DeepVariant does not write the VCF to stdout. Then let it be like a feature request.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:115,reliability,doe,does,115,"> if I understand correctly, you want to pipe the contents of the final VCF to another program? Yes. > DeepVariant does not write the VCF to stdout. Then let it be like a feature request.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:7,testability,understand,understand,7,"> if I understand correctly, you want to pipe the contents of the final VCF to another program? Yes. > DeepVariant does not write the VCF to stdout. Then let it be like a feature request.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:307,availability,down,downstream,307,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,availability,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:151,deployability,releas,release,151,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:353,deployability,log,logging,353,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,deployability,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:160,energy efficiency,Current,Currently,160,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,energy efficiency,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,reliability,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:353,safety,log,logging,353,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,safety,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:353,security,log,logging,353,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:353,testability,log,logging,353,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:382,testability,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:269,usability,user,users,269,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:372,usability,user,users,372,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:394,usability,statu,status,394,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/254:106,deployability,log,logging,106,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:959,deployability,contain,container,959,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:1046,deployability,contain,container,1046,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:24,integrability,messag,messages,24,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:24,interoperability,messag,messages,24,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:106,safety,log,logging,106,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:439,safety,input,input,439,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:888,safety,input,input,888,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:931,safety,input,input,931,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:1127,safety,input,input,1127,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:106,security,log,logging,106,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:106,testability,log,logging,106,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:377,usability,command,command,377,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:439,usability,input,input,439,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:888,usability,input,input,888,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:931,usability,input,input,931,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:1021,usability,command,command,1021,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:1095,usability,command,command,1095,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:1127,usability,input,input,1127,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```. I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. ```. I do notice a possible issue with your Docker command, pasted below. ```. sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: . `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:194,safety,input,input,194,"Hi gunjanbaid. many thanks, I have never used docker before. You where correct the problem was with volume mounts. to debug this issue I did the following. ```. docker run -it -v /data/aligned:/input google/deepvariant:0.9.0. ```. I was then able to use ls to see exactly what the file paths need to be. thanks. Happy Holidays. Andy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/254:194,usability,input,input,194,"Hi gunjanbaid. many thanks, I have never used docker before. You where correct the problem was with volume mounts. to debug this issue I did the following. ```. docker run -it -v /data/aligned:/input google/deepvariant:0.9.0. ```. I was then able to use ls to see exactly what the file paths need to be. thanks. Happy Holidays. Andy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/255:365,availability,operat,operation,365,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:39,deployability,version,version,39,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:174,deployability,version,version,174,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:322,deployability,version,version,322,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:39,integrability,version,version,39,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:174,integrability,version,version,174,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:322,integrability,version,version,322,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:39,modifiability,version,version,39,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:67,modifiability,pac,package,67,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:174,modifiability,version,version,174,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:322,modifiability,version,version,322,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:199,usability,confirm,confirm,199,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:446,usability,help,help,446,"Hi,. this seems like an issue with the version of intervaltree pip package. But this is surprising to me, because in our set up script, we pinned to the correct intervaltree version. Can you clarify/confirm a few things:. (1) Given that you mentioned you're going through the Quick Start, I assume you're using our Docker version, and you're using v0.9.0? (2) What operation system are you on? We'd appreciate more information you can provide to help us reproduce the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:249,availability,cluster,cluster,249,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:249,deployability,cluster,cluster,249,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:329,deployability,version,version,329,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:448,deployability,version,version,448,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1155,deployability,modul,module,1155,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1162,energy efficiency,load,load,1162,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1185,energy efficiency,gpu,gpu-phoenix,1185,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:159,integrability,wrap,wrapper,159,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:212,integrability,sub,submit,212,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:329,integrability,version,version,329,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:448,integrability,version,version,448,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:493,integrability,sub,submit,493,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:159,interoperability,wrapper,wrapper,159,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:329,modifiability,version,version,329,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:448,modifiability,version,version,448,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1155,modifiability,modul,module,1155,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:985,performance,perform,performance-testdata,985,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1057,performance,perform,performance-testdata,1057,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1162,performance,load,load,1162,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1185,performance,gpu,gpu-phoenix,1185,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:633,safety,input,input,633,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:997,safety,test,testdata,997,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1069,safety,test,testdata,1069,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1155,safety,modul,module,1155,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1239,safety,input,input,1239,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1278,safety,input,input,1278,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:997,testability,test,testdata,997,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1069,testability,test,testdata,1069,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:68,usability,command,command,68,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:525,usability,command,commands,525,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:633,usability,input,input,633,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:985,usability,perform,performance-testdata,985,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1057,usability,perform,performance-testdata,1057,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1239,usability,input,input,1239,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1278,usability,input,input,1278,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:. . intervaltree==2.1.0. This is the submit script without the SLURM commands:. ```. export BIN_VERSION=""0.9.0"". export BASE=""${PWD}/deepvariant-run"". export INPUT_DIR=""${BASE}/input"". export REF=""hs37d5.fa.gz"". export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam"". export OUTPUT_DIR=""${BASE}/output"". export DATA_DIR=""${INPUT_DIR}/data"". export OUTPUT_VCF=""HG002.output.vcf.gz"". export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/. module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \. --reads=""${DATA}/input/data/${BAM}"" \. 	 --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:30,deployability,instal,installing,30,Hi @coneheadusa . Can you try installing intervaltree==3.0.2? This is what we required in the run-prereq.sh script:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/run-prereq.sh#L89,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:71,deployability,version,version,71,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:200,deployability,contain,container,200,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:71,integrability,version,version,71,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:259,interoperability,conflict,conflict,259,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:71,modifiability,version,version,71,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:132,modifiability,pac,packages,132,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:170,modifiability,pac,packages,170,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:215,safety,test,test,215,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:215,testability,test,test,215,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/256:131,deployability,stage,stages,131,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:478,deployability,version,versions,478,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:538,deployability,log,logic,538,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:762,deployability,log,logic,762,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:937,deployability,releas,release,937,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1113,deployability,releas,released,1113,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1173,deployability,releas,release,1173,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1237,deployability,releas,released,1237,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:307,energy efficiency,model,model,307,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1122,energy efficiency,model,model,1122,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:478,integrability,version,versions,478,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:478,modifiability,version,versions,478,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:945,performance,time,time,945,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:538,safety,log,logic,538,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:716,safety,input,input,716,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:762,safety,log,logic,762,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:156,security,ident,identifies,156,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:307,security,model,model,307,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:316,security,ident,identify,316,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:538,security,log,logic,538,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:762,security,log,logic,762,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1122,security,model,model,1122,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:225,testability,simpl,simple,225,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:538,testability,log,logic,538,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:762,testability,log,logic,762,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:225,usability,simpl,simple,225,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:716,usability,input,input,716,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:1149,usability,document,document,1149,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:306,energy efficiency,optim,optimizing,306,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:329,energy efficiency,model,models,329,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:306,performance,optimiz,optimizing,306,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:398,safety,detect,detecting,398,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:329,security,model,models,329,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/256:398,security,detect,detecting,398,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting! I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it. It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/257:122,deployability,observ,observed,122,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:140,energy efficiency,model,models,140,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:194,energy efficiency,optim,optimal,194,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:306,energy efficiency,model,model,306,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:458,energy efficiency,model,models,458,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:297,interoperability,specif,specific,297,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:437,interoperability,specif,specific-deepvariant-models,437,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:312,performance,improved perform,improved performance,312,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:140,security,model,models,140,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:306,security,model,model,306,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:458,security,model,models,458,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:122,testability,observ,observed,122,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:321,usability,perform,performance,321,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:352,deployability,observ,observed,352,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:478,energy efficiency,measur,measured,478,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:352,testability,observ,observed,352,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:25,usability,indicat,indicates,25,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:132,usability,close,close,132,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:440,usability,close,close,440,"Hi @aderzelle . As Maria indicates, generally, applying DeepVariant to this organism should work. I think the main question is: How close is there reference genome to the organisms you are sequencing. If the reference genome is highly diverged relative to your population, reference incompleteness will make analysis difficult, and the nearby variants observed in the mosquito blog may also be an issue. However, if the reference genome is close to the population (which can be measured by the variant density X bps/SNP), then I think that applying DeepVariant to this genome should work well. I was always a fan of rotifers, how interesting to study them!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:772,integrability,pub,publicly,772,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:708,interoperability,specif,specifics,708,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:919,testability,simpl,simply,919,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:67,usability,close,close,67,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:170,usability,feedback,feedback,170,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:738,usability,prefer,prefer,738,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:919,usability,simpl,simply,919,"Thank you! . My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback. I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:533,deployability,releas,release,533,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:998,deployability,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:998,integrability,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:998,modifiability,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:902,safety,avoid,avoiding,902,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:998,safety,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:998,testability,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:459,usability,visual,visual,459,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:704,usability,visual,visualization,704,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:742,usability,support,support,742,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:848,usability,close,close,848,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:485,performance,time,time,485,"Ok thank you for the clarification. Oh yes, I noticed the new html report, I used to make those plot with a custom script, I think it's a great addition to deepvariant output to have them out of the box. Just an idea: I think some users might like it if you generated the data table used for each of the plot (I guess you must have them produced at some point anyway). That could make comparison between different runs, callers, etc ... easier. At least in our team, we spent a lot of time making those kind of plots for different runs/callers/conditions and cross-comparing. Having the raw numbers is then easier (okay it's not really a big deal to extract them from the vcf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:461,security,team,team,461,"Ok thank you for the clarification. Oh yes, I noticed the new html report, I used to make those plot with a custom script, I think it's a great addition to deepvariant output to have them out of the box. Just an idea: I think some users might like it if you generated the data table used for each of the plot (I guess you must have them produced at some point anyway). That could make comparison between different runs, callers, etc ... easier. At least in our team, we spent a lot of time making those kind of plots for different runs/callers/conditions and cross-comparing. Having the raw numbers is then easier (okay it's not really a big deal to extract them from the vcf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:108,usability,custom,custom,108,"Ok thank you for the clarification. Oh yes, I noticed the new html report, I used to make those plot with a custom script, I think it's a great addition to deepvariant output to have them out of the box. Just an idea: I think some users might like it if you generated the data table used for each of the plot (I guess you must have them produced at some point anyway). That could make comparison between different runs, callers, etc ... easier. At least in our team, we spent a lot of time making those kind of plots for different runs/callers/conditions and cross-comparing. Having the raw numbers is then easier (okay it's not really a big deal to extract them from the vcf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:231,usability,user,users,231,"Ok thank you for the clarification. Oh yes, I noticed the new html report, I used to make those plot with a custom script, I think it's a great addition to deepvariant output to have them out of the box. Just an idea: I think some users might like it if you generated the data table used for each of the plot (I guess you must have them produced at some point anyway). That could make comparison between different runs, callers, etc ... easier. At least in our team, we spent a lot of time making those kind of plots for different runs/callers/conditions and cross-comparing. Having the raw numbers is then easier (okay it's not really a big deal to extract them from the vcf).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:628,safety,test,tested,628,"Great, glad you found the report useful! If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:628,testability,test,tested,628,"Great, glad you found the report useful! If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:74,usability,visual,visual,74,"Great, glad you found the report useful! If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:432,usability,visual,visual,432,"Great, glad you found the report useful! If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:639,usability,visual,visual,639,"Great, glad you found the report useful! If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1318,availability,error,errors,1318,"ad counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1336,availability,error,errors,1336,"generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1891,availability,replic,replicate,1891,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:546,deployability,releas,release,546,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1017,deployability,depend,depending,1017," I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1017,integrability,depend,depending,1017," I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:2429,interoperability,share,shared,2429,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1017,modifiability,depend,depending,1017," I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1318,performance,error,errors,1318,"ad counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1336,performance,error,errors,1336,"generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:2213,performance,time,time,2213,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:2466,reliability,Doe,Does,2466,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:921,safety,avoid,avoiding,921,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1017,safety,depend,depending,1017," I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1318,safety,error,errors,1318,"ad counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1336,safety,error,errors,1336,"generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:2106,safety,except,except,2106,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1017,testability,depend,depending,1017," I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:472,usability,visual,visual,472,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:723,usability,visual,visualization,723,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:761,usability,support,support,761,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:867,usability,close,close,867,"> Hi @aderzelle. > . > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1318,usability,error,errors,1318,"ad counts instead generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1336,usability,error,errors,1336,"generated the reference call because a candidate was not made at that position. > . > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1683,usability,indicat,indicates,1683,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:1747,usability,user,user-images,1747,"eepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). > . > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? . Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). Therefore, I was thinking of comparing the ancestral and daughter populations, and regard each new variant in the daughter as artefacts and/or the shared variants are the ""truth set"". Does that sound reasonable as a way to retrain the CNN? Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:105,testability,plan,plan,105,"May I gently bump up the issue? I have never trained a CNN before and I would like an opinion, if what I plan make sense. thank you =)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:145,energy efficiency,model,model,145,Hi @aderzelle . I followed up to this similar question in the GitHub issue #266 thread. There we are suggesting to use an experimental non-human model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/257:145,security,model,model,145,Hi @aderzelle . I followed up to this similar question in the GitHub issue #266 thread. There we are suggesting to use an experimental non-human model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257
https://github.com/google/deepvariant/issues/258:33,deployability,releas,release,33,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:104,deployability,releas,release,104,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:158,deployability,updat,updates,158,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:298,deployability,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:25,energy efficiency,current,current,25,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:283,energy efficiency,current,current,283,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:298,integrability,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:240,interoperability,distribut,distributed,240,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:298,modifiability,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:56,reliability,doe,does,56,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:158,safety,updat,updates,158,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:298,safety,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:158,security,updat,updates,158,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:298,testability,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:56,deployability,releas,release,56,Hi @AndrewCarroll . Is there an estimated date for next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:32,energy efficiency,estimat,estimated,32,Hi @AndrewCarroll . Is there an estimated date for next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:32,deployability,releas,release,32,Estimated timeline for the next release is Q2 2020.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:0,energy efficiency,Estimat,Estimated,0,Estimated timeline for the next release is Q2 2020.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:10,performance,time,timeline,10,Estimated timeline for the next release is Q2 2020.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:30,deployability,updat,update,30,"Hi @llllaaaa , to give you an update:. We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:71,deployability,releas,release,71,"Hi @llllaaaa , to give you an update:. We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:30,safety,updat,update,30,"Hi @llllaaaa , to give you an update:. We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/258:30,security,updat,update,30,"Hi @llllaaaa , to give you an update:. We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258
https://github.com/google/deepvariant/issues/259:31,deployability,updat,update,31,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:58,deployability,version,version,58,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:79,deployability,releas,release,79,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:58,integrability,version,version,58,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:58,modifiability,version,version,58,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:31,safety,updat,update,31,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:31,security,updat,update,31,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/259:19,testability,plan,planning,19,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/260:746,deployability,resourc,resources,746,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1184,deployability,log,logs,1184,"in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1260,deployability,log,logs,1260,"depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1588,deployability,log,log,1588," the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask quest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1704,deployability,log,logs,1704,"of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1786,deployability,log,log,1786,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2383,deployability,observ,observe,2383,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:28,energy efficiency,estimat,estimate,28,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:287,energy efficiency,Current,Currently,287,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:423,energy efficiency,estimat,estimation,423,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:746,energy efficiency,resourc,resources,746,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:804,energy efficiency,cloud,cloud,804,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:908,energy efficiency,estimat,estimates,908,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1127,energy efficiency,Current,Currently,1127,"ch as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1496,energy efficiency,CPU,CPU,1496,"Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpecte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2071,energy efficiency,cpu,cpu,2071,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:176,integrability,compon,component,176,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1445,integrability,coupl,couple,1445,"n you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1592,integrability,messag,message,1592,"AM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1617,integrability,Event,Eventually,1617," If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2645,integrability,pub,public,2645,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:176,interoperability,compon,component,176,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:641,interoperability,share,share,641,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1592,interoperability,messag,message,1592,"AM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:176,modifiability,compon,component,176,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:903,modifiability,cost estim,cost estimates,903,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1445,modifiability,coupl,couple,1445,"n you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:54,performance,time,time,54,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:68,performance,time,time,68,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:738,performance,compute resourc,compute resources,738,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1496,performance,CPU,CPU,1496,"Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpecte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1611,performance,time,time,1611,"help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1851,performance,time,time,1851,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2071,performance,cpu,cpu,2071,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:165,safety,input,input,165,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:746,safety,resourc,resources,746,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1184,safety,log,logs,1184,"in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1260,safety,log,logs,1260,"depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1588,safety,log,log,1588," the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask quest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1640,safety,compl,completed,1640,"e via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1704,safety,log,logs,1704,"of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1786,safety,log,log,1786,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1888,safety,compl,completions,1888,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1184,security,log,logs,1184,"in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1260,security,log,logs,1260,"depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1588,security,log,log,1588," the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask quest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1640,security,compl,completed,1640,"e via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1704,security,log,logs,1704,"of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1786,security,log,log,1786,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1888,security,compl,completions,1888,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2666,security,team,team,2666,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:142,testability,coverag,coverage,142,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:746,testability,resourc,resources,746,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1184,testability,log,logs,1184,"in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1260,testability,log,logs,1260,"depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1445,testability,coupl,couple,1445,"n you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1588,testability,log,log,1588," the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask quest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1704,testability,log,logs,1704,"of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1786,testability,log,log,1786,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2383,testability,observ,observe,2383,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:165,usability,input,input,165,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:434,usability,guid,guidelines,434,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:613,usability,help,help,613,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:937,usability,help,help,937,". > 1. Any idea how I might estimate the expected run time? The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1110,usability,progress,progress,1110,"y many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1293,usability,progress,progress,1293,"I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1359,usability,progress,progress,1359,"of make_examples. This makes it hard for me to give general estimation guidelines. Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a differ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2500,usability,behavi,behavior,2500,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:2713,usability,help,help,2713,"better job sizing compute resources? On the GCP DeepVariant tutorial page:. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not? Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck? > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later. > . > thanks. > . > Andy. > . > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . > p.p.s. Is there a better place to ask questions like this? This is a good place to ask :). It's a public forum, so our team and everyone in the community can see and help. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:594,deployability,log,log,594,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1026,deployability,observ,observe,1026,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1274,deployability,Version,Version,1274,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:502,energy efficiency,CPU,CPU,502,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1221,energy efficiency,CPU,CPU,1221,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:451,integrability,coupl,couple,451,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:598,integrability,messag,message,598,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:623,integrability,Event,Eventually,623,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1274,integrability,Version,Version,1274,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:313,interoperability,share,share,313,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:598,interoperability,messag,message,598,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:871,interoperability,standard,standard,871,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:451,modifiability,coupl,couple,451,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1274,modifiability,Version,Version,1274,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:502,performance,CPU,CPU,502,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:617,performance,time,time,617,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1221,performance,CPU,CPU,1221,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:594,safety,log,log,594,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:646,safety,compl,completed,646,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:594,security,log,log,594,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:646,security,compl,completed,646,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:451,testability,coupl,couple,451,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:594,testability,log,log,594,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1026,testability,observ,observe,1026,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:285,usability,help,help,285,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1143,usability,behavi,behavior,1143,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:1246,usability,Learn,Learning,1246,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from? The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com). > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good! > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not. > . > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue. > . region: oregen. m5dn.8xlarge. 32 CPU. 2 x 600GB SSD. Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:87,energy efficiency,estimat,estimate,87,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:304,energy efficiency,cpu,cpu,304,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:983,energy efficiency,core,core,983,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:38,integrability,batch,batch,38,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:279,integrability,batch,batch,279,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:38,performance,batch,batch,38,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:279,performance,batch,batch,279,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:304,performance,cpu,cpu,304,"I am trying to figure out how long my batch might take to run. . 1) any idea how I can estimate the number of examples that will be created? 2) I notices some examples can be created in a min or two, others take almost an hour. any ideas? . Here is a small sample output. From a batch job running on a 4 cpu machine. ```. I0113 21:56:11.182070 139690792257280 make_examples.py:1363] Task 0: 36301 candidates (37329 examples) [1962.23s elapsed]. I0113 21:57:44.984076 139690792257280 make_examples.py:1363] Task 0: 36412 candidates (37442 examples) [93.80s elapsed]. I0113 21:58:41.721879 139690792257280 make_examples.py:1363] Task 0: 36500 candidates (37536 examples) [56.74s elapsed]. I0113 22:54:35.368127 139690792257280 make_examples.py:1363] Task 0: 36601 candidates (37641 examples) [3353.65s elapsed]. I0113 22:58:18.541445 139690792257280 make_examples.py:1363] Task 0: 36715 candidates (37762 examples) [223.17s elapsed]. ```. the sample output below is from a run on a 32 core machine. ```. I0113 19:45:44.880142 140004561770240 make_examples.py:1363] Task 0: 5011 candidates (5121 examples) [3922.37s elapsed]. I0113 19:50:11.671694 140004561770240 make_examples.py:1363] Task 0: 5100 candidates (5210 examples) [266.79s elapsed]. I0113 20:45:11.229505 140004561770240 make_examples.py:1363] Task 0: 5202 candidates (5321 examples) [3299.56s elapsed]. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:487,availability,slo,slower,487,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:98,interoperability,specif,specify,98,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:271,interoperability,specif,specified,271,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:446,interoperability,specif,specify,446,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:487,reliability,slo,slower,487,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:296,safety,input,input,296,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:53,usability,confirm,confirm,53,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:296,usability,input,input,296,"Hi @aedavids ,. since you're running on WES, can you confirm you're using the `--regions` flag to specify a target capture BED? For example, in our WES case study:. https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-exome-case-study.md. You can see that we specified a `--regions=""/input/${CAPTURE_BED}""` flag. The reason why WES runs are faster than WGS runs is because WES only target about 1-2% of the whole genome. If you don't specify that flag, it will be noticeably slower. Please let me know whether you're already using `--regions`. If not, please let me know if adding that solves your problem. That being said, the numbers you reported (with a few thousands seconds elapsed) seem on the higher side. I'll think about how to report and maybe limit the runtime if possible.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:0,deployability,Updat,Update,0,Update: we discussed offline. Using `--regions` helped. I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:0,safety,Updat,Update,0,Update: we discussed offline. Using `--regions` helped. I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:0,security,Updat,Update,0,Update: we discussed offline. Using `--regions` helped. I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:48,usability,help,helped,48,Update: we discussed offline. Using `--regions` helped. I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:61,usability,close,close,61,Update: we discussed offline. Using `--regions` helped. I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/261:415,performance,time,time,415,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/261:99,reliability,doe,does,99,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/261:363,reliability,doe,doesn,363,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/261:524,reliability,pra,practices,524,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/261:69,safety,compl,complicated,69,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/261:69,security,compl,complicated,69,"Hi @aderzelle . This is a good question. VCF normalization is a very complicated area. DeepVariant does attempt to normalize VCF entries, for example by left-aligning indels. This is normalizing within a single sample. Normalizing multiple samples can become quite difficult, as this becomes a combinatorial explosion of alleles to consider and the single sample doesn't see the information of other samples at the time of initial harmonization. For comparing between samples, if possible, I would recommend using our [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md), which will take multiple gVCFs, and can harmonize the representation of all entries together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/261
https://github.com/google/deepvariant/issues/262:48,usability,command,command,48,"Hi cnavery,. Could you please include your full command? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:44,safety,input,input,44,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:181,safety,input,input,181,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:230,safety,input,input,230,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:203,testability,unit,unittest,203,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:44,usability,input,input,44,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:181,usability,input,input,181,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:230,usability,input,input,230,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:454,usability,command,command,454,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:469,usability,document,documentation,469,"```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""0.9.0"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. ```. It is more or less a copy-paste of the command in the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:83,availability,echo,echo,83,What is the value of your ${INPUT_DIR} and ${OUTPUT_DIR} ? What is the output of . echo $INPUT_DIR,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:49,safety,input,input,49,"Also, you may try to replace . `""${INPUT_DIR}"":""/input""`. with. `""${INPUT_DIR}:/input""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:80,safety,input,input,80,"Also, you may try to replace . `""${INPUT_DIR}"":""/input""`. with. `""${INPUT_DIR}:/input""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:49,usability,input,input,49,"Also, you may try to replace . `""${INPUT_DIR}"":""/input""`. with. `""${INPUT_DIR}:/input""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:80,usability,input,input,80,"Also, you may try to replace . `""${INPUT_DIR}"":""/input""`. with. `""${INPUT_DIR}:/input""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:90,availability,echo,echo,90,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:280,availability,error,error,280,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:314,availability,Error,Error,314,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:305,energy efficiency,current,current,305,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:389,interoperability,specif,specification,389,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:280,performance,error,error,280,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:314,performance,Error,Error,314,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:34,safety,test,testdata,34,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:157,safety,test,testdata,157,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:280,safety,error,error,280,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:314,safety,Error,Error,314,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:34,testability,test,testdata,34,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:157,testability,test,testdata,157,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:182,usability,command,commands,182,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:280,usability,error,error,280,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:314,usability,Error,Error,314,"```. INPUT_DIR=""${PWD}/quickstart-testdata"". OUTPUT_DIR=""${PWD}/quickstart-output"". ```. `echo $INPUT_DIR`. returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:67,usability,command,command,67,"`-v ""${OUTPUT_DIR}:/output"" \`. is a direct copy and paste from my command prompt. I can't spot any differences...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:18,deployability,version,version,18,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:278,deployability,version,version,278,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:210,energy efficiency,current,current,210,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:18,integrability,version,version,18,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:278,integrability,version,version,278,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:113,interoperability,specif,specification,113,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:254,interoperability,specif,specify,254,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:18,modifiability,version,version,18,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:278,modifiability,version,version,278,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:36,reliability,doe,doesn,36,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:224,usability,help,help,224,"It looks like you version of docker doesn't parse -v argument the same way as it is parsed on Ubuntu. The volume specification should be ""/output"", instead it is "":/output"" in your case. Check `/usr/bin/docker-current run --help'` to see how to properly specify volume for your version of docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:112,interoperability,specif,specification,112,I linearized the command and it ran. I'm not sure why it couldn't run with the new lines. I'll look into volume specification if I run into an issue with my data. . Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:17,usability,command,command,17,I linearized the command and it ran. I'm not sure why it couldn't run with the new lines. I'll look into volume specification if I run into an issue with my data. . Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/263:127,availability,error,error,127,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:85,deployability,instal,installation,85,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:545,deployability,instal,install,545,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:572,deployability,upgrad,upgrade,572,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:572,modifiability,upgrad,upgrade,572,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:127,performance,error,error,127,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:127,safety,error,error,127,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:15,usability,custom,custom,15,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:127,usability,error,error,127,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:. ```. WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl. curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}"". pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/265:424,deployability,version,version,424,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:453,deployability,version,version,453,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:476,deployability,version,version,476,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:631,energy efficiency,GPU,GPU,631,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:648,energy efficiency,current,currently,648,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:424,integrability,version,version,424,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:453,integrability,version,version,453,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:476,integrability,version,version,476,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:424,modifiability,version,version,424,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:453,modifiability,version,version,453,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:476,modifiability,version,version,476,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:141,performance,time,time,141,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:267,performance,time,time,267,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:344,performance,time,time,344,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:631,performance,GPU,GPU,631,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:544,usability,confirm,confirm,544,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:589,usability,command,command,589,"Hi @ksw9 ,. the issue you reported seems to be the same as:. https://github.com/google/deepvariant/issues/132#issuecomment-482956117. At the time, I opened a follow-up issue here: https://github.com/google/deepvariant/issues/178. I just read through the issue at the time. I don't think we really pinpointed the issue, and it seems like at the time I rebuilt an image. Can you provide the following information:. 1) Your OS version. 2) Your Singularity version. 3) Your numpy version (I'm not sure whether this affects singularity). 4) Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? Thanks. I'm currently on vacation. I'll get to this after next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1344,availability,Error,Errors,1344," whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2103,availability,Error,Errors,2103,"riant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4150,availability,error,errors,4150,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:90,deployability,version,version,90,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:120,deployability,VERSION,VERSION,120,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:268,deployability,version,version,268,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:289,deployability,version,version,289,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:326,deployability,version,version,326,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1369,deployability,modul,module,1369,"gularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1417,deployability,modul,module,1417,"mg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1491,deployability,fail,failed,1491,"I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1539,deployability,fail,failed,1539,"rom here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1637,deployability,fail,failed,1637,"Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2258,deployability,modul,module,2258,"000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2375,deployability,modul,module,2375,"med _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2555,deployability,modul,module,2555,"020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2712,deployability,modul,module,2712,".0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2893,deployability,modul,module,2893,"model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3070,deployability,modul,module,3070,"tput_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4249,deployability,version,version,4249,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:132,energy efficiency,Core,Core,132,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:221,energy efficiency,Core,Core,221,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:489,energy efficiency,GPU,GPU,489,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:568,energy efficiency,GPU,GPU,568,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:775,energy efficiency,gpu,gpu,775,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:852,energy efficiency,GPU,GPU,852,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:921,energy efficiency,gpu,gpu,921,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1468,energy efficiency,core,core,1468,"is with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1528,energy efficiency,core,core,1528,"simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1609,energy efficiency,core,core,1609,"riant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1718,energy efficiency,GPU,GPU,1718,".googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1842,energy efficiency,gpu,gpu,1842,"simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:90,integrability,version,version,90,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:120,integrability,VERSION,VERSION,120,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:268,integrability,version,version,268,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:289,integrability,version,version,289,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:326,integrability,version,version,326,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4249,integrability,version,version,4249,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3379,interoperability,share,shared,3379,". from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3766,interoperability,bind,bind,3766,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4284,interoperability,incompatib,incompatible,4284,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:90,modifiability,version,version,90,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:120,modifiability,VERSION,VERSION,120,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:268,modifiability,version,version,268,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:289,modifiability,version,version,289,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:326,modifiability,version,version,326,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1369,modifiability,modul,module,1369,"gularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1417,modifiability,modul,module,1417,"mg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2258,modifiability,modul,module,2258,"000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2328,modifiability,pac,packages,2328,".vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2375,modifiability,modul,module,2375,"med _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2501,modifiability,pac,packages,2501,"rt. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2555,modifiability,modul,module,2555,"020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2649,modifiability,pac,packages,2649,"16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2712,modifiability,modul,module,2712,".0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2830,modifiability,pac,packages,2830,"ERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2893,modifiability,modul,module,2893,"model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2998,modifiability,pac,packages,2998,"ons ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3070,modifiability,modul,module,3070,"tput_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3167,modifiability,pac,packages,3167,"les_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3766,modifiability,bind,bind,3766,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4249,modifiability,version,version,4249,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:489,performance,GPU,GPU,489,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:568,performance,GPU,GPU,568,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:775,performance,gpu,gpu,775,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:852,performance,GPU,GPU,852,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:921,performance,gpu,gpu,921,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1344,performance,Error,Errors,1344," whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1718,performance,GPU,GPU,1718,".googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1842,performance,gpu,gpu,1842,"simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2103,performance,Error,Errors,2103,"riant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4150,performance,error,errors,4150,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1491,reliability,fail,failed,1491,"I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1539,reliability,fail,failed,1539,"rom here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1637,reliability,fail,failed,1637,"Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3589,reliability,sre,sregistry,3589,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:933,safety,Test,Test,933,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:976,safety,test,test,976,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1344,safety,Error,Errors,1344," whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1369,safety,modul,module,1369,"gularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1417,safety,modul,module,1417,"mg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1684,safety,Test,Test,1684,"y'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1731,safety,test,test,1731,"om/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2103,safety,Error,Errors,2103,"riant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2258,safety,modul,module,2258,"000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2375,safety,modul,module,2375,"med _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2555,safety,modul,module,2555,"020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2712,safety,modul,module,2712,".0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2893,safety,modul,module,2893,"model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3070,safety,modul,module,3070,"tput_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3674,safety,Test,Test,3674,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3794,safety,test,test,3794,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4150,safety,error,errors,4150,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4167,safety,test,test,4167,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:933,testability,Test,Test,933,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:976,testability,test,test,976,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1177,testability,unit,unittest,1177,"ON_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1684,testability,Test,Test,1684,"y'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1731,testability,test,test,1731,"om/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1936,testability,unit,unittest,1936,"ngularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2113,testability,Trace,Traceback,2113,"n_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2758,testability,Trace,Traceback,2758,"B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3674,testability,Test,Test,3674,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3794,testability,test,test,3794,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3967,testability,unit,unittest,3967,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4167,testability,test,test,4167,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:402,usability,confirm,confirm,402,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:447,usability,command,command,447,"Hi @pichuan,. Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version. NAME=""CentOS Linux"". VERSION=""7 (Core)"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""7"". PRETTY_NAME=""CentOS Linux 7 (Core)"". ANSI_COLOR=""0;31"". 2. Your Singularity version. singularity version 3.4.1-1.2.el7. 3. Your numpy version (I'm not sure whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1344,usability,Error,Errors,1344," whether this affects singularity). 1.17.5. 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU? I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`. # Pull Singularity images. INPUT_DIR='singularity'. DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images"". # Non-gpu image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg. # GPU image. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:2103,usability,Error,Errors,2103,"riant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath. ImportError: No module named _multiarray_umath. ImportError: numpy.core._multiarray_umath failed to import. ImportError: numpy.core.umath failed to import. 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data. singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \. ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . # Errors: . Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>. import tensorflow as tf. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>. from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:3694,usability,interact,interactively,3694,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:4150,usability,error,errors,4150,"ensorflow.python import pywrap_tensorflow # pylint: disable=unused-import. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>. from tensorflow.python import pywrap_tensorflow. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>. raise ImportError(msg). ImportError: Traceback (most recent call last):. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>. from tensorflow.python.pywrap_tensorflow_internal import *. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>. _pywrap_tensorflow_internal = swig_import_helper(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper. _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description). ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): . `# Pull the deep variant docker image. sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". . # Test running docker interactively w/Singularity. . BIN_VERSION=""0.9.0"". singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=ucsc.hg19.chr20.unittest.fasta \. --reads=NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=output.vcf.gz \. --output_gvcf=output.g.vcf.gz . `. I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:111,deployability,releas,release,111,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:213,deployability,releas,release,213,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:527,deployability,releas,release,527,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:138,energy efficiency,Current,Currently,138,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:198,testability,plan,plan,198,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:170,usability,support,supporting,170,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:234,usability,support,support,234,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:417,usability,close,close,417,"Hi @ksw9 ,. It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:. https://numpy.org/devdocs/release/1.17.5-notes.html. Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:8,testability,plan,plan,8,"FYI:. I plan to remove the gs://deepvariant/singularity_images directory , and suggest our users to directly pull from our Docker image instead. Example:. ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:91,usability,user,users,91,"FYI:. I plan to remove the gs://deepvariant/singularity_images directory , and suggest our users to directly pull from our Docker image instead. Example:. ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/266:1488,availability,avail,available,1488,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:60,deployability,observ,observed,60,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1136,deployability,observ,observed,1136,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1363,energy efficiency,model,models,1363,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1342,interoperability,specif,specific-deepvariant-models,1342,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1488,reliability,availab,available,1488,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:117,safety,compl,complicated,117,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1164,safety,compl,complicate,1164,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1488,safety,avail,available,1488,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:117,security,compl,complicated,117,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:529,security,sign,signal,529,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1164,security,compl,complicate,1164,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1363,security,model,models,1363,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1488,security,availab,available,1488,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:60,testability,observ,observed,60,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:978,testability,coverag,coverage,978,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1060,testability,coverag,coverage,1060,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1136,testability,observ,observed,1136,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1694,testability,coverag,coverage,1694,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:158,usability,learn,learned,158,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:581,usability,close,closer,581,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:673,usability,close,close,673,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:703,usability,indicat,indicate,703,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:910,usability,user,user,910,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:1606,usability,indicat,indicate,1606,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:526,availability,replic,replicate,526,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:800,availability,error,errors,800,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:800,performance,error,errors,800,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:230,reliability,doe,doesn,230,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:356,reliability,doe,doesn,356,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:978,reliability,doe,does,978,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:800,safety,error,errors,800,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:731,security,ident,identical,731,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:204,testability,coverag,coverage,204,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:641,testability,plan,planning,641,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:970,testability,simpl,simply,970,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:800,usability,error,errors,800,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:970,usability,simpl,simply,970,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). . The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:61,energy efficiency,model,model,61,"Hi @aderzelle . Would you be interested to experiment with a model we have trained on non-human species? If so, perhaps we can move this discussion to email (awcarroll@google.com) and we can provide you the model with some instructions. No guarantee that this will resolve the issue, but we think it is worth trying.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:207,energy efficiency,model,model,207,"Hi @aderzelle . Would you be interested to experiment with a model we have trained on non-human species? If so, perhaps we can move this discussion to email (awcarroll@google.com) and we can provide you the model with some instructions. No guarantee that this will resolve the issue, but we think it is worth trying.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:61,security,model,model,61,"Hi @aderzelle . Would you be interested to experiment with a model we have trained on non-human species? If so, perhaps we can move this discussion to email (awcarroll@google.com) and we can provide you the model with some instructions. No guarantee that this will resolve the issue, but we think it is worth trying.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/266:207,security,model,model,207,"Hi @aderzelle . Would you be interested to experiment with a model we have trained on non-human species? If so, perhaps we can move this discussion to email (awcarroll@google.com) and we can provide you the model with some instructions. No guarantee that this will resolve the issue, but we think it is worth trying.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266
https://github.com/google/deepvariant/issues/267:92,reliability,doe,doesn,92,"yes sorry, I thought by closing the issue you wouldn't have to look at it. . Indeed octopus doesn't produce VAF (well, it does but you have to ask for it at the command line). . Sorry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/267
https://github.com/google/deepvariant/issues/267:122,reliability,doe,does,122,"yes sorry, I thought by closing the issue you wouldn't have to look at it. . Indeed octopus doesn't produce VAF (well, it does but you have to ask for it at the command line). . Sorry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/267
https://github.com/google/deepvariant/issues/267:161,usability,command,command,161,"yes sorry, I thought by closing the issue you wouldn't have to look at it. . Indeed octopus doesn't produce VAF (well, it does but you have to ask for it at the command line). . Sorry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/267
https://github.com/google/deepvariant/issues/268:434,deployability,version,version,434,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:21,energy efficiency,current,current,21,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:426,energy efficiency,current,current,426,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:703,energy efficiency,model,model,703,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:816,energy efficiency,load,loading,816,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:840,energy efficiency,model,model,840,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1038,energy efficiency,model,model,1038,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:434,integrability,version,version,434,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:59,interoperability,specif,specify,59,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:230,interoperability,Specif,Specifically,230,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:251,interoperability,specif,specify,251,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:260,modifiability,PAC,PACBIO,260,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:434,modifiability,version,version,434,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:816,performance,load,loading,816,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:692,safety,input,input,692,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:703,security,model,model,703,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:840,security,model,model,840,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1038,security,model,model,1038,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:692,usability,input,input,692,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:766,usability,behavi,behavior,766,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:829,usability,custom,customized,829,"Hi @aderzelle ,. the current design is that you do need to specify both the `model_type` and `customized_model` flag. The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```. --model_type=WGS \. --customized_model=/input/your.model.ckpt \. ```. (or WES, they should be the same). Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2074,availability,restor,restore,2074,"verableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2217,availability,restor,restore,2217,"1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2304,availability,checkpoint,checkpoint,2304,"cal/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3187,availability,checkpoint,checkpoint,3187,"saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:185,deployability,modul,module,185,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2492,deployability,modul,module,2492,"7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is ther",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3325,deployability,contain,contains,3325,"to_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3736,deployability,log,logs,3736,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4437,deployability,log,log,4437,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:593,energy efficiency,predict,prediction,593,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:611,energy efficiency,predict,predictions,611,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:698,energy efficiency,estimat,estimator,698,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:708,energy efficiency,estimat,estimator,708,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:736,energy efficiency,predict,predict,736,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2338,energy efficiency,model,model,2338,"tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3221,energy efficiency,model,model,3221,". + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3336,energy efficiency,model,model,3336,"odel.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3373,energy efficiency,model,model,3373,"3s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3396,energy efficiency,model,model,3396,"back (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3424,energy efficiency,model,model,3424,":. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4383,energy efficiency,model,model,4383,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2788,integrability,sub,subprocess,2788,"nit_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2881,integrability,sub,subprocess,2881,"ger.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2962,integrability,sub,subprocess,2962,"n2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:271,interoperability,platform,platform,271,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:185,modifiability,modul,module,185,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:244,modifiability,pac,packages,244,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:661,modifiability,pac,packages,661,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:812,modifiability,pac,packages,812,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:978,modifiability,pac,packages,978,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1157,modifiability,pac,packages,1157,"ants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1332,modifiability,pac,packages,1332,"/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1502,modifiability,pac,packages,1502,"iles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1681,modifiability,pac,packages,1681,"mator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1838,modifiability,pac,packages,1838,"ining/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:1977,modifiability,pac,packages,1977,"kages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledPr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2156,modifiability,pac,packages,2156,"kages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2492,modifiability,modul,module,2492,"7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is ther",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2552,modifiability,pac,packages,2552,"py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2652,modifiability,pac,packages,2652,"local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3002,performance,time,time,3002,"training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3977,performance,time,time,3977,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2074,reliability,restor,restore,2074,"verableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2217,reliability,restor,restore,2217,"1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2304,reliability,checkpoint,checkpoint,2304,"cal/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3187,reliability,checkpoint,checkpoint,3187,"saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3310,reliability,doe,does,3310,"nt: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:185,safety,modul,module,185,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:593,safety,predict,prediction,593,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:611,safety,predict,predictions,611,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:736,safety,predict,predict,736,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2298,safety,valid,valid,2298,"""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosqui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2317,safety,input,input,2317,"hon2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2492,safety,modul,module,2492,"7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is ther",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3200,safety,input,input,3200,"line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3736,safety,log,logs,3736,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4014,safety,input,input,4014,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4145,safety,input,input,4145,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4177,safety,input,input,4177,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4217,safety,input,input,4217,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4362,safety,input,input,4362,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4437,safety,log,log,4437,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2338,security,model,model,2338,"tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3221,security,model,model,3221,". + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3336,security,model,model,3336,"odel.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3373,security,model,model,3373,"3s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3396,security,model,model,3396,"back (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3424,security,model,model,3424,":. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3736,security,log,logs,3736,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4383,security,model,model,4383,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4437,security,log,log,4437,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:39,testability,Trace,Traceback,39,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:745,testability,hook,hooks,745,"Hello, I tried but this returns . ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict. hooks=all_hooks) as mon_sess:. File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2394,testability,Trace,Traceback,2394,"7, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3736,testability,log,logs,3736,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4437,testability,log,log,4437,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2317,usability,input,input,2317,"hon2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2365,usability,user,user,2365,"/monitored_session.py"", line 1127, in _create_session. return self._sess_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session. init_fn=self._scaffold.init_fn). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2810,usability,command,command,2810,"ocal/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:2993,usability,Command,Command,2993,"python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3200,usability,input,input,3200,"line 1268, in restore. + compat.as_text(save_path)). ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:3257,usability,statu,status,3257,"eError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s. user	0m9.233s. sys	0m4.817s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4014,usability,input,input,4014,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4145,usability,input,input,4145,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4177,usability,input,input,4177,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4217,usability,input,input,4217,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:4362,usability,input,input,4362,"in/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1. ```. However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model"". mkdir -p ""${OUTPUT_DIR}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""0.9.0"". N_SHARDS=20. LOG_DIR=""${OUTPUT_DIR}/logs"" . mkdir -p ""${LOG_DIR}"" . #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3). #for SAMPLE in ""${decade[@]}"". #do. # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz. #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log"". #done. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:40,energy efficiency,model,model,40,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:132,energy efficiency,model,model,132,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:111,safety,input,input,111,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:40,security,model,model,40,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:132,security,model,model,132,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:111,usability,input,input,111,"@aderzelle . Because your file name is `model.ckpt-97700*`, so you need to pass in:. ```. --customized_model=""/input/mosquito_model/model.ckpt-97700"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:43,energy efficiency,model,model,43,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:143,energy efficiency,model,model,143,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:198,energy efficiency,model,model,198,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:288,interoperability,prox,proximity,288,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:122,safety,input,input,122,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:43,security,model,model,43,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:143,security,model,model,143,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:198,security,model,model,198,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:122,usability,input,input,122,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:282,usability,close,close,282,"> @aderzelle. > Because your file name is `model.ckpt-97700*`, so you need to pass in:. > . > ```. > --customized_model=""/input/mosquito_model/model.ckpt-97700"". > ```. Indeed now it works. And the model seems to work much better, it no longer calls as HomRef Het sites that are in close proximity!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/269:644,energy efficiency,CPU,CPUs,644,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:182,integrability,batch,batches,182,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:820,interoperability,specif,specify,820,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:735,modifiability,interm,intermediate,735,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:182,performance,batch,batches,182,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:644,performance,CPU,CPUs,644,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:655,reliability,diagno,diagnose,655,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:790,reliability,diagno,diagnosing,790,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:655,testability,diagno,diagnose,655,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:790,testability,diagno,diagnosing,790,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:48,usability,progress,progress,48,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:228,usability,experien,experience,228,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:956,usability,help,helps,956,"Hi @aedavids . the `call_variants` step reports progress with the line like you saw here:. ```. I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]. ```. And in my experience it should print the next lines within a reasonable interval. One possibility is that you might need smaller batch_size because your hardware. You can do so by using the `--batch_size` flag for call_variants, which has a default of 512. You can try lowering it to 32, for example. (But I'm also not sure if this is the cause. Because I'm not sure a batch_size too big would have resulted in under utilized CPUs.). To diagnose this, you might want to run the 3 steps separately so you can keep the intermediate files from make_examples. Or, when you're diagnosing this, you can also specify a smaller region for your run, so you don't have to wait for multiple hours again before `call_variants` step starts. Hope this helps. I'll keep this bug open for now. Please let me know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/270:24,availability,error,error,24,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:30,integrability,messag,message,30,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:30,interoperability,messag,message,30,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:24,performance,error,error,24,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:24,safety,error,error,24,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:24,usability,error,error,24,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:210,usability,tool,tools,210,Hi @situssog . From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:595,energy efficiency,predict,predict,595,"Hi @situssog . To expand on Pi-Chuan's answer - DeepVariant will expect quality scores to be present in the BAM file (and to have the same length as the bases for each read). In order to run DeepVariant, you would need to add QUAL scores of all one value. It is unclear how DeepVariant's accuracy and behavior in variant calling would change as a result. . I believe that a value of ""!"" corresponds to the lowest base qual value. DeepVariant may be conservative in calling reads if Base Qualities are given that as the confidence. A value like ""@"" would be more balanced, but it is difficult to predict how DeepVariant will behave either way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:595,safety,predict,predict,595,"Hi @situssog . To expand on Pi-Chuan's answer - DeepVariant will expect quality scores to be present in the BAM file (and to have the same length as the bases for each read). In order to run DeepVariant, you would need to add QUAL scores of all one value. It is unclear how DeepVariant's accuracy and behavior in variant calling would change as a result. . I believe that a value of ""!"" corresponds to the lowest base qual value. DeepVariant may be conservative in calling reads if Base Qualities are given that as the confidence. A value like ""@"" would be more balanced, but it is difficult to predict how DeepVariant will behave either way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:301,usability,behavi,behavior,301,"Hi @situssog . To expand on Pi-Chuan's answer - DeepVariant will expect quality scores to be present in the BAM file (and to have the same length as the bases for each read). In order to run DeepVariant, you would need to add QUAL scores of all one value. It is unclear how DeepVariant's accuracy and behavior in variant calling would change as a result. . I believe that a value of ""!"" corresponds to the lowest base qual value. DeepVariant may be conservative in calling reads if Base Qualities are given that as the confidence. A value like ""@"" would be more balanced, but it is difficult to predict how DeepVariant will behave either way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:26,interoperability,specif,specific,26,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:85,modifiability,Pac,PacBio,85,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:119,modifiability,Pac,PacificBiosciences,119,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:192,testability,simpl,simply,192,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:94,usability,tool,tool,94,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:192,usability,simpl,simply,192,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:306,usability,tool,tools,306,"Hi @situssog . Is there a specific reason why you have a fastA file and not a fastQ? PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:21,deployability,updat,update,21,"Okay, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:21,safety,updat,update,21,"Okay, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:21,security,updat,update,21,"Okay, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/271:464,interoperability,specif,specify,464,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:728,interoperability,specif,specify,728,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:823,interoperability,share,share,823,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:167,performance,parallel,parallelism,167,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:218,security,control,controlled,218,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:218,testability,control,controlled,218,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:622,usability,document,documentation,622,"Hi @aderzelle . This is not unexpected:. the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:. `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto. (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:77,energy efficiency,GPU,GPU,77,"I am running it on a local machine, as far as I know I am not running on the GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:77,performance,GPU,GPU,77,"I am running it on a local machine, as far as I know I am not running on the GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:175,safety,compl,completely,175,Hi @aderzelle . The flag option I gave was just an example. Maybe use_per_session_threads in the config proto can be relevant. I haven't used this option before so I can't be completely sure. . Are you trying to limit to just one thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:175,security,compl,completely,175,Hi @aderzelle . The flag option I gave was just an example. Maybe use_per_session_threads in the config proto can be relevant. I haven't used this option before so I can't be completely sure. . Are you trying to limit to just one thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:103,deployability,resourc,resource,103,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:103,energy efficiency,resourc,resource,103,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:180,energy efficiency,cpu,cpu,180,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:103,performance,resourc,resource,103,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:180,performance,cpu,cpu,180,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:103,safety,resourc,resource,103,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:103,testability,resourc,resource,103,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:240,testability,plan,planning,240,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:212,usability,help,help,212,"Oh no, it's just that we are 3 sharing a local big computer, hence it's important we know exactly what resource each piece of software uses. But now that I know that 1 shard =/= 1 cpu thread that's already a big help. It's just a matter of planning with coworkers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:23,testability,context,context,23,"Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/271:117,usability,close,close,117,"Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/272:347,deployability,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:347,integrability,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:347,modifiability,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:347,safety,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:347,testability,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:190,usability,support,support,190,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:338,usability,behavi,behavior,338,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:434,usability,user,user-images,434,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for SmithWaterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:417,usability,command,command,417,And child reads should have been aligned the same way as dad's. But we have a special algorithm that decides whether to run realignment. In the case of a child this algorithm decided against realignment. This algorithm can be turned off in which case reads will be realigned for all samples the same way. In order to do that make_examples needs to run with --nows_use_window_selector_model flag. Below is the example command I used to run make_example:. make_examples \. --ref GRCh38.genome.fa \. --reads kid_sorted.bam \. --examples /tmp/examples.tfrecord.gz \. --mode calling \. --regions chr8:75144950-75145050 \. --nows_use_window_selector_model,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:295,availability,down,downside,295,"hi, thanks for the replies. so `--make_examples_extra_args ""ws_use_window_selector_model=false""` (since I am using `run_deepvariant`) will accomplish the same as what @akolesnikov suggests with `--nows_use_window_selector_model` ? and I don't see much documentation on that option, is there any downside to that? (I assume it just affects run time?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:343,performance,time,time,343,"hi, thanks for the replies. so `--make_examples_extra_args ""ws_use_window_selector_model=false""` (since I am using `run_deepvariant`) will accomplish the same as what @akolesnikov suggests with `--nows_use_window_selector_model` ? and I don't see much documentation on that option, is there any downside to that? (I assume it just affects run time?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:252,usability,document,documentation,252,"hi, thanks for the replies. so `--make_examples_extra_args ""ws_use_window_selector_model=false""` (since I am using `run_deepvariant`) will accomplish the same as what @akolesnikov suggests with `--nows_use_window_selector_model` ? and I don't see much documentation on that option, is there any downside to that? (I assume it just affects run time?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:764,availability,down,downside,764,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:853,availability,slo,slower,853,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1116,deployability,releas,released,1116,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1173,deployability,releas,releases,1173,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:874,energy efficiency,model,model,874,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1244,energy efficiency,model,model,1244,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:46,integrability,wrap,wrapper,46,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:387,integrability,event,eventually,387,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:46,interoperability,wrapper,wrapper,46,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:148,interoperability,specif,specify,148,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:300,interoperability,specif,specify,300,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:853,reliability,slo,slower,853,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:874,security,model,model,874,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:992,security,sign,significantly,992,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1244,security,model,model,1244,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1299,security,control,controlled,1299,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1299,testability,control,controlled,1299,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:522,usability,document,documented,522,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. . In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`. We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:. https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:. https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:470,energy efficiency,CPU,CPU,470,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:573,energy efficiency,cpu,cpu-only-machine-on-google-cloud-platform,573,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:606,interoperability,platform,platform,606,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:288,modifiability,PAC,PACBIO,288,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:368,modifiability,PAC,PACBIO,368,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:470,performance,CPU,CPU,470,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:573,performance,cpu,cpu-only-machine-on-google-cloud-platform,573,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:309,reliability,doe,doesn,309,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:559,usability,command,command-for-a-cpu-only-machine-on-google-cloud-platform,559,"@kokyriakidis to your question, **having ws_use_window_selector_model was to improve runtime**, not to improve accuracy or sensitivity. But empirically we expect the trade-off on accuracy to be small. Below you can see my comparison between turning it on/off for WGS and WES on v0.9. (On PACBIO setting, this doesn't affect anything because we don't run realigner for PACBIO.). ---. I ran some numbers based on the v0.9 WGS and WES case study data. I used [this type of CPU machine](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) for the runtime. The following results were done in two settings:. **`BASE`**: This is the default (i.e., `ws_use_window_selector_model` is true). **`EXPT`**: Turn off window selector (i.e., set `ws_use_window_selector_model` to false). ## On WGS case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 81m29.320s | 0.998112 | 0.999633 . **`EXPT`** | 107m3.893s | 0.998156 | 0.999642 . ## On WES case study:. Settings | `make_examples` runtime | Indel F1 | SNP F1. --- | --- | --- | --- . **`BASE`** | 10m5.515s | 0.973295 | 0.999318 . **`EXPT`** | 19m25.869s | 0.974056 | 0.999362 . For the detailed hap.py output, you can see [here](https://gist.github.com/pichuan/af1c058e188ca8ad5e0770322aff6f54).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:515,deployability,releas,released,515,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:129,energy efficiency,reduc,reduce,129,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:569,energy efficiency,frequenc,frequency,569,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1153,energy efficiency,frequenc,frequency,1153,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:270,integrability,filter,filtering,270,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:468,integrability,filter,filtered,468,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:805,integrability,filter,filtering,805,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:124,reliability,doe,does,124,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1105,reliability,doe,does,1105,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1571,reliability,doe,does,1571,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:642,security,DNs,DNs,642,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:326,usability,user,user-images,326,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:824,usability,user,user-images,824,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1425,usability,help,help,1425,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:1504,usability,help,help,1504,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`. does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. . A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:496,deployability,releas,release,496,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:332,energy efficiency,profil,profile,332,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:300,modifiability,interm,intermediate,300,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:832,modifiability,interm,intermediate,832,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:332,performance,profil,profile,332,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:340,performance,perform,performance,340,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:398,performance,time,time,398,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:845,performance,time,timescale,845,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:340,usability,perform,performance,340,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:69,modifiability,exten,extent,69,"Hi Andrew, thanks for the reply. I hadn't previously appreciated the extent of this problem before either--not just in DV, but everywhere. The ""kid"" and ""dad"" variants I posted above can be made to be the same variant-set if we know they are on the same haplotype--and running with the window selection off does this, but it still creates a problem with annotating across different call-sets when different representations are used. [gnomad prefers the representation of an insertion and a deletion](https://gnomad.broadinstitute.org/region/8-75144962-75145002?dataset=gnomad_r3) rather than the 3 individual SNPs. So, even if this is further resolved in DV (running with `ws_use_window_selector_model=false` is sufficient for me), then it will be, in cases like this, impossible to correctly annotate across cohorts without phasing information. edit: feel free to close this issue as my original issue is addressed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:307,reliability,doe,does,307,"Hi Andrew, thanks for the reply. I hadn't previously appreciated the extent of this problem before either--not just in DV, but everywhere. The ""kid"" and ""dad"" variants I posted above can be made to be the same variant-set if we know they are on the same haplotype--and running with the window selection off does this, but it still creates a problem with annotating across different call-sets when different representations are used. [gnomad prefers the representation of an insertion and a deletion](https://gnomad.broadinstitute.org/region/8-75144962-75145002?dataset=gnomad_r3) rather than the 3 individual SNPs. So, even if this is further resolved in DV (running with `ws_use_window_selector_model=false` is sufficient for me), then it will be, in cases like this, impossible to correctly annotate across cohorts without phasing information. edit: feel free to close this issue as my original issue is addressed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:441,usability,prefer,prefers,441,"Hi Andrew, thanks for the reply. I hadn't previously appreciated the extent of this problem before either--not just in DV, but everywhere. The ""kid"" and ""dad"" variants I posted above can be made to be the same variant-set if we know they are on the same haplotype--and running with the window selection off does this, but it still creates a problem with annotating across different call-sets when different representations are used. [gnomad prefers the representation of an insertion and a deletion](https://gnomad.broadinstitute.org/region/8-75144962-75145002?dataset=gnomad_r3) rather than the 3 individual SNPs. So, even if this is further resolved in DV (running with `ws_use_window_selector_model=false` is sufficient for me), then it will be, in cases like this, impossible to correctly annotate across cohorts without phasing information. edit: feel free to close this issue as my original issue is addressed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:865,usability,close,close,865,"Hi Andrew, thanks for the reply. I hadn't previously appreciated the extent of this problem before either--not just in DV, but everywhere. The ""kid"" and ""dad"" variants I posted above can be made to be the same variant-set if we know they are on the same haplotype--and running with the window selection off does this, but it still creates a problem with annotating across different call-sets when different representations are used. [gnomad prefers the representation of an insertion and a deletion](https://gnomad.broadinstitute.org/region/8-75144962-75145002?dataset=gnomad_r3) rather than the 3 individual SNPs. So, even if this is further resolved in DV (running with `ws_use_window_selector_model=false` is sufficient for me), then it will be, in cases like this, impossible to correctly annotate across cohorts without phasing information. edit: feel free to close this issue as my original issue is addressed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:56,integrability,coupl,couple,56,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:708,integrability,filter,filter,708,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:56,modifiability,coupl,couple,56,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:56,testability,coupl,couple,56,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:697,testability,simpl,simpler,697,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:36,usability,help,helpful,36,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:697,usability,simpl,simpler,697,"I can open a separate issue if it's helpful, but just a couple more things related to this... . First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:56,availability,Heal,Health,56,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:129,availability,consist,consistency,129,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:69,deployability,releas,released,69,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:255,deployability,Updat,Updated,255,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:378,deployability,releas,release,378,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:431,deployability,releas,releases,431,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:309,energy efficiency,model,model,309,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:302,modifiability,Pac,PacBio,302,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:255,safety,Updat,Updated,255,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:41,security,team,team,41,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:255,security,Updat,Updated,255,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:309,security,model,model,309,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:129,usability,consist,consistency,129,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:470,usability,feedback,feedback,470,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:490,usability,experien,experience,490,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:. - Updated to Python3 and TensorFlow2. - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 . If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/274:137,usability,behavi,behavior,137,"Hi @aderzelle . It might take me a while to prioritize this. But I will get back to this later. . As mentioned before, our call_variants behavior is based on how TensorFlow works. If you need this to be resolved soon, it might help to look into how to adjust TensorFlow's behavior first. Thanks for reporting the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:227,usability,help,help,227,"Hi @aderzelle . It might take me a while to prioritize this. But I will get back to this later. . As mentioned before, our call_variants behavior is based on how TensorFlow works. If you need this to be resolved soon, it might help to look into how to adjust TensorFlow's behavior first. Thanks for reporting the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:272,usability,behavi,behavior,272,"Hi @aderzelle . It might take me a while to prioritize this. But I will get back to this later. . As mentioned before, our call_variants behavior is based on how TensorFlow works. If you need this to be resolved soon, it might help to look into how to adjust TensorFlow's behavior first. Thanks for reporting the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:118,availability,error,error,118,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:288,availability,unavail,unavailable,288,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:765,availability,slo,slower,765,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1033,availability,avail,available,1033,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1078,availability,slo,slowdown,1078,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:239,deployability,fail,failed,239,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:267,deployability,Resourc,Resource,267,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:56,energy efficiency,core,cores,56,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:173,energy efficiency,current,current,173,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:267,energy efficiency,Resourc,Resource,267,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:613,energy efficiency,core,cores,613,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:677,energy efficiency,core,cores,677,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:813,energy efficiency,core,cores,813,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:457,modifiability,variab,variable,457,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:516,modifiability,interm,intermediate,516,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:118,performance,error,error,118,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:267,performance,Resourc,Resource,267,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:239,reliability,fail,failed,239,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:765,reliability,slo,slower,765,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1033,reliability,availab,available,1033,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1078,reliability,slo,slowdown,1078,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:14,safety,test,testing,14,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:118,safety,error,error,118,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:267,safety,Resourc,Resource,267,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:557,safety,reme,remember,557,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1033,safety,avail,available,1033,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1033,security,availab,available,1033,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:14,testability,test,testing,14,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:267,testability,Resourc,Resource,267,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:118,usability,error,error,118,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:100,interoperability,specif,specifically,100,"Thanks @chrisfleisch for following up on this issue. If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right? I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:229,safety,test,test,229,"Thanks @chrisfleisch for following up on this issue. If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right? I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:58,testability,understand,understand,58,"Thanks @chrisfleisch for following up on this issue. If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right? I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:229,testability,test,test,229,"Thanks @chrisfleisch for following up on this issue. If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right? I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:271,usability,progress,progress,271,"Thanks @chrisfleisch for following up on this issue. If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right? I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:186,availability,error,error,186,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:355,availability,unavail,unavailable,355,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1861,availability,operat,operating,1861," . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2041,availability,error,error,2041,"iled build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 108",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2173,availability,error,error,2173,"all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2979,availability,unavail,unavailable,2979,"iler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:306,deployability,fail,failed,306,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:334,deployability,Resourc,Resource,334,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:586,deployability,modul,module,586,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:698,deployability,modul,module,698,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:814,deployability,modul,module,814,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:985,deployability,modul,module,985,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:992,deployability,fail,failed,992,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1042,deployability,fail,failed,1042,"and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this erro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1049,deployability,build,build,1049,"elieve it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1196,deployability,version,version,1196,"LAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1278,deployability,version,version,1278,"ad_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1300,deployability,instal,installed,1300," failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1319,deployability,instal,installation,1319,"3 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1640,deployability,version,versions,1640,"on2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1805,deployability,instal,installed,1805,"in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1835,deployability,instal,installed,1835,"or(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1916,deployability,version,versions,1916,"SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1935,deployability,instal,installed,1935,". > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1990,deployability,version,versions,1990,"iled. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2013,deployability,build,build,2013,"ou are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2019,deployability,log,log,2019,"e trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2218,deployability,modul,module,2218,"ild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2372,deployability,loader,loader,2372,"you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2555,deployability,modul,module,2555,"tories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2958,deployability,Resourc,Resource,2958," from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3174,deployability,modul,module,3174,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:141,energy efficiency,core,cores,141,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:240,energy efficiency,current,current,240,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:334,energy efficiency,Resourc,Resource,334,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:415,energy efficiency,current,current,415,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:723,energy efficiency,core,core,723,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:782,energy efficiency,core,core,782,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2372,energy efficiency,load,loader,2372,"you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2958,energy efficiency,Resourc,Resource,2958," from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1131,integrability,repositor,repository,1131," the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1196,integrability,version,version,1196,"LAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1278,integrability,version,version,1278,"ad_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1640,integrability,version,versions,1640,"on2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1916,integrability,version,versions,1916,"SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1990,integrability,version,versions,1990,"iled. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2671,integrability,sub,subprocess,2671,"If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2778,integrability,sub,subprocess,2778,"ails on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2866,integrability,sub,subprocess,2866,"system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3484,integrability,sub,subprocess,3484,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3579,integrability,sub,subprocess,3579,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3664,integrability,sub,subprocess,3664,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3740,integrability,buffer,buffer,3740,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1131,interoperability,repositor,repository,1131," the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:586,modifiability,modul,module,586,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:655,modifiability,pac,packages,655,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:698,modifiability,modul,module,698,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:767,modifiability,pac,packages,767,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:814,modifiability,modul,module,814,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:975,modifiability,extens,extension,975,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:985,modifiability,modul,module,985,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1196,modifiability,version,version,1196,"LAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1278,modifiability,version,version,1278,"ad_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1640,modifiability,version,versions,1640,"on2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1916,modifiability,version,versions,1916,"SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1990,modifiability,version,versions,1990,"iled. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2218,modifiability,modul,module,2218,"ild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2555,modifiability,modul,module,2555,"tories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3174,modifiability,modul,module,3174,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3238,modifiability,pac,packages,3238,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3342,modifiability,pac,packages,3342,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:186,performance,error,error,186,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:334,performance,Resourc,Resource,334,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2041,performance,error,error,2041,"iled build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 108",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2173,performance,error,error,2173,"all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2372,performance,load,loader,2372,"you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2958,performance,Resourc,Resource,2958," from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3704,performance,time,time,3704,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3721,performance,parallel,parallel,3721,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:306,reliability,fail,failed,306,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:992,reliability,fail,failed,992,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1042,reliability,fail,failed,1042,"and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this erro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:186,safety,error,error,186,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:334,safety,Resourc,Resource,334,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:586,safety,modul,module,586,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:698,safety,modul,module,698,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:814,safety,modul,module,814,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:985,safety,modul,module,985,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2019,safety,log,log,2019,"e trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2041,safety,error,error,2041,"iled build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 108",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2173,safety,error,error,2173,"all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2218,safety,modul,module,2218,"ild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2555,safety,modul,module,2555,"tories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2958,safety,Resourc,Resource,2958," from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3174,safety,modul,module,3174,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1204,security,control,control,1204,"_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2019,security,log,log,2019,"e trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:334,testability,Resourc,Resource,334,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:439,testability,Trace,Traceback,439,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1204,testability,control,control,1204,"_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1247,testability,simpl,simply,1247,"254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2019,testability,log,log,2019,"e trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2243,testability,Trace,Traceback,2243,"imply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2958,testability,Resourc,Resource,2958," from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3074,testability,Trace,Traceback,3074,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:186,usability,error,error,186,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1247,usability,simpl,simply,1247,"254915 max. > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>. > import numpy as np. > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>. > from . import core. > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>. > raise ImportError(msg). > ImportError:. > . > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! > . > Importing the multiarray numpy extension module failed. Most. > likely you are trying to import a failed build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2041,usability,error,error,2041,"iled build of numpy. > Here is how to proceed:. > - If you're working with a numpy git repository, try `git clean -xdf`. > (removes all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 108",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2173,usability,error,error,2173,"all files not under version control) and rebuild numpy. > - If you are simply trying to use the numpy version that you have installed:. > your installation is broken - please reinstall numpy. > - If you have already reinstalled and that did not fix the problem, then:. > 1. Check that you are using the Python you expect (you're using /usr/bin/python),. > and that you have no directories in your PATH or PYTHONPATH that can. > interfere with the Python and numpy versions you're trying to use. > 2. If (1) looks fine, you can open a new issue at. > https://github.com/numpy/numpy/issues. Please include details on:. > - how you installed Python. > - how you installed numpy. > - your operating system. > - whether or not you have multiple versions of Python installed. > - if you built from source, your compiler versions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3036,usability,user,user,3036,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3506,usability,command,command,3506,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3695,usability,Command,Command,3695,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3983,usability,statu,status,3983,"rsions and ideally a build log. > . > Note: this error has many possible causes, so please don't comment on. > an existing issue about this - open a new one instead. > . > Original error was: PyCapsule_Import could not import module ""datetime"". > . > Traceback (most recent call last):. > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main. > ""__main__"", fname, loader, pkg_name). > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code. > exec code in run_globals. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>. > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main. > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call. > return Popen(*popenargs, **kwargs).wait(). > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__. > errread, errwrite). > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child. > self.pid = os.fork(). > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'. > . > real 19m19.271s. > user 1084m5.580s. > sys 17m12.750s. > Traceback (most recent call last):. > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. > app.run(main). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. > subprocess.check_call(command, shell=True, executable='/bin/bash'). > File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. > raise CalledProcessError(retcode, cmd). > subprocess.CalledProcessError: Command 'time seq 0 127 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode callin. > g --ref ""/my_path.fa"" --reads ""/my_path.cram"" --examples ""/tmp_data/my_path/make_examples.tfrecord@128.gz"" --gvcf ""/my_path/gvcf.tfrecord@128.gz"" --task {}' return. > ed non-zero exit status 67",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:251,availability,error,error,251,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:420,availability,unavail,unavailable,420,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:275,deployability,Stack,Stackoverflow,275,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:321,deployability,stack,stackoverflow,321,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:399,deployability,resourc,resource-temporarily-unavailable,399,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:485,deployability,stack,stackoverflow,485,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1438,deployability,api,apic,1438,"cally, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take clo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:111,energy efficiency,Cloud,Cloud,111,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:399,energy efficiency,resourc,resource-temporarily-unavailable,399,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:710,energy efficiency,cloud,cloud-platform,710,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:789,energy efficiency,cloud,cloud,789,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:949,energy efficiency,CPU,CPU,949,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1008,energy efficiency,CPU,CPU,1008,"leisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1028,energy efficiency,CPU,CPU,1028,"ok me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1061,energy efficiency,core,core,1061,"o try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl htt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1070,energy efficiency,Core,Core,1070,"s. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1149,energy efficiency,CPU,CPU,1149,"uce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1165,energy efficiency,Model,Model,1165," the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1176,energy efficiency,Model,Model,1176,"examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1216,energy efficiency,CPU,CPU,1216,"low (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1382,energy efficiency,CPU,CPU,1382,"pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as exp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1598,energy efficiency,cpu,cpuid,1598,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2371,energy efficiency,core,cores,2371,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2451,energy efficiency,CPU,CPU,2451,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1438,integrability,api,apic,1438,"cally, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take clo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:433,interoperability,Specif,Specifically,433,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:716,interoperability,platform,platform,716,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:819,interoperability,standard,standard-,819,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:927,interoperability,Architectur,Architecture,927,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1082,interoperability,socket,socket,1082," an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,interoperability,Socket,Socket,1093,"hine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1438,interoperability,api,apic,1438,"cally, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take clo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:57,performance,time,time,57,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:251,performance,error,error,251,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:399,performance,resourc,resource-temporarily-unavailable,399,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:839,performance,disk,disk-size,839,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:949,performance,CPU,CPU,949,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1008,performance,CPU,CPU,1008,"leisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1028,performance,CPU,CPU,1028,"ok me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1149,performance,CPU,CPU,1149,"uce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1216,performance,CPU,CPU,1216,"low (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1309,performance,cach,cache,1309," https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1325,performance,cach,cache,1325,"erflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1340,performance,cach,cache,1340,"tions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1356,performance,cach,cache,1356,"penblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Becaus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1382,performance,CPU,CPU,1382,"pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as exp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1598,performance,cpu,cpuid,1598,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2451,performance,CPU,CPU,2451,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1539,reliability,rdt,rdtscp,1539,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:120,safety,test,test,120,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:251,safety,error,error,251,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:399,safety,resourc,resource-temporarily-unavailable,399,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:563,safety,compl,completeness,563,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:563,security,compl,completeness,563,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1082,security,soc,socket,1082," an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,security,Soc,Socket,1093,"hine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1135,security,Auth,AuthenticAMD,1135," to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1165,security,Model,Model,1165," the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1176,security,Model,Model,1176,"examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:120,testability,test,test,120,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:399,testability,resourc,resource-temporarily-unavailable,399,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:251,usability,error,error,251,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:662,usability,USER,USER,662,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step. I'll post what I did below (which didn't reproduce your error). But, maybe this Stackoverflow issue could be related? https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable. Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I jus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2260,usability,user,user-images,2260,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2437,usability,close,close,2437,"try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you? ---. For completeness, here is what I tried:. I got a AMD machine:. ```. gcloud compute instances create ""${USER}-amd"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \. --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \. --zone ""europe-west4-b"". ```. On that machine, I ran:. ```. $ lscpu. Architecture: x86_64. CPU op-mode(s): 32-bit, 64-bit. Byte Order: Little Endian. CPU(s): 16. On-line CPU(s) list: 0-15. Thread(s) per core: 2. Core(s) per socket: 8. Socket(s): 1. NUMA node(s): 1. Vendor ID: AuthenticAMD. CPU family: 23. Model: 49. Model name: AMD EPYC 7B12. Stepping: 0. CPU MHz: 2249.998. BogoMIPS: 4499.99. Hypervisor vendor: KVM. Virtualization type: full. L1d cache: 32K. L1i cache: 32K. L2 cache: 512K. L3 cache: 16384K. NUMA node0 CPU(s): 0-15. Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid. ```. Then, I just directly run the WES case study with docker:. ```. $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x . ```. Once make_examples started running, I used top to look at the jobs:. ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,availability,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2994,availability,slo,slows,2994,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:3000,availability,down,down,3000,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:887,deployability,api,apic,887,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,deployability,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2094,deployability,api,apic,2094,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:55,energy efficiency,cloud,cloud,55,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:221,energy efficiency,core,core,221,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:249,energy efficiency,core,core,249,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:321,energy efficiency,CPU,CPU,321,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:384,energy efficiency,CPU,CPU,384,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:406,energy efficiency,CPU,CPU,406,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:441,energy efficiency,core,core,441,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:452,energy efficiency,Core,Core,452,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:540,energy efficiency,CPU,CPU,540,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:557,energy efficiency,Model,Model,557,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:570,energy efficiency,Model,Model,570,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:614,energy efficiency,CPU,CPU,614,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:646,energy efficiency,CPU,CPU,646,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:829,energy efficiency,CPU,CPU,829,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1047,energy efficiency,cpu,cpuid,1047," any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,energy efficiency,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1490,energy efficiency,CPU,CPU,1490,"> NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1553,energy efficiency,CPU,CPU,1553,"> Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1575,energy efficiency,CPU,CPU,1575,"ame: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1610,energy efficiency,core,core,1610,"M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. Whe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1621,energy efficiency,Core,Core,1621,"0GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1709,energy efficiency,CPU,CPU,1709," > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1727,energy efficiency,Model,Model,1727,"ype: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1739,energy efficiency,Model,Model,1739," L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1783,energy efficiency,CPU,CPU,1783,"he: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1964,energy efficiency,CPU,CPU,1964," pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1996,energy efficiency,CPU,CPU,1996,"arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2029,energy efficiency,CPU,CPU,2029,"ogy nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2244,energy efficiency,cpu,cpuid,2244,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2688,energy efficiency,load,load,2688,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2721,energy efficiency,load,load,2721,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2848,energy efficiency,load,load,2848,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2881,energy efficiency,load,load,2881,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:887,integrability,api,apic,887,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2094,integrability,api,apic,2094,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:297,interoperability,Architectur,Architecture,297,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:464,interoperability,socket,socket,464,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:478,interoperability,Socket,Socket,478,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:887,interoperability,api,apic,887,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1466,interoperability,Architectur,Architecture,1466,"24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1633,interoperability,socket,socket,1633,"ing: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_stu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1647,interoperability,Socket,Socket,1647,"MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2094,interoperability,api,apic,2094,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:126,performance,performance issu,performance issue,126,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:321,performance,CPU,CPU,321,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:384,performance,CPU,CPU,384,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:406,performance,CPU,CPU,406,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:540,performance,CPU,CPU,540,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:614,performance,CPU,CPU,614,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:646,performance,CPU,CPU,646,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:747,performance,cach,cache,747,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:765,performance,cach,cache,765,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:782,performance,cach,cache,782,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:801,performance,cach,cache,801,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:829,performance,CPU,CPU,829,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1047,performance,cpu,cpuid,1047," any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1490,performance,CPU,CPU,1490,"> NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1553,performance,CPU,CPU,1553,"> Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1575,performance,CPU,CPU,1575,"ame: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1709,performance,CPU,CPU,1709," > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1783,performance,CPU,CPU,1783,"he: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1884,performance,cach,cache,1884,"apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1902,performance,cach,cache,1902,"mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes ab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1919,performance,cach,cache,1919,"6 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1937,performance,cach,cache,1937," sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1964,performance,CPU,CPU,1964," pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1996,performance,CPU,CPU,1996,"arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2029,performance,CPU,CPU,2029,"ogy nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2244,performance,cpu,cpuid,2244,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2688,performance,load,load,2688,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2721,performance,load,load,2721,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2848,performance,load,load,2848,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2881,performance,load,load,2881,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:975,reliability,rdt,rdtscp,975,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,reliability,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2195,reliability,rdt,rdtscp,2195,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2994,reliability,slo,slows,2994,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:192,safety,test,testing,192,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,safety,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2774,safety,compl,completes,2774,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2928,safety,compl,completes,2928,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:464,security,soc,socket,464,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:478,security,Soc,Socket,478,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:557,security,Model,Model,557,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:570,security,Model,Model,570,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1633,security,soc,socket,1633,"ing: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_stu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1647,security,Soc,Socket,1647,"MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1693,security,Auth,AuthenticAMD,1693,"vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1727,security,Model,Model,1727,"ype: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1739,security,Model,Model,1739," L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2774,security,compl,completes,2774,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:2928,security,compl,completes,2928,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:192,testability,test,testing,192,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:1093,testability,monitor,monitor,1093,". I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(s): 8-15,32-39. > NUMA node2 CPU(s): 16-23,40-47. > Flags: fpu vme de pse tsc msr pae mce cx8 api",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:126,usability,perform,performance,126,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 1. > Vendor ID: GenuineIntel. > CPU family: 6. > Model: 85. > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz. > Stepping: 4. > CPU MHz: 1520.299. > BogoMIPS: 4999.99. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 32K. > L2 cache: 1024K. > L3 cache: 33792K. > NUMA node0 CPU(s): 0-47. > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu. > Architecture: x86_64. > CPU op-mode(s): 32-bit, 64-bit. > Byte Order: Little Endian. > CPU(s): 48. > On-line CPU(s) list: 0-47. > Thread(s) per core: 2. > Core(s) per socket: 24. > Socket(s): 1. > NUMA node(s): 3. > Vendor ID: AuthenticAMD. > CPU family: 23. > Model: 1. > Model name: AMD EPYC 7571. > Stepping: 2. > CPU MHz: 2524.374. > BogoMIPS: 4399.90. > Hypervisor vendor: KVM. > Virtualization type: full. > L1d cache: 32K. > L1i cache: 64K. > L2 cache: 512K. > L3 cache: 8192K. > NUMA node0 CPU(s): 0-7,24-31. > NUMA node1 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:69,deployability,observ,observations,69,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:304,deployability,build,building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,304,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:295,energy efficiency,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,295,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:377,energy efficiency,optim,optimizations,377,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:486,energy efficiency,optim,optimization,486,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:630,integrability,topic,topic,630,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:332,interoperability,platform,platform-how-deepVariant-uses-intels-avx-,332,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:107,performance,performance evalu,performance evaluations,107,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:162,performance,perform,performance,162,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:377,performance,optimiz,optimizations,377,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:486,performance,optimiz,optimization,486,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:421,reliability,doe,does,421,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:69,testability,observ,observations,69,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:406,testability,understand,understand,406,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:107,usability,perform,performance,107,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:162,usability,perform,performance,162,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:430,usability,support,support,430,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/274:41,performance,time,time,41,"Hi @aderzelle, I kept this open all this time because I think there was something I wanted to try. But then it was open for too long and I forgot what I was trying to do. :-/ . @aderzelle If you still have any request/questions, can you open another issue and we can restart from there? Sorry that this has lagged for too long.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/275:230,safety,input,input,230,"Hi Peter,. I'm not sure what do you mean by ""additional quality information"". May be if you could clarify that I will be able to give a better answer. DeepVariant uses base quality scores as well as mapping quality scores from an input BAM file. . Although using different mappers may affect the output of DeepVariant, candidate variants are calculated by DeepVariant by counting alleles at each position inside of a region of interest. So, if resulting alignments are identical (as well as base quality scores and mapping scores) then DeepVariant should output the identical variant calls. Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:469,security,ident,identical,469,"Hi Peter,. I'm not sure what do you mean by ""additional quality information"". May be if you could clarify that I will be able to give a better answer. DeepVariant uses base quality scores as well as mapping quality scores from an input BAM file. . Although using different mappers may affect the output of DeepVariant, candidate variants are calculated by DeepVariant by counting alleles at each position inside of a region of interest. So, if resulting alignments are identical (as well as base quality scores and mapping scores) then DeepVariant should output the identical variant calls. Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:566,security,ident,identical,566,"Hi Peter,. I'm not sure what do you mean by ""additional quality information"". May be if you could clarify that I will be able to give a better answer. DeepVariant uses base quality scores as well as mapping quality scores from an input BAM file. . Although using different mappers may affect the output of DeepVariant, candidate variants are calculated by DeepVariant by counting alleles at each position inside of a region of interest. So, if resulting alignments are identical (as well as base quality scores and mapping scores) then DeepVariant should output the identical variant calls. Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:230,usability,input,input,230,"Hi Peter,. I'm not sure what do you mean by ""additional quality information"". May be if you could clarify that I will be able to give a better answer. DeepVariant uses base quality scores as well as mapping quality scores from an input BAM file. . Although using different mappers may affect the output of DeepVariant, candidate variants are calculated by DeepVariant by counting alleles at each position inside of a region of interest. So, if resulting alignments are identical (as well as base quality scores and mapping scores) then DeepVariant should output the identical variant calls. Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:392,deployability,contain,contain,392,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:553,deployability,version,version,553,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:553,integrability,version,version,553,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:27,modifiability,exten,extend,27,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:188,modifiability,Pac,PacBio,188,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:553,modifiability,version,version,553,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:141,performance,time,time,141,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:150,reliability,doe,does,150,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:576,reliability,doe,doesn,576,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:268,security,ident,identical,268,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:281,usability,behavi,behavior,281,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:461,usability,confirm,confirm,461,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:151,modifiability,Pac,PacBio-native,151,"Hi @akolesnikov and @AndrewCarroll ,. thanks for sharing your insights. We have one collaboration where (at least some) HiFi/CCS reads were shipped as PacBio-native BAMs; hence, I just wanted to make sure that I am not omitting necessary information from DeepVariant by not using pbmm2 together with the PacBio-native BAMs to generate the input alignments. Thanks for clarifying. Best,. Peter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:304,modifiability,Pac,PacBio-native,304,"Hi @akolesnikov and @AndrewCarroll ,. thanks for sharing your insights. We have one collaboration where (at least some) HiFi/CCS reads were shipped as PacBio-native BAMs; hence, I just wanted to make sure that I am not omitting necessary information from DeepVariant by not using pbmm2 together with the PacBio-native BAMs to generate the input alignments. Thanks for clarifying. Best,. Peter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:339,safety,input,input,339,"Hi @akolesnikov and @AndrewCarroll ,. thanks for sharing your insights. We have one collaboration where (at least some) HiFi/CCS reads were shipped as PacBio-native BAMs; hence, I just wanted to make sure that I am not omitting necessary information from DeepVariant by not using pbmm2 together with the PacBio-native BAMs to generate the input alignments. Thanks for clarifying. Best,. Peter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:339,usability,input,input,339,"Hi @akolesnikov and @AndrewCarroll ,. thanks for sharing your insights. We have one collaboration where (at least some) HiFi/CCS reads were shipped as PacBio-native BAMs; hence, I just wanted to make sure that I am not omitting necessary information from DeepVariant by not using pbmm2 together with the PacBio-native BAMs to generate the input alignments. Thanks for clarifying. Best,. Peter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/277:393,deployability,version,version,393,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:435,deployability,continu,continuous,435,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:385,energy efficiency,current,current,385,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:393,integrability,version,version,393,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:41,modifiability,Pac,PacBio,41,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:254,modifiability,PAC,PACBIO,254,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:393,modifiability,version,version,393,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:466,modifiability,Pac,PacBio,466,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:179,performance,content,content,179,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:416,reliability,doe,does,416,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:523,reliability,pra,practices,523,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:488,testability,coverag,coverage,488,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:136,availability,error,error,136,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:4,energy efficiency,Current,Currently,4,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:14,modifiability,Pac,PacBio,14,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:157,modifiability,Pac,PacBio,157,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:136,performance,error,error,136,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:136,safety,error,error,136,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:73,usability,support,supported,73,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/277:136,usability,error,error,136,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277
https://github.com/google/deepvariant/issues/278:54,integrability,Filter,Filter,54,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:232,performance,network,network,232,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:427,performance,network,network,427,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:604,performance,network,network,604,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:232,security,network,network,232,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:427,security,network,network,427,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:604,security,network,network,604,"Hi @aderzelle . PASS is a commonly used value for the Filter field in VCFs. We use this to align with the terminology frequently used for variant calls. When an entry has PASS, it means that a candidate was generated and the neural network classifier gave the probability of the non-reference genotype call as higher than for reference. When an entry has RefCall present, it means that a candidate was generated and the neural network classifier gave a higher probability for a reference call. When there is no entry, this should correspond to gVCF blocks where no candidate was generated, so the neural network did not classify the position (the call is reference but made in a different way). So PASS or not pass relates to the main output of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:513,reliability,doe,does,513,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:522,safety,except,except,522,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:720,safety,compl,complexe,720,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:720,security,compl,complexe,720,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:567,testability,coverag,coverage,567,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:759,usability,user,user-images,759,"That makes a lot of sense, thank you. So RefCall is not equivalent to the absence of entry. In case you would find this interesting, in the following example, the third T is set as RefCall in one vcf, and as a A to T SNP in another one. ```. chromosome_1	10764356	.	A	T,<*>	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:36:290:223,64,0:0.22069,0:0,35,56,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. ```. This is a bit mysterious to me why it does so; except that one of my sample has a very high coverage (> 300) and maybe this confuses DeepVariant regarding the allele ratio, maybe it thinks that 64/223 is not high enough. This region seems quite complexe in fact. . ![example](https://user-images.githubusercontent.com/23341393/75356206-e9028900-58af-11ea-864c-16f423ca9f53.png).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3718,integrability,event,events,3718,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3710,safety,compl,complex,3710,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:1794,security,control,controlH,1794,"30:300:156,69,0:0.23,0:30,0,65,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:301:0,300,2999. ```. In 2 other samples for the same site. ```. zgrep -w ""chromosome_1"" H4A4.g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764351	.	GAC	G,<*>	30.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:31:41:16,25,0:0.609756,0:30,0,53,990,990,990. chromosome_1	10764354	.	A	G,<*>	30.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:30:40:15,25,0:0.625,0:30,0,51,990,990,990. chromosome_1	10764355	.	C	T,<*>	28.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:29:41:15,26,0:0.634146,0:28,0,53,990,990,990. chromosome_1	10764356	.	A	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:40:0,120,1199. chromosome_1	10764358	.	T	TTC,<*>	31.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:31:41:14,27,0:0.658537,0:31,0,60,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:39:0,123,1229. chromosome_1	10764382	.	T	G,<*>	38.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:40:25,15,0:0.375,0:38,0,40,990,990,990. zgrep -w ""chromosome_1"" controlH.g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3710,security,compl,complex,3710,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:1794,testability,control,controlH,1794,"30:300:156,69,0:0.23,0:30,0,65,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:301:0,300,2999. ```. In 2 other samples for the same site. ```. zgrep -w ""chromosome_1"" H4A4.g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764351	.	GAC	G,<*>	30.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:31:41:16,25,0:0.609756,0:30,0,53,990,990,990. chromosome_1	10764354	.	A	G,<*>	30.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:30:40:15,25,0:0.625,0:30,0,51,990,990,990. chromosome_1	10764355	.	C	T,<*>	28.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:29:41:15,26,0:0.634146,0:28,0,53,990,990,990. chromosome_1	10764356	.	A	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:40:0,120,1199. chromosome_1	10764358	.	T	TTC,<*>	31.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:31:41:14,27,0:0.658537,0:31,0,60,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:39:0,123,1229. chromosome_1	10764382	.	T	G,<*>	38.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:40:25,15,0:0.375,0:38,0,40,990,990,990. zgrep -w ""chromosome_1"" controlH.g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3282,testability,understand,understand,3282,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3374,usability,clear,clearly,3374,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:3517,usability,user,user-images,3517,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990. chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990. chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990. chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999. chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990. chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```. To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```. zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248"". chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999. chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990. chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939. chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990. chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999. ```. In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:24,integrability,messag,messages,24,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:24,interoperability,messag,messages,24,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:413,interoperability,mismatch,mismatches,413,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:70,security,control,controlH,70,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:192,security,control,controlH,192,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:70,testability,control,controlH,70,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:192,testability,control,controlH,192,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:240,usability,visual,visually,240,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:400,availability,down,downsampling,400,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:344,deployability,observ,observed,344,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:944,performance,network,network,944,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:944,security,network,network,944,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:322,testability,coverag,coverage,322,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:344,testability,observ,observed,344,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/278:29,usability,close,close,29,"@aderzelle I'll go ahead and close this issue, but feel free to reopen if you still have pending questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278
https://github.com/google/deepvariant/issues/279:171,availability,down,down,171,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:397,availability,down,down,397,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:415,availability,error,errors,415,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:325,energy efficiency,current,current-and-historical-wgs-accuracy-across-coverage,325,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:415,performance,error,errors,415,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:415,safety,error,errors,415,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:97,testability,coverag,coverage,97,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:368,testability,coverag,coverage,368,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:434,testability,coverag,coverage,434,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:488,testability,coverag,coverage,488,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:693,testability,coverag,coverage,693,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:708,testability,understand,understand,708,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:415,usability,error,errors,415,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:850,availability,down,down,850,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1083,availability,down,down,1083,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1101,availability,error,errors,1101,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:416,deployability,observ,observe,416,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1009,energy efficiency,current,current-and-historical-wgs-accuracy-across-coverage,1009,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1101,performance,error,errors,1101,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1101,safety,error,errors,1101,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1844,security,auth,auth,1844,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:118,testability,coverag,coverages,118,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:416,testability,observ,observe,416,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:773,testability,coverag,coverage,773,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1052,testability,coverag,coverage,1052,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1120,testability,coverag,coverage,1120,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1179,testability,coverag,coverage,1179,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1393,testability,coverag,coverage,1393,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1408,testability,understand,understand,1408,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:1101,usability,error,errors,1101,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes. at coverages between 0.1-1X. So data missingness is a reality we know how. to deal with, I just want to make sure that the calls we are making are. accurate. The second family of data I am working with are indigenous groups that are. at least diverged from the reference over 2000 generations ago. We observe. a strong bias towards the reference allele using GATK, but it'll be very. interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>. wrote:. > Hi @yassineS <https://github.com/yassineS>. >. > Thank you for the question. In short, DeepVariant tends to call fewer. > variants as coverage drops (this is similar to other callers). We have. > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog. > post. > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>. > This also breaks down the types of errors by class as coverage falls. >. > I am not sure what the lower bound of coverage for using DeepVariant. At. > some point, imputation approaches will be required instead of variant. > calling ones. I would guess this is somewhere around 5x-8x. >. > I am curious how low you consider low coverage, so I understand your use. > case. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:100,energy efficiency,current,current,100,"@yassineS thanks for the details! For 0.1-1x data, I would not recommend using DeepVariant. In it's current form, is not suited to handle such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:147,testability,coverag,coverages,147,"@yassineS thanks for the details! For 0.1-1x data, I would not recommend using DeepVariant. In it's current form, is not suited to handle such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/279:28,usability,close,close,28,"@yassineS I'll go ahead and close this issue, but feel free to reopen if you still have pending questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279
https://github.com/google/deepvariant/issues/280:99,usability,user,user-images,99,"Ok, I found some other cases, everytime in the vicinity of an abrupt mapping ending. ![W1](https://user-images.githubusercontent.com/23341393/75542735-ab713d80-5a20-11ea-9d71-adb22bfcb841.png). ![W2](https://user-images.githubusercontent.com/23341393/75542798-cb086600-5a20-11ea-9514-4eb573f68f9a.png). Is DeepVariant catching what are reference genome misassemblies?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:208,usability,user,user-images,208,"Ok, I found some other cases, everytime in the vicinity of an abrupt mapping ending. ![W1](https://user-images.githubusercontent.com/23341393/75542735-ab713d80-5a20-11ea-9d71-adb22bfcb841.png). ![W2](https://user-images.githubusercontent.com/23341393/75542798-cb086600-5a20-11ea-9514-4eb573f68f9a.png). Is DeepVariant catching what are reference genome misassemblies?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:23,performance,perform,performs,23,"@aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. . By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. `--emit_realigned_reads` - enables writing out of realigned reads. `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:23,usability,perform,performs,23,"@aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. . By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. `--emit_realigned_reads` - enables writing out of realigned reads. `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:32,performance,perform,perform,32,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:24,reliability,doe,does,24,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:61,reliability,doe,does,61,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:32,usability,perform,perform,32,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:49,interoperability,specif,specifically,49,"DeepVariant uses Smith-Waterman for realignment, specifically this library: https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:103,safety,Compl,Complete-Striped-Smith-Waterman-Library,103,"DeepVariant uses Smith-Waterman for realignment, specifically this library: https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:103,security,Compl,Complete-Striped-Smith-Waterman-Library,103,"DeepVariant uses Smith-Waterman for realignment, specifically this library: https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:16,usability,behavi,behavior,16,"Hello! But what behavior is correct for the two examples in this issue? It seems that this ""local realignment"" breaks the correct one and produces incorrect calls!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:112,interoperability,share,share,112,"We can help take a look if you send us the bam and realigned bam, and even the make_examples output, if you can share any of these. Otherwise, you may want to experiment with turning realignment off by using the `--norealign_reads` flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:7,usability,help,help,7,"We can help take a look if you send us the bam and realigned bam, and even the make_examples output, if you can share any of these. Otherwise, you may want to experiment with turning realignment off by using the `--norealign_reads` flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:130,availability,slo,slot,130,"Hello, . isn't it a case of ... ""well, here we need wetlab Sanger sequencing""? . I certainly can share, but I need to find a time slot to relaunch DeepVariant with the production of evidence bam. I will be on it because I am curious. @MariaNattestad where can I send you the bam?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:97,interoperability,share,share,97,"Hello, . isn't it a case of ... ""well, here we need wetlab Sanger sequencing""? . I certainly can share, but I need to find a time slot to relaunch DeepVariant with the production of evidence bam. I will be on it because I am curious. @MariaNattestad where can I send you the bam?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:125,performance,time,time,125,"Hello, . isn't it a case of ... ""well, here we need wetlab Sanger sequencing""? . I certainly can share, but I need to find a time slot to relaunch DeepVariant with the production of evidence bam. I will be on it because I am curious. @MariaNattestad where can I send you the bam?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:130,reliability,slo,slot,130,"Hello, . isn't it a case of ... ""well, here we need wetlab Sanger sequencing""? . I certainly can share, but I need to find a time slot to relaunch DeepVariant with the production of evidence bam. I will be on it because I am curious. @MariaNattestad where can I send you the bam?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:427,reliability,doe,does,427,"Hi @aderzelle . If the question is why a variant is getting called when you see no sign of it in the original bam, then that can only be answered by inspecting the realigned bam, not by additional sequencing. It seems likely given the split reads that you are seeing the signal of some kind of misassembly or structural variant (if the genomes producing the bam and the reference are not the same). If seeing the realigned bam does not answer your question, then you can send the files to marianattestad@google.com and I can take a look. I would need to see the original bam, the realigned bam, the reference genome, and the position of the variant you are asking about.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:83,security,sign,sign,83,"Hi @aderzelle . If the question is why a variant is getting called when you see no sign of it in the original bam, then that can only be answered by inspecting the realigned bam, not by additional sequencing. It seems likely given the split reads that you are seeing the signal of some kind of misassembly or structural variant (if the genomes producing the bam and the reference are not the same). If seeing the realigned bam does not answer your question, then you can send the files to marianattestad@google.com and I can take a look. I would need to see the original bam, the realigned bam, the reference genome, and the position of the variant you are asking about.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:271,security,sign,signal,271,"Hi @aderzelle . If the question is why a variant is getting called when you see no sign of it in the original bam, then that can only be answered by inspecting the realigned bam, not by additional sequencing. It seems likely given the split reads that you are seeing the signal of some kind of misassembly or structural variant (if the genomes producing the bam and the reference are not the same). If seeing the realigned bam does not answer your question, then you can send the files to marianattestad@google.com and I can take a look. I would need to see the original bam, the realigned bam, the reference genome, and the position of the variant you are asking about.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1479,deployability,modul,module,1479,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:2026,deployability,updat,update,2026,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1201,energy efficiency,model,model,1201,"you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . Valu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:766,integrability,wrap,wrapper,766,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:766,interoperability,wrapper,wrapper,766,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1479,modifiability,modul,module,1479,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1541,modifiability,pac,packages,1541,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1643,modifiability,pac,packages,1643,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:25,performance,perform,performs,25,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:829,safety,input,input,829,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:960,safety,input,input,960,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:992,safety,input,input,992,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1032,safety,input,input,1032,"ocal realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1180,safety,input,input,1180," different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1479,safety,modul,module,1479,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:2026,safety,updat,update,2026,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1201,security,model,model,1201,"you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . Valu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:2026,security,updat,update,2026,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1380,testability,Trace,Traceback,1380,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:25,usability,perform,performs,25,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:829,usability,input,input,829,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:960,usability,input,input,960,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:992,usability,input,input,992,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1032,usability,input,input,1032,"ocal realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1180,usability,input,input,1180," different than what you see in the original BAM, explaining the candidates that are generated. > . > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/280:1782,usability,command,commands,1782,"reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. > . > `--emit_realigned_reads` - enables writing out of realigned reads. > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics. ```. but runs into. ```. Traceback (most recent call last): . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> . app.run(main) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run . _run_main(main, args) . File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main . sys.exit(main(argv)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main . commands = create_all_commands() . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands . sample_name=FLAGS.sample_name)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command . kwargs.update(_extra_args_to_dict(extra_args)) . File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict . (flag_name, flag_value) = extra_arg.split('=') . ValueError: need more than 1 value to unpack . ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280
https://github.com/google/deepvariant/issues/282:253,integrability,sub,subscribed,253,"Setting it to 1 will produce the desired effect. On Sat, Mar 14, 2020 at 10:34 AM aderzelle <notifications@github.com> wrote:. > Mmmm actually it runs into ""gq resolution must be a non negative integer"". >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/282#issuecomment-599070391>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:475,security,auth,auth,475,"Setting it to 1 will produce the desired effect. On Sat, Mar 14, 2020 at 10:34 AM aderzelle <notifications@github.com> wrote:. > Mmmm actually it runs into ""gq resolution must be a non negative integer"". >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/282#issuecomment-599070391>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:1205,availability,state,statements,1205,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:242,integrability,sub,subscribed,242,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:793,integrability,FILTER,FILTER,793,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:1205,integrability,state,statements,1205,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:805,interoperability,FORMAT,FORMAT,805,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:1225,reliability,Doe,Does,1225,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:471,security,auth,auth,471,"> Setting it to 1 will produce the desired effect. > [](#). > On Sat, Mar 14, 2020 at 10:34 AM aderzelle ***@***.***> wrote: Mmmm actually it runs into ""gq resolution must be a non negative integer"".  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#282 (comment)](https://github.com/google/deepvariant/issues/282#issuecomment-599070391)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAJOESTIIOT7AAKRVLVDNGDRHOIYNANCNFSM4LJFGJFA> . Hello, I'm using DeepVariant with the `--gvcf_gq_binsize=0` to get a VFC file as obtained with GATK HaplotypeCaller with BP_RESOLUTION option. But, the output VCF file has many blocks of surrounding sites sharing the same GQ. For example:. > #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	SAMPLE. > chr6	1	.	C	<*>	0	.	END=1000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	1001	.	C	<*>	0	.	END=2000	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2001	.	A	<*>	0	.	END=2241	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0. > chr6	2242	.	C	<*>	0	.	END=2243	GT:GQ:MIN_DP:PL	0/0:4:1:0,3,29. I supposed the first three records have the same genotype quality (GQ=1), but, they are divided in 1000 position blocks. Are these statements correct? Does DeepVariant do this by default? I was looking for a VCF with both non-variant and variant positions in a single line per site. Thanks in advance,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:236,modifiability,paramet,parameter,236,"> Hi @jaspez please try `--gvcf_gq_binsize=1` instead of `0` as described above. Hi @tedyun, thanks for your answer. I meant `--gvcf_gq_binsize=1` instead of 0. Sorry for the inconvenience. I got the output I mentioned before with this parameter with `1` as value. I hope you can help me. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:280,usability,help,help,280,"> Hi @jaspez please try `--gvcf_gq_binsize=1` instead of `0` as described above. Hi @tedyun, thanks for your answer. I meant `--gvcf_gq_binsize=1` instead of 0. Sorry for the inconvenience. I got the output I mentioned before with this parameter with `1` as value. I hope you can help me. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:607,availability,slo,slow,607,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:612,availability,down,down,612,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:621,availability,down,downstream,621,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:913,deployability,configurat,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:413,energy efficiency,current,currently,413,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:596,integrability,event,eventually,596,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:913,integrability,configur,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:351,interoperability,specif,specified,351,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:913,modifiability,configur,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:607,reliability,slo,slow,607,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:195,security,ident,identical,195,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:237,security,loss,loss,237,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:913,security,configur,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:1068,usability,prefer,prefers,1068,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written? Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:218,availability,operat,operating,218,"p.s. My coworker let me know that the reason the first three records in your quoted gVCF are split into three rather than a single one is an implementation detail, where the genome is processed by concurrent processes operating on 1,000 bp chunks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:197,performance,concurren,concurrent,197,"p.s. My coworker let me know that the reason the first three records in your quoted gVCF are split into three rather than a single one is an implementation detail, where the genome is processed by concurrent processes operating on 1,000 bp chunks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:615,availability,slo,slow,615,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:620,availability,down,down,620,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:629,availability,down,downstream,629,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:921,deployability,configurat,configuration-and-analysis,921,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:421,energy efficiency,current,currently,421,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:604,integrability,event,eventually,604,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:921,integrability,configur,configuration-and-analysis,921,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:353,interoperability,specif,specified,353,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:921,modifiability,configur,configuration-and-analysis,921,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:615,reliability,slo,slow,615,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:197,security,ident,identical,197,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:239,security,loss,loss,239,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:921,security,configur,configuration-and-analysis,921,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/282:1082,usability,prefer,prefers,1082,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). > . > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). > . > May I ask what is your use case that prefers every non-variant position to be written? > . > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,. David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282
https://github.com/google/deepvariant/issues/283:188,energy efficiency,adapt,adapted,188,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:188,integrability,adapt,adapted,188,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:188,interoperability,adapt,adapted,188,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:261,interoperability,share,share,261,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:188,modifiability,adapt,adapted,188,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:177,usability,user,users,177,"Hi @claudiologiudice ,. You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115. Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:77,deployability,releas,released,77,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:100,energy efficiency,model,model,100,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:54,performance,time,time,54,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:100,security,model,model,100,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:42,usability,close,closed,42,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/283:316,usability,feedback,feedback,316,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283
https://github.com/google/deepvariant/issues/284:229,security,trust,trust,229,"Dear @pichuan ,. thanks a lot for your kind reply, and sorry for the late reply,. As you said I did the realigned flag active and I can confirm that the indel exists in the realignment file. This makes me more confused, should I trust my BAM or the Variants in the VCF? . Is there a way to flag the variant in the VCF file. This will help a lot, and we can easily find them. Kind regards. <img width=""1659"" alt=""Screenshot 2020-03-19 at 14 49 27"" src=""https://user-images.githubusercontent.com/12953535/77074721-88b7c080-69f1-11ea-8723-22e6b2fb252f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:136,usability,confirm,confirm,136,"Dear @pichuan ,. thanks a lot for your kind reply, and sorry for the late reply,. As you said I did the realigned flag active and I can confirm that the indel exists in the realignment file. This makes me more confused, should I trust my BAM or the Variants in the VCF? . Is there a way to flag the variant in the VCF file. This will help a lot, and we can easily find them. Kind regards. <img width=""1659"" alt=""Screenshot 2020-03-19 at 14 49 27"" src=""https://user-images.githubusercontent.com/12953535/77074721-88b7c080-69f1-11ea-8723-22e6b2fb252f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:334,usability,help,help,334,"Dear @pichuan ,. thanks a lot for your kind reply, and sorry for the late reply,. As you said I did the realigned flag active and I can confirm that the indel exists in the realignment file. This makes me more confused, should I trust my BAM or the Variants in the VCF? . Is there a way to flag the variant in the VCF file. This will help a lot, and we can easily find them. Kind regards. <img width=""1659"" alt=""Screenshot 2020-03-19 at 14 49 27"" src=""https://user-images.githubusercontent.com/12953535/77074721-88b7c080-69f1-11ea-8723-22e6b2fb252f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:460,usability,user,user-images,460,"Dear @pichuan ,. thanks a lot for your kind reply, and sorry for the late reply,. As you said I did the realigned flag active and I can confirm that the indel exists in the realignment file. This makes me more confused, should I trust my BAM or the Variants in the VCF? . Is there a way to flag the variant in the VCF file. This will help a lot, and we can easily find them. Kind regards. <img width=""1659"" alt=""Screenshot 2020-03-19 at 14 49 27"" src=""https://user-images.githubusercontent.com/12953535/77074721-88b7c080-69f1-11ea-8723-22e6b2fb252f.png"">.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:370,deployability,observ,observing,370,"Hi @aardes . Currently, realignment step in DeepVariant is important to get accurate Indel calls for short reads. It's not guaranteed to be always correct, but systematically, turning it on significantly improves our Indel accuracy. If you have the reference samples (e.g., HG001, HG002) in your runs, you can try turning realignment on/off and see what accuracy you're observing. Our recommendation is that for short reads, you should keep realignment on by default.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:13,energy efficiency,Current,Currently,13,"Hi @aardes . Currently, realignment step in DeepVariant is important to get accurate Indel calls for short reads. It's not guaranteed to be always correct, but systematically, turning it on significantly improves our Indel accuracy. If you have the reference samples (e.g., HG001, HG002) in your runs, you can try turning realignment on/off and see what accuracy you're observing. Our recommendation is that for short reads, you should keep realignment on by default.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:190,security,sign,significantly,190,"Hi @aardes . Currently, realignment step in DeepVariant is important to get accurate Indel calls for short reads. It's not guaranteed to be always correct, but systematically, turning it on significantly improves our Indel accuracy. If you have the reference samples (e.g., HG001, HG002) in your runs, you can try turning realignment on/off and see what accuracy you're observing. Our recommendation is that for short reads, you should keep realignment on by default.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:370,testability,observ,observing,370,"Hi @aardes . Currently, realignment step in DeepVariant is important to get accurate Indel calls for short reads. It's not guaranteed to be always correct, but systematically, turning it on significantly improves our Indel accuracy. If you have the reference samples (e.g., HG001, HG002) in your runs, you can try turning realignment on/off and see what accuracy you're observing. Our recommendation is that for short reads, you should keep realignment on by default.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:76,deployability,build,building,76,"@akolesnikov also added:. ""Realignment is done to the haplotypes created by building DeBruijn graph from input reads. Initial mapping by design cannot align reads correctly in some cases, which we try to mitigate with local realignment.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:105,safety,input,input,105,"@akolesnikov also added:. ""Realignment is done to the haplotypes created by building DeBruijn graph from input reads. Initial mapping by design cannot align reads correctly in some cases, which we try to mitigate with local realignment.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/284:105,usability,input,input,105,"@akolesnikov also added:. ""Realignment is done to the haplotypes created by building DeBruijn graph from input reads. Initial mapping by design cannot align reads correctly in some cases, which we try to mitigate with local realignment.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/284
https://github.com/google/deepvariant/issues/285:47,deployability,releas,release,47,"Hi @matthdsm , we're actively working on a new release that will be using Python3. We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/285:94,deployability,updat,updated,94,"Hi @matthdsm , we're actively working on a new release that will be using Python3. We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/285:284,deployability,releas,release,284,"Hi @matthdsm , we're actively working on a new release that will be using Python3. We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/285:94,safety,updat,updated,94,"Hi @matthdsm , we're actively working on a new release that will be using Python3. We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/285:94,security,updat,updated,94,"Hi @matthdsm , we're actively working on a new release that will be using Python3. We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/287:61,deployability,releas,release,61,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:344,deployability,updat,updated,344,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:368,deployability,version,version,368,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:368,integrability,version,version,368,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:368,modifiability,version,version,368,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:344,safety,updat,updated,344,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:344,security,updat,updated,344,"Hi @moldach . Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:. https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:53,availability,error,error,53,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:25,deployability,version,versions,25,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:410,deployability,fail,failed,410,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:17,energy efficiency,cpu,cpu,17,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:21,energy efficiency,gpu,gpu,21,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:25,integrability,version,versions,25,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:536,interoperability,format,format,536,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:25,modifiability,version,versions,25,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:341,modifiability,PAC,PACBIO,341,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:17,performance,cpu,cpu,17,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:21,performance,gpu,gpu,21,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:53,performance,error,error,53,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:410,reliability,fail,failed,410,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:53,safety,error,error,53,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:603,testability,unit,unittest,603,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:871,testability,unit,unittest,871,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:53,usability,error,error,53,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**. FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format. [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:320,modifiability,PAC,PACBIO,320,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:369,modifiability,variab,variables,369,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:411,modifiability,variab,variables,411,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
